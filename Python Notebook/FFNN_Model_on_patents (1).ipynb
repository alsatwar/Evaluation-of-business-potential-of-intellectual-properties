{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFNN_Model_on_patents.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTLrpjSk24GN",
        "colab_type": "text"
      },
      "source": [
        "### **Access Dataset from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xlAX9TGAfUU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d2ba200d-8eea-4543-d080-ca0c0419a400"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/Business'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Business\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU6HwlkYBe6R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "79e9b894-8854-4716-b8cd-06fdba596ca9"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN_Model_on_patents.ipynb  patent.h5        patent_trainingdata.csv\n",
            "patent_accuracy.png          patent_loss.png  patent_weights.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X71REuVe3YiR",
        "colab_type": "text"
      },
      "source": [
        "### **Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT29Kv31CRe1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "754aab38-0056-4f21-cf0f-f61a9b4bda18"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from keras import models,Sequential, layers\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOoZi0eZRmK8",
        "colab_type": "text"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy7IUy8pBqHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reading csv file from dataset\n",
        "data = pd.read_csv(\"patent_trainingdata.csv\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmmFacS0CZPH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "aa3cbd91-253c-4ce3-d8dc-641751476fdb"
      },
      "source": [
        "#Analyzing dataset by looking at first 5 values\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reg_num</th>\n",
              "      <th>number_ipc</th>\n",
              "      <th>number_word_abstract</th>\n",
              "      <th>number_word_ft</th>\n",
              "      <th>number_citation</th>\n",
              "      <th>number_citation_nation</th>\n",
              "      <th>number_priority</th>\n",
              "      <th>number_priority_nation</th>\n",
              "      <th>number_claim</th>\n",
              "      <th>number_claim_indep</th>\n",
              "      <th>number_claim_dep</th>\n",
              "      <th>number_claim_altered</th>\n",
              "      <th>number_applicant</th>\n",
              "      <th>number_foreign_applicant</th>\n",
              "      <th>number_applicant_nation</th>\n",
              "      <th>number_assignee</th>\n",
              "      <th>number_avgword_indep</th>\n",
              "      <th>average_gap_citation</th>\n",
              "      <th>delivery_time</th>\n",
              "      <th>number_family</th>\n",
              "      <th>number_foreign_family</th>\n",
              "      <th>number_family_nation</th>\n",
              "      <th>ipc_A</th>\n",
              "      <th>ipc_B</th>\n",
              "      <th>ipc_C</th>\n",
              "      <th>ipc_D</th>\n",
              "      <th>ipc_E</th>\n",
              "      <th>ipc_F</th>\n",
              "      <th>ipc_G</th>\n",
              "      <th>ipc_H</th>\n",
              "      <th>ipc_activity</th>\n",
              "      <th>ipc_comp</th>\n",
              "      <th>ipc_size</th>\n",
              "      <th>CBPs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6512831</td>\n",
              "      <td>5</td>\n",
              "      <td>129</td>\n",
              "      <td>149</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>46</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>800.20</td>\n",
              "      <td>2.330443</td>\n",
              "      <td>3024.2000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6970090</td>\n",
              "      <td>5</td>\n",
              "      <td>146</td>\n",
              "      <td>6369</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>98</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>751.25</td>\n",
              "      <td>1.928689</td>\n",
              "      <td>4385.5000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7752789</td>\n",
              "      <td>11</td>\n",
              "      <td>126</td>\n",
              "      <td>5516</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>238</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>640.50</td>\n",
              "      <td>2.177702</td>\n",
              "      <td>5576.5000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7188703</td>\n",
              "      <td>7</td>\n",
              "      <td>144</td>\n",
              "      <td>6420</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>232</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>712.50</td>\n",
              "      <td>2.357421</td>\n",
              "      <td>5181.1667</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6418781</td>\n",
              "      <td>4</td>\n",
              "      <td>145</td>\n",
              "      <td>154</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>127</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1915.75</td>\n",
              "      <td>2.417015</td>\n",
              "      <td>5485.7500</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   reg_num  number_ipc  number_word_abstract  ...  ipc_comp   ipc_size  CBPs\n",
              "0  6512831           5                   129  ...  2.330443  3024.2000     1\n",
              "1  6970090           5                   146  ...  1.928689  4385.5000     0\n",
              "2  7752789          11                   126  ...  2.177702  5576.5000     0\n",
              "3  7188703           7                   144  ...  2.357421  5181.1667     0\n",
              "4  6418781           4                   145  ...  2.417015  5485.7500     0\n",
              "\n",
              "[5 rows x 34 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwvfbySv39LK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Deleting unneccessary columns from our data\n",
        "\n",
        "del data['reg_num']\n",
        "del data['number_claim']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Js7SgBLldYH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "594f38f3-9126-4d7f-c3ef-ca84ba891e54"
      },
      "source": [
        "data.describe(include = 'all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>number_ipc</th>\n",
              "      <th>number_word_abstract</th>\n",
              "      <th>number_word_ft</th>\n",
              "      <th>number_citation</th>\n",
              "      <th>number_citation_nation</th>\n",
              "      <th>number_priority</th>\n",
              "      <th>number_priority_nation</th>\n",
              "      <th>number_claim_indep</th>\n",
              "      <th>number_claim_dep</th>\n",
              "      <th>number_claim_altered</th>\n",
              "      <th>number_applicant</th>\n",
              "      <th>number_foreign_applicant</th>\n",
              "      <th>number_applicant_nation</th>\n",
              "      <th>number_assignee</th>\n",
              "      <th>number_avgword_indep</th>\n",
              "      <th>average_gap_citation</th>\n",
              "      <th>delivery_time</th>\n",
              "      <th>number_family</th>\n",
              "      <th>number_foreign_family</th>\n",
              "      <th>number_family_nation</th>\n",
              "      <th>ipc_A</th>\n",
              "      <th>ipc_B</th>\n",
              "      <th>ipc_C</th>\n",
              "      <th>ipc_D</th>\n",
              "      <th>ipc_E</th>\n",
              "      <th>ipc_F</th>\n",
              "      <th>ipc_G</th>\n",
              "      <th>ipc_H</th>\n",
              "      <th>ipc_activity</th>\n",
              "      <th>ipc_comp</th>\n",
              "      <th>ipc_size</th>\n",
              "      <th>CBPs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>2.000000e+05</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.00000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.00000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "      <td>200000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.589165</td>\n",
              "      <td>114.210290</td>\n",
              "      <td>5.700186e+03</td>\n",
              "      <td>18.455020</td>\n",
              "      <td>1.485225</td>\n",
              "      <td>0.511210</td>\n",
              "      <td>0.400855</td>\n",
              "      <td>11.286545</td>\n",
              "      <td>6.646160</td>\n",
              "      <td>4.353170</td>\n",
              "      <td>2.47689</td>\n",
              "      <td>1.984785</td>\n",
              "      <td>1.111645</td>\n",
              "      <td>0.93465</td>\n",
              "      <td>107.520205</td>\n",
              "      <td>11.133305</td>\n",
              "      <td>2.789625</td>\n",
              "      <td>13.175915</td>\n",
              "      <td>10.866925</td>\n",
              "      <td>3.277840</td>\n",
              "      <td>0.146355</td>\n",
              "      <td>0.173140</td>\n",
              "      <td>0.115605</td>\n",
              "      <td>0.008390</td>\n",
              "      <td>0.024835</td>\n",
              "      <td>0.078560</td>\n",
              "      <td>0.299645</td>\n",
              "      <td>0.262580</td>\n",
              "      <td>3449.961226</td>\n",
              "      <td>2.501563</td>\n",
              "      <td>18204.478479</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.110773</td>\n",
              "      <td>72.226458</td>\n",
              "      <td>1.197407e+04</td>\n",
              "      <td>34.346125</td>\n",
              "      <td>1.091832</td>\n",
              "      <td>0.806522</td>\n",
              "      <td>0.493357</td>\n",
              "      <td>13.524096</td>\n",
              "      <td>10.826539</td>\n",
              "      <td>40.672522</td>\n",
              "      <td>1.77434</td>\n",
              "      <td>1.844389</td>\n",
              "      <td>0.372036</td>\n",
              "      <td>0.37023</td>\n",
              "      <td>105.614183</td>\n",
              "      <td>9.040089</td>\n",
              "      <td>1.411948</td>\n",
              "      <td>245.525385</td>\n",
              "      <td>245.352720</td>\n",
              "      <td>3.776829</td>\n",
              "      <td>0.353463</td>\n",
              "      <td>0.378369</td>\n",
              "      <td>0.319751</td>\n",
              "      <td>0.091212</td>\n",
              "      <td>0.155622</td>\n",
              "      <td>0.269051</td>\n",
              "      <td>0.458104</td>\n",
              "      <td>0.440037</td>\n",
              "      <td>5293.540686</td>\n",
              "      <td>0.492818</td>\n",
              "      <td>32051.862608</td>\n",
              "      <td>0.500001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.400000e+01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-164.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>1.540000e+02</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>527.000000</td>\n",
              "      <td>2.248199</td>\n",
              "      <td>2548.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>1.680000e+02</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1438.000000</td>\n",
              "      <td>2.465129</td>\n",
              "      <td>6949.000000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>144.000000</td>\n",
              "      <td>7.941250e+03</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>142.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3409.000000</td>\n",
              "      <td>2.697965</td>\n",
              "      <td>18973.687500</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>33.000000</td>\n",
              "      <td>22004.000000</td>\n",
              "      <td>1.206389e+06</td>\n",
              "      <td>1531.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>694.000000</td>\n",
              "      <td>274.000000</td>\n",
              "      <td>5357.000000</td>\n",
              "      <td>32.00000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>13.00000</td>\n",
              "      <td>13639.000000</td>\n",
              "      <td>1008.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>9809.000000</td>\n",
              "      <td>9806.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>41590.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>294445.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          number_ipc  number_word_abstract  ...       ipc_size           CBPs\n",
              "count  200000.000000         200000.000000  ...  200000.000000  200000.000000\n",
              "mean        1.589165            114.210290  ...   18204.478479       0.500000\n",
              "std         1.110773             72.226458  ...   32051.862608       0.500001\n",
              "min         1.000000              0.000000  ...       1.000000       0.000000\n",
              "25%         1.000000             79.000000  ...    2548.000000       0.000000\n",
              "50%         1.000000            112.000000  ...    6949.000000       0.500000\n",
              "75%         2.000000            144.000000  ...   18973.687500       1.000000\n",
              "max        33.000000          22004.000000  ...  294445.000000       1.000000\n",
              "\n",
              "[8 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6xLrHaBrrv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalizing all the values in the columns in the range of (0,1)\n",
        "\n",
        "x = data.values \n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x)\n",
        "data = pd.DataFrame(x_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiDlAdxssG6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "39393aa4-2d88-4fce-eac9-e75176340ab7"
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.005863</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.011104</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.021614</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.06250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.003373</td>\n",
              "      <td>0.153584</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.001121</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.019217</td>\n",
              "      <td>0.070023</td>\n",
              "      <td>0.010267</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.006635</td>\n",
              "      <td>0.005251</td>\n",
              "      <td>0.005879</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005764</td>\n",
              "      <td>0.065693</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007185</td>\n",
              "      <td>0.146758</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.018040</td>\n",
              "      <td>0.048878</td>\n",
              "      <td>0.014891</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.005726</td>\n",
              "      <td>0.004544</td>\n",
              "      <td>0.001960</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.004323</td>\n",
              "      <td>0.054745</td>\n",
              "      <td>0.000373</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.06250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.017450</td>\n",
              "      <td>0.154437</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.039216</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.015377</td>\n",
              "      <td>0.061984</td>\n",
              "      <td>0.018936</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.18750</td>\n",
              "      <td>0.006544</td>\n",
              "      <td>0.005294</td>\n",
              "      <td>0.016329</td>\n",
              "      <td>0.088889</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001441</td>\n",
              "      <td>0.058394</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.06250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.017010</td>\n",
              "      <td>0.147611</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.001019</td>\n",
              "      <td>0.000816</td>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.017108</td>\n",
              "      <td>0.071443</td>\n",
              "      <td>0.017593</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.09375</td>\n",
              "      <td>0.006590</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.012410</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.004323</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.15625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.009312</td>\n",
              "      <td>0.153584</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.046040</td>\n",
              "      <td>0.074580</td>\n",
              "      <td>0.018627</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.09375</td>\n",
              "      <td>0.002999</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.019595</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.043228</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.09375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.004546</td>\n",
              "      <td>0.146758</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000510</td>\n",
              "      <td>0.039216</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.035081</td>\n",
              "      <td>0.059703</td>\n",
              "      <td>0.023031</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.008408</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.002613</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.050432</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.096774</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.002346</td>\n",
              "      <td>0.148464</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.000510</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.055207</td>\n",
              "      <td>0.129457</td>\n",
              "      <td>0.035523</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.003954</td>\n",
              "      <td>0.003709</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001441</td>\n",
              "      <td>0.010949</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.03125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.020163</td>\n",
              "      <td>0.156143</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000510</td>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.077557</td>\n",
              "      <td>0.067625</td>\n",
              "      <td>0.098529</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.005590</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.009144</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.03125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004033</td>\n",
              "      <td>0.155290</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.009089</td>\n",
              "      <td>0.060496</td>\n",
              "      <td>0.003722</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.006544</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.018732</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.06250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.005646</td>\n",
              "      <td>0.147611</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.078431</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.013407</td>\n",
              "      <td>0.067835</td>\n",
              "      <td>0.005651</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3   ...        28        29        30   31\n",
              "0  0.12500  0.005863  0.000095  0.011104  ...  0.019217  0.070023  0.010267  1.0\n",
              "1  0.12500  0.006635  0.005251  0.005879  ...  0.018040  0.048878  0.014891  0.0\n",
              "2  0.31250  0.005726  0.004544  0.001960  ...  0.015377  0.061984  0.018936  0.0\n",
              "3  0.18750  0.006544  0.005294  0.016329  ...  0.017108  0.071443  0.017593  0.0\n",
              "4  0.09375  0.006590  0.000099  0.012410  ...  0.046040  0.074580  0.018627  0.0\n",
              "5  0.09375  0.002999  0.000095  0.019595  ...  0.035081  0.059703  0.023031  1.0\n",
              "6  0.12500  0.008408  0.000100  0.002613  ...  0.055207  0.129457  0.035523  1.0\n",
              "7  0.12500  0.003954  0.003709  0.005225  ...  0.077557  0.067625  0.098529  0.0\n",
              "8  0.12500  0.005590  0.000098  0.009144  ...  0.009089  0.060496  0.003722  0.0\n",
              "9  0.12500  0.006544  0.000114  0.005225  ...  0.013407  0.067835  0.005651  0.0\n",
              "\n",
              "[10 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrFULeu5ct1N",
        "colab_type": "text"
      },
      "source": [
        "# ***Splitting dataset into training data and test data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oebhq8faatJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stratified Sampling for splitting dataset using CBPs\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.250, random_state=42)\n",
        "for train_index, test_index in split.split(data, data[31]):\n",
        "    train_set = data.loc[train_index] # Training set\n",
        "    test_set = data.loc[test_index] # Test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMzwFeFGePJu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "61af4ae3-dfbc-4f04-e617-8ba5437d8ce4"
      },
      "source": [
        "#Checking distribution of training data\n",
        "train_set[31].value_counts() / len(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    0.5\n",
              "1.0    0.5\n",
              "Name: 31, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-JYORhGiRWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5574c592-0222-4786-cc63-7e20746becd0"
      },
      "source": [
        "#Checking distribution of test data\n",
        "test_set[31].value_counts()/len(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    0.166667\n",
              "0.0    0.166667\n",
              "Name: 31, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GonMm7wCcNoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e42e8fb4-ef4a-40c8-88bd-5204c166f13b"
      },
      "source": [
        "#Checking the data used for training set\n",
        "\n",
        "train_set.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>165590</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.004590</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.038905</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002613</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.03125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.003593</td>\n",
              "      <td>0.149317</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.039216</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.094544</td>\n",
              "      <td>0.078419</td>\n",
              "      <td>0.049201</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8860</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.013063</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.030259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002987</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.03125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.002493</td>\n",
              "      <td>0.151024</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.039216</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.010556</td>\n",
              "      <td>0.076555</td>\n",
              "      <td>0.004551</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170356</th>\n",
              "      <td>0.06250</td>\n",
              "      <td>0.003181</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.005879</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015850</td>\n",
              "      <td>0.010949</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.09375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.003299</td>\n",
              "      <td>0.149317</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.132751</td>\n",
              "      <td>0.134048</td>\n",
              "      <td>0.116786</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11052</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.008771</td>\n",
              "      <td>0.003891</td>\n",
              "      <td>0.007185</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004323</td>\n",
              "      <td>0.062044</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.010118</td>\n",
              "      <td>0.147611</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.039216</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.009065</td>\n",
              "      <td>0.073517</td>\n",
              "      <td>0.007220</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150531</th>\n",
              "      <td>0.03125</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.011104</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024496</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.03125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.004986</td>\n",
              "      <td>0.154437</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.039216</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.047721</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2   ...        29        30   31\n",
              "165590  0.00000  0.004590  0.000111  ...  0.078419  0.049201  1.0\n",
              "8860    0.00000  0.002181  0.000104  ...  0.076555  0.004551  1.0\n",
              "170356  0.06250  0.003181  0.000102  ...  0.134048  0.116786  0.0\n",
              "11052   0.00000  0.008771  0.003891  ...  0.073517  0.007220  1.0\n",
              "150531  0.03125  0.007317  0.000106  ...  0.047721  0.000379  1.0\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb30boigdcVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "f8711d15-df05-4d9c-8ab0-6fdeab87af16"
      },
      "source": [
        "#Checking the data used for test set\n",
        "\n",
        "test_set.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62016</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007590</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.001960</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003650</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.03125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.146758</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015028</td>\n",
              "      <td>0.070876</td>\n",
              "      <td>0.007906</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182907</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004181</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>0.015676</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005764</td>\n",
              "      <td>0.058394</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.010778</td>\n",
              "      <td>0.152730</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008175</td>\n",
              "      <td>0.060503</td>\n",
              "      <td>0.007808</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37714</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001818</td>\n",
              "      <td>0.005957</td>\n",
              "      <td>0.007838</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001441</td>\n",
              "      <td>0.021898</td>\n",
              "      <td>0.000373</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>0.06250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.010558</td>\n",
              "      <td>0.145904</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.059270</td>\n",
              "      <td>0.062343</td>\n",
              "      <td>0.072951</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107636</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005408</td>\n",
              "      <td>0.009324</td>\n",
              "      <td>0.001960</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004323</td>\n",
              "      <td>0.054745</td>\n",
              "      <td>0.000373</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.03125</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.013711</td>\n",
              "      <td>0.144198</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.090699</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90776</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006317</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.018289</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.033141</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002240</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.09375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.004106</td>\n",
              "      <td>0.148464</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.272211</td>\n",
              "      <td>0.077167</td>\n",
              "      <td>0.110153</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3   ...        28        29        30   31\n",
              "62016   0.0  0.007590  0.000109  0.001960  ...  0.015028  0.070876  0.007906  0.0\n",
              "182907  0.0  0.004181  0.001824  0.015676  ...  0.008175  0.060503  0.007808  0.0\n",
              "37714   0.0  0.001818  0.005957  0.007838  ...  0.059270  0.062343  0.072951  0.0\n",
              "107636  0.0  0.005408  0.009324  0.001960  ...  1.000000  0.090699  1.000000  0.0\n",
              "90776   0.0  0.006317  0.000107  0.018289  ...  0.272211  0.077167  0.110153  0.0\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMs12ECD5kEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "fe183354-2bd7-44f4-aa6e-0e5fbe410b43"
      },
      "source": [
        "# Getting size of the trainset (m x n)\n",
        "\n",
        "print(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             0         1         2   ...        29        30   31\n",
            "165590  0.00000  0.004590  0.000111  ...  0.078419  0.049201  1.0\n",
            "8860    0.00000  0.002181  0.000104  ...  0.076555  0.004551  1.0\n",
            "170356  0.06250  0.003181  0.000102  ...  0.134048  0.116786  0.0\n",
            "11052   0.00000  0.008771  0.003891  ...  0.073517  0.007220  1.0\n",
            "150531  0.03125  0.007317  0.000106  ...  0.047721  0.000379  1.0\n",
            "...         ...       ...       ...  ...       ...       ...  ...\n",
            "60030   0.00000  0.004317  0.000094  ...  0.071808  0.005583  1.0\n",
            "163644  0.00000  0.013134  0.000105  ...  0.075765  0.035844  0.0\n",
            "128887  0.06250  0.004636  0.000100  ...  0.092429  0.016272  1.0\n",
            "65409   0.00000  0.006908  0.000126  ...  0.055703  0.012281  0.0\n",
            "44623   0.00000  0.004136  0.000097  ...  0.085955  0.134097  1.0\n",
            "\n",
            "[150000 rows x 32 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anDh6zTd06Ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting X_train, y_train, X_test and y_test\n",
        "\n",
        "X_train = train_set.iloc[:,0:31]\n",
        "\n",
        "y_train = train_set.iloc[:,31]\n",
        "\n",
        "X_test = test_set.iloc[:,0:31]\n",
        "\n",
        "y_test = test_set.iloc[:,31]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Exq2kgqigXr",
        "colab_type": "text"
      },
      "source": [
        "# ***Developing Feed Forward Neural Network Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8_oN73pBU_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining number of input nodes\n",
        "\n",
        "number_of_features = 31\n",
        "\n",
        "# Start neural network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=128, activation='relu', input_shape=(number_of_features,)))\n",
        "\n",
        "# Add a dropout layer for previous hidden layer\n",
        "network.add(layers.Dropout(0.45))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "# Add a dropout layer for previous hidden layer\n",
        "network.add(layers.Dropout(0.50))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "# Add a dropout layer for previous hidden layer\n",
        "network.add(layers.Dropout(0.45))\n",
        "\n",
        "# Add fully connected layer with a sigmoid activation function\n",
        "network.add(layers.Dense(units=1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miCB72IGBqr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the network for defining optimizers, loss function and metrics\n",
        "network.compile(loss='binary_crossentropy', \n",
        "                optimizer='adam', \n",
        "                metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El-wIX0nB2bv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dcd761fc-d9b7-4e4d-a220-b4ddeec214a7"
      },
      "source": [
        "# Fitting the model to train for number of epochs and batch size\n",
        "history = network.fit(X_train, y_train, \n",
        "                      epochs = 2500, \n",
        "                      verbose=1, \n",
        "                      batch_size=512, \n",
        "                      validation_data=(X_test, y_test)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "293/293 [==============================] - 1s 4ms/step - loss: 0.6532 - accuracy: 0.6020 - val_loss: 0.6010 - val_accuracy: 0.6737\n",
            "Epoch 2/2500\n",
            "293/293 [==============================] - 1s 4ms/step - loss: 0.6021 - accuracy: 0.6737 - val_loss: 0.5789 - val_accuracy: 0.6930\n",
            "Epoch 3/2500\n",
            "293/293 [==============================] - 1s 4ms/step - loss: 0.5867 - accuracy: 0.6876 - val_loss: 0.5705 - val_accuracy: 0.6990\n",
            "Epoch 4/2500\n",
            "293/293 [==============================] - 1s 4ms/step - loss: 0.5786 - accuracy: 0.6926 - val_loss: 0.5647 - val_accuracy: 0.7017\n",
            "Epoch 5/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5733 - accuracy: 0.6949 - val_loss: 0.5613 - val_accuracy: 0.7023\n",
            "Epoch 6/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5701 - accuracy: 0.6970 - val_loss: 0.5579 - val_accuracy: 0.7035\n",
            "Epoch 7/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5662 - accuracy: 0.6987 - val_loss: 0.5531 - val_accuracy: 0.7065\n",
            "Epoch 8/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5643 - accuracy: 0.6993 - val_loss: 0.5532 - val_accuracy: 0.7076\n",
            "Epoch 9/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5627 - accuracy: 0.7017 - val_loss: 0.5522 - val_accuracy: 0.7064\n",
            "Epoch 10/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5589 - accuracy: 0.7031 - val_loss: 0.5489 - val_accuracy: 0.7105\n",
            "Epoch 11/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5576 - accuracy: 0.7029 - val_loss: 0.5484 - val_accuracy: 0.7098\n",
            "Epoch 12/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5570 - accuracy: 0.7036 - val_loss: 0.5484 - val_accuracy: 0.7106\n",
            "Epoch 13/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5551 - accuracy: 0.7048 - val_loss: 0.5442 - val_accuracy: 0.7119\n",
            "Epoch 14/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5541 - accuracy: 0.7041 - val_loss: 0.5488 - val_accuracy: 0.7071\n",
            "Epoch 15/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5540 - accuracy: 0.7047 - val_loss: 0.5427 - val_accuracy: 0.7112\n",
            "Epoch 16/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5521 - accuracy: 0.7057 - val_loss: 0.5411 - val_accuracy: 0.7115\n",
            "Epoch 17/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5501 - accuracy: 0.7077 - val_loss: 0.5406 - val_accuracy: 0.7132\n",
            "Epoch 18/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5504 - accuracy: 0.7063 - val_loss: 0.5411 - val_accuracy: 0.7126\n",
            "Epoch 19/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5489 - accuracy: 0.7081 - val_loss: 0.5397 - val_accuracy: 0.7133\n",
            "Epoch 20/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5490 - accuracy: 0.7081 - val_loss: 0.5403 - val_accuracy: 0.7131\n",
            "Epoch 21/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5486 - accuracy: 0.7071 - val_loss: 0.5422 - val_accuracy: 0.7128\n",
            "Epoch 22/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5472 - accuracy: 0.7081 - val_loss: 0.5390 - val_accuracy: 0.7139\n",
            "Epoch 23/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5474 - accuracy: 0.7073 - val_loss: 0.5373 - val_accuracy: 0.7148\n",
            "Epoch 24/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5456 - accuracy: 0.7084 - val_loss: 0.5384 - val_accuracy: 0.7142\n",
            "Epoch 25/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5455 - accuracy: 0.7091 - val_loss: 0.5392 - val_accuracy: 0.7135\n",
            "Epoch 26/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5451 - accuracy: 0.7093 - val_loss: 0.5349 - val_accuracy: 0.7157\n",
            "Epoch 27/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5451 - accuracy: 0.7089 - val_loss: 0.5353 - val_accuracy: 0.7148\n",
            "Epoch 28/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5437 - accuracy: 0.7103 - val_loss: 0.5330 - val_accuracy: 0.7159\n",
            "Epoch 29/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5430 - accuracy: 0.7092 - val_loss: 0.5344 - val_accuracy: 0.7156\n",
            "Epoch 30/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5429 - accuracy: 0.7098 - val_loss: 0.5364 - val_accuracy: 0.7149\n",
            "Epoch 31/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5424 - accuracy: 0.7113 - val_loss: 0.5347 - val_accuracy: 0.7155\n",
            "Epoch 32/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5422 - accuracy: 0.7099 - val_loss: 0.5327 - val_accuracy: 0.7158\n",
            "Epoch 33/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5424 - accuracy: 0.7106 - val_loss: 0.5379 - val_accuracy: 0.7138\n",
            "Epoch 34/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5415 - accuracy: 0.7108 - val_loss: 0.5342 - val_accuracy: 0.7144\n",
            "Epoch 35/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5412 - accuracy: 0.7113 - val_loss: 0.5342 - val_accuracy: 0.7166\n",
            "Epoch 36/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5410 - accuracy: 0.7109 - val_loss: 0.5314 - val_accuracy: 0.7176\n",
            "Epoch 37/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5416 - accuracy: 0.7111 - val_loss: 0.5334 - val_accuracy: 0.7163\n",
            "Epoch 38/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5404 - accuracy: 0.7111 - val_loss: 0.5316 - val_accuracy: 0.7171\n",
            "Epoch 39/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5393 - accuracy: 0.7116 - val_loss: 0.5307 - val_accuracy: 0.7180\n",
            "Epoch 40/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5400 - accuracy: 0.7110 - val_loss: 0.5319 - val_accuracy: 0.7170\n",
            "Epoch 41/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5402 - accuracy: 0.7117 - val_loss: 0.5311 - val_accuracy: 0.7166\n",
            "Epoch 42/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5395 - accuracy: 0.7118 - val_loss: 0.5334 - val_accuracy: 0.7168\n",
            "Epoch 43/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5395 - accuracy: 0.7108 - val_loss: 0.5304 - val_accuracy: 0.7168\n",
            "Epoch 44/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5388 - accuracy: 0.7113 - val_loss: 0.5302 - val_accuracy: 0.7169\n",
            "Epoch 45/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5398 - accuracy: 0.7114 - val_loss: 0.5294 - val_accuracy: 0.7166\n",
            "Epoch 46/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5385 - accuracy: 0.7130 - val_loss: 0.5334 - val_accuracy: 0.7168\n",
            "Epoch 47/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5384 - accuracy: 0.7123 - val_loss: 0.5312 - val_accuracy: 0.7161\n",
            "Epoch 48/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5384 - accuracy: 0.7124 - val_loss: 0.5298 - val_accuracy: 0.7183\n",
            "Epoch 49/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5383 - accuracy: 0.7129 - val_loss: 0.5274 - val_accuracy: 0.7192\n",
            "Epoch 50/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5367 - accuracy: 0.7134 - val_loss: 0.5281 - val_accuracy: 0.7182\n",
            "Epoch 51/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5378 - accuracy: 0.7126 - val_loss: 0.5268 - val_accuracy: 0.7192\n",
            "Epoch 52/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5383 - accuracy: 0.7129 - val_loss: 0.5321 - val_accuracy: 0.7161\n",
            "Epoch 53/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5371 - accuracy: 0.7132 - val_loss: 0.5331 - val_accuracy: 0.7160\n",
            "Epoch 54/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5373 - accuracy: 0.7137 - val_loss: 0.5295 - val_accuracy: 0.7161\n",
            "Epoch 55/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5365 - accuracy: 0.7139 - val_loss: 0.5299 - val_accuracy: 0.7186\n",
            "Epoch 56/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5364 - accuracy: 0.7139 - val_loss: 0.5296 - val_accuracy: 0.7184\n",
            "Epoch 57/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5366 - accuracy: 0.7133 - val_loss: 0.5309 - val_accuracy: 0.7171\n",
            "Epoch 58/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5365 - accuracy: 0.7132 - val_loss: 0.5298 - val_accuracy: 0.7188\n",
            "Epoch 59/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5366 - accuracy: 0.7136 - val_loss: 0.5316 - val_accuracy: 0.7161\n",
            "Epoch 60/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5361 - accuracy: 0.7136 - val_loss: 0.5279 - val_accuracy: 0.7188\n",
            "Epoch 61/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5355 - accuracy: 0.7145 - val_loss: 0.5301 - val_accuracy: 0.7175\n",
            "Epoch 62/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5370 - accuracy: 0.7126 - val_loss: 0.5320 - val_accuracy: 0.7178\n",
            "Epoch 63/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5355 - accuracy: 0.7139 - val_loss: 0.5267 - val_accuracy: 0.7177\n",
            "Epoch 64/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5358 - accuracy: 0.7140 - val_loss: 0.5289 - val_accuracy: 0.7185\n",
            "Epoch 65/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5364 - accuracy: 0.7140 - val_loss: 0.5291 - val_accuracy: 0.7170\n",
            "Epoch 66/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5360 - accuracy: 0.7138 - val_loss: 0.5323 - val_accuracy: 0.7160\n",
            "Epoch 67/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5357 - accuracy: 0.7149 - val_loss: 0.5279 - val_accuracy: 0.7182\n",
            "Epoch 68/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5357 - accuracy: 0.7134 - val_loss: 0.5276 - val_accuracy: 0.7190\n",
            "Epoch 69/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5358 - accuracy: 0.7142 - val_loss: 0.5291 - val_accuracy: 0.7177\n",
            "Epoch 70/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5354 - accuracy: 0.7144 - val_loss: 0.5261 - val_accuracy: 0.7197\n",
            "Epoch 71/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5346 - accuracy: 0.7137 - val_loss: 0.5282 - val_accuracy: 0.7187\n",
            "Epoch 72/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5347 - accuracy: 0.7144 - val_loss: 0.5271 - val_accuracy: 0.7195\n",
            "Epoch 73/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5350 - accuracy: 0.7141 - val_loss: 0.5260 - val_accuracy: 0.7203\n",
            "Epoch 74/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5350 - accuracy: 0.7141 - val_loss: 0.5265 - val_accuracy: 0.7195\n",
            "Epoch 75/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5348 - accuracy: 0.7157 - val_loss: 0.5277 - val_accuracy: 0.7186\n",
            "Epoch 76/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5333 - accuracy: 0.7155 - val_loss: 0.5289 - val_accuracy: 0.7174\n",
            "Epoch 77/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5334 - accuracy: 0.7154 - val_loss: 0.5254 - val_accuracy: 0.7184\n",
            "Epoch 78/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5339 - accuracy: 0.7157 - val_loss: 0.5286 - val_accuracy: 0.7191\n",
            "Epoch 79/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5339 - accuracy: 0.7147 - val_loss: 0.5286 - val_accuracy: 0.7184\n",
            "Epoch 80/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5337 - accuracy: 0.7152 - val_loss: 0.5273 - val_accuracy: 0.7189\n",
            "Epoch 81/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5343 - accuracy: 0.7153 - val_loss: 0.5263 - val_accuracy: 0.7184\n",
            "Epoch 82/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5325 - accuracy: 0.7160 - val_loss: 0.5270 - val_accuracy: 0.7189\n",
            "Epoch 83/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5328 - accuracy: 0.7155 - val_loss: 0.5259 - val_accuracy: 0.7200\n",
            "Epoch 84/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5349 - accuracy: 0.7151 - val_loss: 0.5265 - val_accuracy: 0.7199\n",
            "Epoch 85/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5336 - accuracy: 0.7162 - val_loss: 0.5258 - val_accuracy: 0.7198\n",
            "Epoch 86/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5323 - accuracy: 0.7159 - val_loss: 0.5256 - val_accuracy: 0.7196\n",
            "Epoch 87/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5335 - accuracy: 0.7148 - val_loss: 0.5250 - val_accuracy: 0.7202\n",
            "Epoch 88/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5336 - accuracy: 0.7152 - val_loss: 0.5246 - val_accuracy: 0.7202\n",
            "Epoch 89/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5322 - accuracy: 0.7149 - val_loss: 0.5247 - val_accuracy: 0.7200\n",
            "Epoch 90/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5339 - accuracy: 0.7157 - val_loss: 0.5256 - val_accuracy: 0.7212\n",
            "Epoch 91/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5323 - accuracy: 0.7156 - val_loss: 0.5285 - val_accuracy: 0.7180\n",
            "Epoch 92/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5326 - accuracy: 0.7167 - val_loss: 0.5264 - val_accuracy: 0.7190\n",
            "Epoch 93/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5329 - accuracy: 0.7166 - val_loss: 0.5256 - val_accuracy: 0.7201\n",
            "Epoch 94/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5320 - accuracy: 0.7160 - val_loss: 0.5269 - val_accuracy: 0.7169\n",
            "Epoch 95/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5325 - accuracy: 0.7158 - val_loss: 0.5280 - val_accuracy: 0.7191\n",
            "Epoch 96/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5335 - accuracy: 0.7158 - val_loss: 0.5246 - val_accuracy: 0.7200\n",
            "Epoch 97/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5318 - accuracy: 0.7160 - val_loss: 0.5222 - val_accuracy: 0.7218\n",
            "Epoch 98/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5318 - accuracy: 0.7158 - val_loss: 0.5239 - val_accuracy: 0.7201\n",
            "Epoch 99/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5324 - accuracy: 0.7153 - val_loss: 0.5240 - val_accuracy: 0.7215\n",
            "Epoch 100/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7164 - val_loss: 0.5229 - val_accuracy: 0.7221\n",
            "Epoch 101/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5320 - accuracy: 0.7163 - val_loss: 0.5237 - val_accuracy: 0.7191\n",
            "Epoch 102/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5315 - accuracy: 0.7171 - val_loss: 0.5237 - val_accuracy: 0.7210\n",
            "Epoch 103/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5320 - accuracy: 0.7173 - val_loss: 0.5245 - val_accuracy: 0.7213\n",
            "Epoch 104/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5313 - accuracy: 0.7175 - val_loss: 0.5253 - val_accuracy: 0.7193\n",
            "Epoch 105/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5313 - accuracy: 0.7165 - val_loss: 0.5239 - val_accuracy: 0.7213\n",
            "Epoch 106/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5316 - accuracy: 0.7164 - val_loss: 0.5246 - val_accuracy: 0.7201\n",
            "Epoch 107/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5318 - accuracy: 0.7154 - val_loss: 0.5244 - val_accuracy: 0.7185\n",
            "Epoch 108/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5309 - accuracy: 0.7158 - val_loss: 0.5241 - val_accuracy: 0.7191\n",
            "Epoch 109/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5320 - accuracy: 0.7162 - val_loss: 0.5251 - val_accuracy: 0.7195\n",
            "Epoch 110/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5313 - accuracy: 0.7168 - val_loss: 0.5235 - val_accuracy: 0.7199\n",
            "Epoch 111/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5310 - accuracy: 0.7157 - val_loss: 0.5249 - val_accuracy: 0.7214\n",
            "Epoch 112/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5311 - accuracy: 0.7166 - val_loss: 0.5239 - val_accuracy: 0.7213\n",
            "Epoch 113/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5309 - accuracy: 0.7171 - val_loss: 0.5269 - val_accuracy: 0.7207\n",
            "Epoch 114/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5314 - accuracy: 0.7161 - val_loss: 0.5266 - val_accuracy: 0.7189\n",
            "Epoch 115/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5300 - accuracy: 0.7172 - val_loss: 0.5233 - val_accuracy: 0.7203\n",
            "Epoch 116/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5303 - accuracy: 0.7176 - val_loss: 0.5219 - val_accuracy: 0.7226\n",
            "Epoch 117/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5302 - accuracy: 0.7168 - val_loss: 0.5225 - val_accuracy: 0.7217\n",
            "Epoch 118/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5316 - accuracy: 0.7164 - val_loss: 0.5243 - val_accuracy: 0.7196\n",
            "Epoch 119/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5305 - accuracy: 0.7171 - val_loss: 0.5257 - val_accuracy: 0.7201\n",
            "Epoch 120/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5309 - accuracy: 0.7177 - val_loss: 0.5240 - val_accuracy: 0.7195\n",
            "Epoch 121/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5313 - accuracy: 0.7162 - val_loss: 0.5231 - val_accuracy: 0.7203\n",
            "Epoch 122/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5300 - accuracy: 0.7172 - val_loss: 0.5246 - val_accuracy: 0.7204\n",
            "Epoch 123/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5305 - accuracy: 0.7177 - val_loss: 0.5229 - val_accuracy: 0.7214\n",
            "Epoch 124/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5303 - accuracy: 0.7169 - val_loss: 0.5227 - val_accuracy: 0.7203\n",
            "Epoch 125/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5304 - accuracy: 0.7166 - val_loss: 0.5267 - val_accuracy: 0.7198\n",
            "Epoch 126/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5296 - accuracy: 0.7180 - val_loss: 0.5223 - val_accuracy: 0.7216\n",
            "Epoch 127/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5294 - accuracy: 0.7182 - val_loss: 0.5228 - val_accuracy: 0.7213\n",
            "Epoch 128/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5295 - accuracy: 0.7173 - val_loss: 0.5224 - val_accuracy: 0.7214\n",
            "Epoch 129/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5299 - accuracy: 0.7169 - val_loss: 0.5241 - val_accuracy: 0.7200\n",
            "Epoch 130/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5288 - accuracy: 0.7184 - val_loss: 0.5225 - val_accuracy: 0.7205\n",
            "Epoch 131/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5296 - accuracy: 0.7178 - val_loss: 0.5228 - val_accuracy: 0.7218\n",
            "Epoch 132/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5299 - accuracy: 0.7166 - val_loss: 0.5229 - val_accuracy: 0.7205\n",
            "Epoch 133/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5301 - accuracy: 0.7170 - val_loss: 0.5245 - val_accuracy: 0.7213\n",
            "Epoch 134/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5291 - accuracy: 0.7183 - val_loss: 0.5230 - val_accuracy: 0.7213\n",
            "Epoch 135/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5297 - accuracy: 0.7185 - val_loss: 0.5235 - val_accuracy: 0.7211\n",
            "Epoch 136/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5286 - accuracy: 0.7175 - val_loss: 0.5217 - val_accuracy: 0.7210\n",
            "Epoch 137/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5289 - accuracy: 0.7180 - val_loss: 0.5222 - val_accuracy: 0.7210\n",
            "Epoch 138/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5294 - accuracy: 0.7181 - val_loss: 0.5244 - val_accuracy: 0.7209\n",
            "Epoch 139/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5299 - accuracy: 0.7179 - val_loss: 0.5233 - val_accuracy: 0.7212\n",
            "Epoch 140/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5295 - accuracy: 0.7175 - val_loss: 0.5222 - val_accuracy: 0.7211\n",
            "Epoch 141/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5301 - accuracy: 0.7173 - val_loss: 0.5215 - val_accuracy: 0.7220\n",
            "Epoch 142/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5292 - accuracy: 0.7178 - val_loss: 0.5243 - val_accuracy: 0.7200\n",
            "Epoch 143/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5292 - accuracy: 0.7183 - val_loss: 0.5236 - val_accuracy: 0.7185\n",
            "Epoch 144/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5292 - accuracy: 0.7178 - val_loss: 0.5221 - val_accuracy: 0.7203\n",
            "Epoch 145/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5289 - accuracy: 0.7183 - val_loss: 0.5228 - val_accuracy: 0.7215\n",
            "Epoch 146/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5284 - accuracy: 0.7181 - val_loss: 0.5227 - val_accuracy: 0.7221\n",
            "Epoch 147/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5284 - accuracy: 0.7184 - val_loss: 0.5222 - val_accuracy: 0.7212\n",
            "Epoch 148/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5289 - accuracy: 0.7186 - val_loss: 0.5225 - val_accuracy: 0.7213\n",
            "Epoch 149/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5279 - accuracy: 0.7189 - val_loss: 0.5253 - val_accuracy: 0.7203\n",
            "Epoch 150/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5288 - accuracy: 0.7178 - val_loss: 0.5217 - val_accuracy: 0.7216\n",
            "Epoch 151/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5281 - accuracy: 0.7185 - val_loss: 0.5227 - val_accuracy: 0.7213\n",
            "Epoch 152/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5285 - accuracy: 0.7182 - val_loss: 0.5223 - val_accuracy: 0.7211\n",
            "Epoch 153/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5286 - accuracy: 0.7184 - val_loss: 0.5221 - val_accuracy: 0.7212\n",
            "Epoch 154/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5287 - accuracy: 0.7183 - val_loss: 0.5227 - val_accuracy: 0.7224\n",
            "Epoch 155/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5282 - accuracy: 0.7179 - val_loss: 0.5235 - val_accuracy: 0.7206\n",
            "Epoch 156/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5281 - accuracy: 0.7179 - val_loss: 0.5239 - val_accuracy: 0.7209\n",
            "Epoch 157/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5286 - accuracy: 0.7176 - val_loss: 0.5231 - val_accuracy: 0.7217\n",
            "Epoch 158/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5282 - accuracy: 0.7180 - val_loss: 0.5213 - val_accuracy: 0.7205\n",
            "Epoch 159/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5284 - accuracy: 0.7186 - val_loss: 0.5240 - val_accuracy: 0.7225\n",
            "Epoch 160/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5279 - accuracy: 0.7196 - val_loss: 0.5214 - val_accuracy: 0.7228\n",
            "Epoch 161/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5274 - accuracy: 0.7185 - val_loss: 0.5226 - val_accuracy: 0.7206\n",
            "Epoch 162/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5281 - accuracy: 0.7190 - val_loss: 0.5219 - val_accuracy: 0.7220\n",
            "Epoch 163/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5286 - accuracy: 0.7175 - val_loss: 0.5221 - val_accuracy: 0.7211\n",
            "Epoch 164/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5274 - accuracy: 0.7187 - val_loss: 0.5209 - val_accuracy: 0.7229\n",
            "Epoch 165/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5281 - accuracy: 0.7181 - val_loss: 0.5203 - val_accuracy: 0.7236\n",
            "Epoch 166/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5279 - accuracy: 0.7179 - val_loss: 0.5224 - val_accuracy: 0.7223\n",
            "Epoch 167/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5270 - accuracy: 0.7184 - val_loss: 0.5201 - val_accuracy: 0.7228\n",
            "Epoch 168/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5270 - accuracy: 0.7189 - val_loss: 0.5218 - val_accuracy: 0.7225\n",
            "Epoch 169/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5276 - accuracy: 0.7194 - val_loss: 0.5198 - val_accuracy: 0.7225\n",
            "Epoch 170/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5272 - accuracy: 0.7194 - val_loss: 0.5232 - val_accuracy: 0.7219\n",
            "Epoch 171/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5275 - accuracy: 0.7185 - val_loss: 0.5220 - val_accuracy: 0.7225\n",
            "Epoch 172/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5269 - accuracy: 0.7189 - val_loss: 0.5207 - val_accuracy: 0.7215\n",
            "Epoch 173/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5269 - accuracy: 0.7190 - val_loss: 0.5210 - val_accuracy: 0.7219\n",
            "Epoch 174/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5276 - accuracy: 0.7192 - val_loss: 0.5228 - val_accuracy: 0.7216\n",
            "Epoch 175/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5270 - accuracy: 0.7200 - val_loss: 0.5203 - val_accuracy: 0.7238\n",
            "Epoch 176/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7192 - val_loss: 0.5199 - val_accuracy: 0.7222\n",
            "Epoch 177/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5276 - accuracy: 0.7192 - val_loss: 0.5221 - val_accuracy: 0.7211\n",
            "Epoch 178/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5272 - accuracy: 0.7195 - val_loss: 0.5262 - val_accuracy: 0.7216\n",
            "Epoch 179/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5268 - accuracy: 0.7187 - val_loss: 0.5211 - val_accuracy: 0.7228\n",
            "Epoch 180/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5268 - accuracy: 0.7183 - val_loss: 0.5219 - val_accuracy: 0.7207\n",
            "Epoch 181/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5271 - accuracy: 0.7184 - val_loss: 0.5208 - val_accuracy: 0.7234\n",
            "Epoch 182/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5264 - accuracy: 0.7188 - val_loss: 0.5210 - val_accuracy: 0.7215\n",
            "Epoch 183/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5271 - accuracy: 0.7187 - val_loss: 0.5216 - val_accuracy: 0.7212\n",
            "Epoch 184/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5269 - accuracy: 0.7196 - val_loss: 0.5205 - val_accuracy: 0.7215\n",
            "Epoch 185/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5262 - accuracy: 0.7196 - val_loss: 0.5216 - val_accuracy: 0.7217\n",
            "Epoch 186/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5276 - accuracy: 0.7189 - val_loss: 0.5223 - val_accuracy: 0.7220\n",
            "Epoch 187/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5267 - accuracy: 0.7195 - val_loss: 0.5195 - val_accuracy: 0.7219\n",
            "Epoch 188/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5260 - accuracy: 0.7197 - val_loss: 0.5216 - val_accuracy: 0.7220\n",
            "Epoch 189/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5256 - accuracy: 0.7198 - val_loss: 0.5231 - val_accuracy: 0.7217\n",
            "Epoch 190/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5268 - accuracy: 0.7187 - val_loss: 0.5214 - val_accuracy: 0.7238\n",
            "Epoch 191/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5275 - accuracy: 0.7191 - val_loss: 0.5204 - val_accuracy: 0.7218\n",
            "Epoch 192/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5260 - accuracy: 0.7194 - val_loss: 0.5204 - val_accuracy: 0.7221\n",
            "Epoch 193/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5269 - accuracy: 0.7191 - val_loss: 0.5208 - val_accuracy: 0.7208\n",
            "Epoch 194/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5262 - accuracy: 0.7197 - val_loss: 0.5218 - val_accuracy: 0.7225\n",
            "Epoch 195/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5255 - accuracy: 0.7200 - val_loss: 0.5217 - val_accuracy: 0.7225\n",
            "Epoch 196/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5264 - accuracy: 0.7188 - val_loss: 0.5214 - val_accuracy: 0.7223\n",
            "Epoch 197/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5263 - accuracy: 0.7197 - val_loss: 0.5221 - val_accuracy: 0.7219\n",
            "Epoch 198/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5264 - accuracy: 0.7193 - val_loss: 0.5219 - val_accuracy: 0.7214\n",
            "Epoch 199/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5265 - accuracy: 0.7195 - val_loss: 0.5213 - val_accuracy: 0.7227\n",
            "Epoch 200/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5256 - accuracy: 0.7195 - val_loss: 0.5223 - val_accuracy: 0.7220\n",
            "Epoch 201/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5257 - accuracy: 0.7196 - val_loss: 0.5199 - val_accuracy: 0.7230\n",
            "Epoch 202/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5263 - accuracy: 0.7194 - val_loss: 0.5205 - val_accuracy: 0.7226\n",
            "Epoch 203/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5262 - accuracy: 0.7199 - val_loss: 0.5207 - val_accuracy: 0.7229\n",
            "Epoch 204/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5260 - accuracy: 0.7201 - val_loss: 0.5223 - val_accuracy: 0.7233\n",
            "Epoch 205/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7203 - val_loss: 0.5203 - val_accuracy: 0.7225\n",
            "Epoch 206/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5250 - accuracy: 0.7198 - val_loss: 0.5213 - val_accuracy: 0.7220\n",
            "Epoch 207/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5252 - accuracy: 0.7194 - val_loss: 0.5194 - val_accuracy: 0.7229\n",
            "Epoch 208/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5256 - accuracy: 0.7198 - val_loss: 0.5194 - val_accuracy: 0.7231\n",
            "Epoch 209/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5257 - accuracy: 0.7194 - val_loss: 0.5215 - val_accuracy: 0.7238\n",
            "Epoch 210/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5253 - accuracy: 0.7207 - val_loss: 0.5209 - val_accuracy: 0.7233\n",
            "Epoch 211/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5249 - accuracy: 0.7210 - val_loss: 0.5202 - val_accuracy: 0.7217\n",
            "Epoch 212/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5253 - accuracy: 0.7208 - val_loss: 0.5205 - val_accuracy: 0.7222\n",
            "Epoch 213/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5257 - accuracy: 0.7194 - val_loss: 0.5204 - val_accuracy: 0.7225\n",
            "Epoch 214/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5262 - accuracy: 0.7195 - val_loss: 0.5201 - val_accuracy: 0.7229\n",
            "Epoch 215/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5256 - accuracy: 0.7198 - val_loss: 0.5208 - val_accuracy: 0.7230\n",
            "Epoch 216/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7205 - val_loss: 0.5216 - val_accuracy: 0.7226\n",
            "Epoch 217/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5257 - accuracy: 0.7198 - val_loss: 0.5203 - val_accuracy: 0.7218\n",
            "Epoch 218/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5247 - accuracy: 0.7206 - val_loss: 0.5188 - val_accuracy: 0.7228\n",
            "Epoch 219/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5253 - accuracy: 0.7197 - val_loss: 0.5186 - val_accuracy: 0.7237\n",
            "Epoch 220/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5259 - accuracy: 0.7187 - val_loss: 0.5219 - val_accuracy: 0.7231\n",
            "Epoch 221/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5251 - accuracy: 0.7207 - val_loss: 0.5199 - val_accuracy: 0.7231\n",
            "Epoch 222/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5246 - accuracy: 0.7200 - val_loss: 0.5194 - val_accuracy: 0.7224\n",
            "Epoch 223/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5242 - accuracy: 0.7208 - val_loss: 0.5190 - val_accuracy: 0.7226\n",
            "Epoch 224/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5251 - accuracy: 0.7202 - val_loss: 0.5218 - val_accuracy: 0.7229\n",
            "Epoch 225/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5259 - accuracy: 0.7193 - val_loss: 0.5208 - val_accuracy: 0.7229\n",
            "Epoch 226/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5251 - accuracy: 0.7197 - val_loss: 0.5211 - val_accuracy: 0.7219\n",
            "Epoch 227/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5252 - accuracy: 0.7203 - val_loss: 0.5188 - val_accuracy: 0.7224\n",
            "Epoch 228/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5250 - accuracy: 0.7211 - val_loss: 0.5202 - val_accuracy: 0.7232\n",
            "Epoch 229/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5256 - accuracy: 0.7203 - val_loss: 0.5211 - val_accuracy: 0.7228\n",
            "Epoch 230/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5252 - accuracy: 0.7199 - val_loss: 0.5192 - val_accuracy: 0.7219\n",
            "Epoch 231/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5250 - accuracy: 0.7205 - val_loss: 0.5193 - val_accuracy: 0.7230\n",
            "Epoch 232/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5249 - accuracy: 0.7201 - val_loss: 0.5197 - val_accuracy: 0.7230\n",
            "Epoch 233/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5254 - accuracy: 0.7197 - val_loss: 0.5199 - val_accuracy: 0.7222\n",
            "Epoch 234/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5252 - accuracy: 0.7195 - val_loss: 0.5201 - val_accuracy: 0.7230\n",
            "Epoch 235/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5245 - accuracy: 0.7205 - val_loss: 0.5206 - val_accuracy: 0.7233\n",
            "Epoch 236/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5245 - accuracy: 0.7209 - val_loss: 0.5213 - val_accuracy: 0.7227\n",
            "Epoch 237/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5251 - accuracy: 0.7201 - val_loss: 0.5197 - val_accuracy: 0.7228\n",
            "Epoch 238/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5245 - accuracy: 0.7195 - val_loss: 0.5201 - val_accuracy: 0.7234\n",
            "Epoch 239/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5247 - accuracy: 0.7197 - val_loss: 0.5227 - val_accuracy: 0.7212\n",
            "Epoch 240/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5253 - accuracy: 0.7190 - val_loss: 0.5187 - val_accuracy: 0.7231\n",
            "Epoch 241/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5250 - accuracy: 0.7210 - val_loss: 0.5198 - val_accuracy: 0.7229\n",
            "Epoch 242/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5255 - accuracy: 0.7191 - val_loss: 0.5209 - val_accuracy: 0.7233\n",
            "Epoch 243/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5249 - accuracy: 0.7200 - val_loss: 0.5196 - val_accuracy: 0.7226\n",
            "Epoch 244/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5244 - accuracy: 0.7201 - val_loss: 0.5187 - val_accuracy: 0.7236\n",
            "Epoch 245/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5250 - accuracy: 0.7197 - val_loss: 0.5213 - val_accuracy: 0.7220\n",
            "Epoch 246/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5247 - accuracy: 0.7201 - val_loss: 0.5183 - val_accuracy: 0.7227\n",
            "Epoch 247/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5251 - accuracy: 0.7209 - val_loss: 0.5226 - val_accuracy: 0.7215\n",
            "Epoch 248/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5237 - accuracy: 0.7208 - val_loss: 0.5193 - val_accuracy: 0.7222\n",
            "Epoch 249/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5237 - accuracy: 0.7213 - val_loss: 0.5199 - val_accuracy: 0.7226\n",
            "Epoch 250/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5249 - accuracy: 0.7202 - val_loss: 0.5188 - val_accuracy: 0.7229\n",
            "Epoch 251/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5240 - accuracy: 0.7221 - val_loss: 0.5197 - val_accuracy: 0.7237\n",
            "Epoch 252/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5254 - accuracy: 0.7201 - val_loss: 0.5202 - val_accuracy: 0.7222\n",
            "Epoch 253/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5241 - accuracy: 0.7199 - val_loss: 0.5206 - val_accuracy: 0.7241\n",
            "Epoch 254/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5241 - accuracy: 0.7200 - val_loss: 0.5202 - val_accuracy: 0.7233\n",
            "Epoch 255/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5249 - accuracy: 0.7199 - val_loss: 0.5202 - val_accuracy: 0.7227\n",
            "Epoch 256/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5243 - accuracy: 0.7205 - val_loss: 0.5196 - val_accuracy: 0.7223\n",
            "Epoch 257/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5242 - accuracy: 0.7206 - val_loss: 0.5202 - val_accuracy: 0.7218\n",
            "Epoch 258/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5247 - accuracy: 0.7200 - val_loss: 0.5205 - val_accuracy: 0.7224\n",
            "Epoch 259/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5246 - accuracy: 0.7210 - val_loss: 0.5198 - val_accuracy: 0.7243\n",
            "Epoch 260/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5237 - accuracy: 0.7200 - val_loss: 0.5190 - val_accuracy: 0.7240\n",
            "Epoch 261/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5242 - accuracy: 0.7204 - val_loss: 0.5190 - val_accuracy: 0.7231\n",
            "Epoch 262/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5236 - accuracy: 0.7205 - val_loss: 0.5177 - val_accuracy: 0.7230\n",
            "Epoch 263/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5231 - accuracy: 0.7220 - val_loss: 0.5216 - val_accuracy: 0.7217\n",
            "Epoch 264/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5236 - accuracy: 0.7210 - val_loss: 0.5192 - val_accuracy: 0.7235\n",
            "Epoch 265/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5237 - accuracy: 0.7204 - val_loss: 0.5197 - val_accuracy: 0.7227\n",
            "Epoch 266/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5236 - accuracy: 0.7206 - val_loss: 0.5194 - val_accuracy: 0.7229\n",
            "Epoch 267/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5242 - accuracy: 0.7208 - val_loss: 0.5199 - val_accuracy: 0.7229\n",
            "Epoch 268/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5238 - accuracy: 0.7212 - val_loss: 0.5184 - val_accuracy: 0.7233\n",
            "Epoch 269/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5240 - accuracy: 0.7204 - val_loss: 0.5182 - val_accuracy: 0.7229\n",
            "Epoch 270/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5241 - accuracy: 0.7206 - val_loss: 0.5176 - val_accuracy: 0.7242\n",
            "Epoch 271/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.7214 - val_loss: 0.5190 - val_accuracy: 0.7228\n",
            "Epoch 272/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5250 - accuracy: 0.7208 - val_loss: 0.5182 - val_accuracy: 0.7248\n",
            "Epoch 273/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5239 - accuracy: 0.7205 - val_loss: 0.5190 - val_accuracy: 0.7222\n",
            "Epoch 274/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5243 - accuracy: 0.7207 - val_loss: 0.5192 - val_accuracy: 0.7228\n",
            "Epoch 275/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5236 - accuracy: 0.7215 - val_loss: 0.5188 - val_accuracy: 0.7248\n",
            "Epoch 276/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5242 - accuracy: 0.7211 - val_loss: 0.5177 - val_accuracy: 0.7231\n",
            "Epoch 277/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5236 - accuracy: 0.7209 - val_loss: 0.5197 - val_accuracy: 0.7224\n",
            "Epoch 278/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5238 - accuracy: 0.7210 - val_loss: 0.5187 - val_accuracy: 0.7242\n",
            "Epoch 279/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5239 - accuracy: 0.7221 - val_loss: 0.5211 - val_accuracy: 0.7234\n",
            "Epoch 280/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.7214 - val_loss: 0.5193 - val_accuracy: 0.7234\n",
            "Epoch 281/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5232 - accuracy: 0.7209 - val_loss: 0.5176 - val_accuracy: 0.7229\n",
            "Epoch 282/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5234 - accuracy: 0.7208 - val_loss: 0.5195 - val_accuracy: 0.7239\n",
            "Epoch 283/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5239 - accuracy: 0.7203 - val_loss: 0.5180 - val_accuracy: 0.7239\n",
            "Epoch 284/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5228 - accuracy: 0.7213 - val_loss: 0.5187 - val_accuracy: 0.7243\n",
            "Epoch 285/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5242 - accuracy: 0.7209 - val_loss: 0.5189 - val_accuracy: 0.7238\n",
            "Epoch 286/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5229 - accuracy: 0.7212 - val_loss: 0.5204 - val_accuracy: 0.7227\n",
            "Epoch 287/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5239 - accuracy: 0.7210 - val_loss: 0.5180 - val_accuracy: 0.7220\n",
            "Epoch 288/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5229 - accuracy: 0.7220 - val_loss: 0.5179 - val_accuracy: 0.7238\n",
            "Epoch 289/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5226 - accuracy: 0.7223 - val_loss: 0.5210 - val_accuracy: 0.7223\n",
            "Epoch 290/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5241 - accuracy: 0.7200 - val_loss: 0.5206 - val_accuracy: 0.7218\n",
            "Epoch 291/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5225 - accuracy: 0.7211 - val_loss: 0.5189 - val_accuracy: 0.7237\n",
            "Epoch 292/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.7212 - val_loss: 0.5183 - val_accuracy: 0.7238\n",
            "Epoch 293/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.7216 - val_loss: 0.5186 - val_accuracy: 0.7229\n",
            "Epoch 294/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7210 - val_loss: 0.5184 - val_accuracy: 0.7230\n",
            "Epoch 295/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5225 - accuracy: 0.7221 - val_loss: 0.5192 - val_accuracy: 0.7242\n",
            "Epoch 296/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5239 - accuracy: 0.7208 - val_loss: 0.5189 - val_accuracy: 0.7242\n",
            "Epoch 297/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7219 - val_loss: 0.5181 - val_accuracy: 0.7238\n",
            "Epoch 298/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5236 - accuracy: 0.7207 - val_loss: 0.5187 - val_accuracy: 0.7238\n",
            "Epoch 299/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5227 - accuracy: 0.7222 - val_loss: 0.5202 - val_accuracy: 0.7235\n",
            "Epoch 300/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7218 - val_loss: 0.5193 - val_accuracy: 0.7230\n",
            "Epoch 301/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5227 - accuracy: 0.7213 - val_loss: 0.5202 - val_accuracy: 0.7231\n",
            "Epoch 302/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5222 - accuracy: 0.7216 - val_loss: 0.5178 - val_accuracy: 0.7236\n",
            "Epoch 303/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7221 - val_loss: 0.5191 - val_accuracy: 0.7244\n",
            "Epoch 304/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5231 - accuracy: 0.7210 - val_loss: 0.5179 - val_accuracy: 0.7238\n",
            "Epoch 305/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5226 - accuracy: 0.7219 - val_loss: 0.5212 - val_accuracy: 0.7246\n",
            "Epoch 306/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5225 - accuracy: 0.7215 - val_loss: 0.5195 - val_accuracy: 0.7237\n",
            "Epoch 307/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5225 - accuracy: 0.7211 - val_loss: 0.5190 - val_accuracy: 0.7239\n",
            "Epoch 308/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5234 - accuracy: 0.7204 - val_loss: 0.5186 - val_accuracy: 0.7246\n",
            "Epoch 309/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5219 - accuracy: 0.7219 - val_loss: 0.5237 - val_accuracy: 0.7230\n",
            "Epoch 310/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7213 - val_loss: 0.5203 - val_accuracy: 0.7237\n",
            "Epoch 311/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5234 - accuracy: 0.7215 - val_loss: 0.5181 - val_accuracy: 0.7241\n",
            "Epoch 312/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7208 - val_loss: 0.5193 - val_accuracy: 0.7236\n",
            "Epoch 313/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5223 - accuracy: 0.7210 - val_loss: 0.5183 - val_accuracy: 0.7237\n",
            "Epoch 314/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7215 - val_loss: 0.5183 - val_accuracy: 0.7222\n",
            "Epoch 315/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7216 - val_loss: 0.5172 - val_accuracy: 0.7232\n",
            "Epoch 316/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5228 - accuracy: 0.7227 - val_loss: 0.5202 - val_accuracy: 0.7217\n",
            "Epoch 317/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5227 - accuracy: 0.7214 - val_loss: 0.5184 - val_accuracy: 0.7235\n",
            "Epoch 318/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5226 - accuracy: 0.7216 - val_loss: 0.5173 - val_accuracy: 0.7240\n",
            "Epoch 319/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5226 - accuracy: 0.7215 - val_loss: 0.5175 - val_accuracy: 0.7244\n",
            "Epoch 320/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5220 - accuracy: 0.7217 - val_loss: 0.5193 - val_accuracy: 0.7238\n",
            "Epoch 321/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.7213 - val_loss: 0.5220 - val_accuracy: 0.7215\n",
            "Epoch 322/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5227 - accuracy: 0.7225 - val_loss: 0.5182 - val_accuracy: 0.7231\n",
            "Epoch 323/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5224 - accuracy: 0.7220 - val_loss: 0.5200 - val_accuracy: 0.7226\n",
            "Epoch 324/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5228 - accuracy: 0.7211 - val_loss: 0.5173 - val_accuracy: 0.7220\n",
            "Epoch 325/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5221 - accuracy: 0.7216 - val_loss: 0.5189 - val_accuracy: 0.7234\n",
            "Epoch 326/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5232 - accuracy: 0.7217 - val_loss: 0.5206 - val_accuracy: 0.7246\n",
            "Epoch 327/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5221 - accuracy: 0.7218 - val_loss: 0.5184 - val_accuracy: 0.7242\n",
            "Epoch 328/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5217 - accuracy: 0.7212 - val_loss: 0.5217 - val_accuracy: 0.7240\n",
            "Epoch 329/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5224 - accuracy: 0.7210 - val_loss: 0.5165 - val_accuracy: 0.7245\n",
            "Epoch 330/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5222 - accuracy: 0.7227 - val_loss: 0.5193 - val_accuracy: 0.7238\n",
            "Epoch 331/2500\n",
            "293/293 [==============================] - 1s 4ms/step - loss: 0.5226 - accuracy: 0.7219 - val_loss: 0.5192 - val_accuracy: 0.7238\n",
            "Epoch 332/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5222 - accuracy: 0.7215 - val_loss: 0.5179 - val_accuracy: 0.7248\n",
            "Epoch 333/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5220 - accuracy: 0.7229 - val_loss: 0.5183 - val_accuracy: 0.7232\n",
            "Epoch 334/2500\n",
            "293/293 [==============================] - 1s 4ms/step - loss: 0.5219 - accuracy: 0.7231 - val_loss: 0.5197 - val_accuracy: 0.7249\n",
            "Epoch 335/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5223 - accuracy: 0.7219 - val_loss: 0.5198 - val_accuracy: 0.7238\n",
            "Epoch 336/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5220 - accuracy: 0.7223 - val_loss: 0.5191 - val_accuracy: 0.7245\n",
            "Epoch 337/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5214 - accuracy: 0.7223 - val_loss: 0.5191 - val_accuracy: 0.7240\n",
            "Epoch 338/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7223 - val_loss: 0.5197 - val_accuracy: 0.7231\n",
            "Epoch 339/2500\n",
            "293/293 [==============================] - 1s 4ms/step - loss: 0.5218 - accuracy: 0.7218 - val_loss: 0.5176 - val_accuracy: 0.7236\n",
            "Epoch 340/2500\n",
            "293/293 [==============================] - 1s 4ms/step - loss: 0.5224 - accuracy: 0.7223 - val_loss: 0.5184 - val_accuracy: 0.7241\n",
            "Epoch 341/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5217 - accuracy: 0.7214 - val_loss: 0.5195 - val_accuracy: 0.7235\n",
            "Epoch 342/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7228 - val_loss: 0.5172 - val_accuracy: 0.7243\n",
            "Epoch 343/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5219 - accuracy: 0.7215 - val_loss: 0.5179 - val_accuracy: 0.7222\n",
            "Epoch 344/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5220 - accuracy: 0.7211 - val_loss: 0.5181 - val_accuracy: 0.7244\n",
            "Epoch 345/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5222 - accuracy: 0.7218 - val_loss: 0.5182 - val_accuracy: 0.7240\n",
            "Epoch 346/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5221 - accuracy: 0.7222 - val_loss: 0.5197 - val_accuracy: 0.7224\n",
            "Epoch 347/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7235 - val_loss: 0.5188 - val_accuracy: 0.7241\n",
            "Epoch 348/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5220 - accuracy: 0.7218 - val_loss: 0.5183 - val_accuracy: 0.7236\n",
            "Epoch 349/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5222 - accuracy: 0.7219 - val_loss: 0.5215 - val_accuracy: 0.7221\n",
            "Epoch 350/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5221 - accuracy: 0.7213 - val_loss: 0.5176 - val_accuracy: 0.7237\n",
            "Epoch 351/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5223 - accuracy: 0.7215 - val_loss: 0.5190 - val_accuracy: 0.7245\n",
            "Epoch 352/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5217 - accuracy: 0.7222 - val_loss: 0.5179 - val_accuracy: 0.7238\n",
            "Epoch 353/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5224 - accuracy: 0.7215 - val_loss: 0.5183 - val_accuracy: 0.7239\n",
            "Epoch 354/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5226 - accuracy: 0.7231 - val_loss: 0.5183 - val_accuracy: 0.7233\n",
            "Epoch 355/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5223 - accuracy: 0.7224 - val_loss: 0.5180 - val_accuracy: 0.7240\n",
            "Epoch 356/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7226 - val_loss: 0.5177 - val_accuracy: 0.7238\n",
            "Epoch 357/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5218 - accuracy: 0.7217 - val_loss: 0.5173 - val_accuracy: 0.7236\n",
            "Epoch 358/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7226 - val_loss: 0.5163 - val_accuracy: 0.7241\n",
            "Epoch 359/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7222 - val_loss: 0.5171 - val_accuracy: 0.7229\n",
            "Epoch 360/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7212 - val_loss: 0.5170 - val_accuracy: 0.7254\n",
            "Epoch 361/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7225 - val_loss: 0.5190 - val_accuracy: 0.7238\n",
            "Epoch 362/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5218 - accuracy: 0.7224 - val_loss: 0.5182 - val_accuracy: 0.7236\n",
            "Epoch 363/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7226 - val_loss: 0.5186 - val_accuracy: 0.7234\n",
            "Epoch 364/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5214 - accuracy: 0.7225 - val_loss: 0.5190 - val_accuracy: 0.7248\n",
            "Epoch 365/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7220 - val_loss: 0.5180 - val_accuracy: 0.7221\n",
            "Epoch 366/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5222 - accuracy: 0.7227 - val_loss: 0.5190 - val_accuracy: 0.7236\n",
            "Epoch 367/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5218 - accuracy: 0.7220 - val_loss: 0.5173 - val_accuracy: 0.7245\n",
            "Epoch 368/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5221 - accuracy: 0.7224 - val_loss: 0.5186 - val_accuracy: 0.7241\n",
            "Epoch 369/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5211 - accuracy: 0.7228 - val_loss: 0.5168 - val_accuracy: 0.7251\n",
            "Epoch 370/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7221 - val_loss: 0.5193 - val_accuracy: 0.7243\n",
            "Epoch 371/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7226 - val_loss: 0.5175 - val_accuracy: 0.7243\n",
            "Epoch 372/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5221 - accuracy: 0.7219 - val_loss: 0.5198 - val_accuracy: 0.7246\n",
            "Epoch 373/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7228 - val_loss: 0.5182 - val_accuracy: 0.7237\n",
            "Epoch 374/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5217 - accuracy: 0.7218 - val_loss: 0.5169 - val_accuracy: 0.7235\n",
            "Epoch 375/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5221 - accuracy: 0.7218 - val_loss: 0.5171 - val_accuracy: 0.7233\n",
            "Epoch 376/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7216 - val_loss: 0.5177 - val_accuracy: 0.7245\n",
            "Epoch 377/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5209 - accuracy: 0.7225 - val_loss: 0.5172 - val_accuracy: 0.7237\n",
            "Epoch 378/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7231 - val_loss: 0.5167 - val_accuracy: 0.7238\n",
            "Epoch 379/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5217 - accuracy: 0.7230 - val_loss: 0.5177 - val_accuracy: 0.7227\n",
            "Epoch 380/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7218 - val_loss: 0.5185 - val_accuracy: 0.7235\n",
            "Epoch 381/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5218 - accuracy: 0.7219 - val_loss: 0.5175 - val_accuracy: 0.7241\n",
            "Epoch 382/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7231 - val_loss: 0.5171 - val_accuracy: 0.7242\n",
            "Epoch 383/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5223 - accuracy: 0.7213 - val_loss: 0.5168 - val_accuracy: 0.7256\n",
            "Epoch 384/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7216 - val_loss: 0.5211 - val_accuracy: 0.7242\n",
            "Epoch 385/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5216 - accuracy: 0.7223 - val_loss: 0.5173 - val_accuracy: 0.7254\n",
            "Epoch 386/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7222 - val_loss: 0.5174 - val_accuracy: 0.7235\n",
            "Epoch 387/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7219 - val_loss: 0.5160 - val_accuracy: 0.7258\n",
            "Epoch 388/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7224 - val_loss: 0.5173 - val_accuracy: 0.7247\n",
            "Epoch 389/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5217 - accuracy: 0.7221 - val_loss: 0.5172 - val_accuracy: 0.7249\n",
            "Epoch 390/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5210 - accuracy: 0.7223 - val_loss: 0.5176 - val_accuracy: 0.7253\n",
            "Epoch 391/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5211 - accuracy: 0.7214 - val_loss: 0.5205 - val_accuracy: 0.7240\n",
            "Epoch 392/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5216 - accuracy: 0.7217 - val_loss: 0.5178 - val_accuracy: 0.7246\n",
            "Epoch 393/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5211 - accuracy: 0.7226 - val_loss: 0.5186 - val_accuracy: 0.7250\n",
            "Epoch 394/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5209 - accuracy: 0.7231 - val_loss: 0.5174 - val_accuracy: 0.7238\n",
            "Epoch 395/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7218 - val_loss: 0.5177 - val_accuracy: 0.7234\n",
            "Epoch 396/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5211 - accuracy: 0.7225 - val_loss: 0.5180 - val_accuracy: 0.7235\n",
            "Epoch 397/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7236 - val_loss: 0.5206 - val_accuracy: 0.7243\n",
            "Epoch 398/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7231 - val_loss: 0.5167 - val_accuracy: 0.7257\n",
            "Epoch 399/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5216 - accuracy: 0.7227 - val_loss: 0.5184 - val_accuracy: 0.7246\n",
            "Epoch 400/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7224 - val_loss: 0.5171 - val_accuracy: 0.7257\n",
            "Epoch 401/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5218 - accuracy: 0.7224 - val_loss: 0.5226 - val_accuracy: 0.7240\n",
            "Epoch 402/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5220 - accuracy: 0.7223 - val_loss: 0.5175 - val_accuracy: 0.7242\n",
            "Epoch 403/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7229 - val_loss: 0.5168 - val_accuracy: 0.7247\n",
            "Epoch 404/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5214 - accuracy: 0.7221 - val_loss: 0.5190 - val_accuracy: 0.7235\n",
            "Epoch 405/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5210 - accuracy: 0.7229 - val_loss: 0.5171 - val_accuracy: 0.7244\n",
            "Epoch 406/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7232 - val_loss: 0.5189 - val_accuracy: 0.7237\n",
            "Epoch 407/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7229 - val_loss: 0.5183 - val_accuracy: 0.7231\n",
            "Epoch 408/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5209 - accuracy: 0.7223 - val_loss: 0.5170 - val_accuracy: 0.7235\n",
            "Epoch 409/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7227 - val_loss: 0.5177 - val_accuracy: 0.7249\n",
            "Epoch 410/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7224 - val_loss: 0.5175 - val_accuracy: 0.7253\n",
            "Epoch 411/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7241 - val_loss: 0.5182 - val_accuracy: 0.7240\n",
            "Epoch 412/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5210 - accuracy: 0.7218 - val_loss: 0.5181 - val_accuracy: 0.7243\n",
            "Epoch 413/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5218 - accuracy: 0.7220 - val_loss: 0.5176 - val_accuracy: 0.7250\n",
            "Epoch 414/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5216 - accuracy: 0.7223 - val_loss: 0.5168 - val_accuracy: 0.7249\n",
            "Epoch 415/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7236 - val_loss: 0.5179 - val_accuracy: 0.7244\n",
            "Epoch 416/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7221 - val_loss: 0.5190 - val_accuracy: 0.7244\n",
            "Epoch 417/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5210 - accuracy: 0.7225 - val_loss: 0.5170 - val_accuracy: 0.7238\n",
            "Epoch 418/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7228 - val_loss: 0.5168 - val_accuracy: 0.7246\n",
            "Epoch 419/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5201 - accuracy: 0.7231 - val_loss: 0.5181 - val_accuracy: 0.7235\n",
            "Epoch 420/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7223 - val_loss: 0.5173 - val_accuracy: 0.7231\n",
            "Epoch 421/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7218 - val_loss: 0.5170 - val_accuracy: 0.7241\n",
            "Epoch 422/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7233 - val_loss: 0.5162 - val_accuracy: 0.7249\n",
            "Epoch 423/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5210 - accuracy: 0.7231 - val_loss: 0.5164 - val_accuracy: 0.7251\n",
            "Epoch 424/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5209 - accuracy: 0.7236 - val_loss: 0.5163 - val_accuracy: 0.7235\n",
            "Epoch 425/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7232 - val_loss: 0.5192 - val_accuracy: 0.7233\n",
            "Epoch 426/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5201 - accuracy: 0.7233 - val_loss: 0.5179 - val_accuracy: 0.7237\n",
            "Epoch 427/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7230 - val_loss: 0.5164 - val_accuracy: 0.7238\n",
            "Epoch 428/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7222 - val_loss: 0.5183 - val_accuracy: 0.7236\n",
            "Epoch 429/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7218 - val_loss: 0.5177 - val_accuracy: 0.7242\n",
            "Epoch 430/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7224 - val_loss: 0.5210 - val_accuracy: 0.7229\n",
            "Epoch 431/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7224 - val_loss: 0.5162 - val_accuracy: 0.7250\n",
            "Epoch 432/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7221 - val_loss: 0.5173 - val_accuracy: 0.7250\n",
            "Epoch 433/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7224 - val_loss: 0.5171 - val_accuracy: 0.7232\n",
            "Epoch 434/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7240 - val_loss: 0.5185 - val_accuracy: 0.7246\n",
            "Epoch 435/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5209 - accuracy: 0.7224 - val_loss: 0.5165 - val_accuracy: 0.7242\n",
            "Epoch 436/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5199 - accuracy: 0.7228 - val_loss: 0.5181 - val_accuracy: 0.7228\n",
            "Epoch 437/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7234 - val_loss: 0.5172 - val_accuracy: 0.7244\n",
            "Epoch 438/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7225 - val_loss: 0.5195 - val_accuracy: 0.7242\n",
            "Epoch 439/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7229 - val_loss: 0.5191 - val_accuracy: 0.7240\n",
            "Epoch 440/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5204 - accuracy: 0.7235 - val_loss: 0.5187 - val_accuracy: 0.7242\n",
            "Epoch 441/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5209 - accuracy: 0.7222 - val_loss: 0.5181 - val_accuracy: 0.7242\n",
            "Epoch 442/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5209 - accuracy: 0.7215 - val_loss: 0.5179 - val_accuracy: 0.7228\n",
            "Epoch 443/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7232 - val_loss: 0.5179 - val_accuracy: 0.7232\n",
            "Epoch 444/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7240 - val_loss: 0.5172 - val_accuracy: 0.7246\n",
            "Epoch 445/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7235 - val_loss: 0.5176 - val_accuracy: 0.7245\n",
            "Epoch 446/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5201 - accuracy: 0.7234 - val_loss: 0.5180 - val_accuracy: 0.7249\n",
            "Epoch 447/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7230 - val_loss: 0.5192 - val_accuracy: 0.7241\n",
            "Epoch 448/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7230 - val_loss: 0.5191 - val_accuracy: 0.7245\n",
            "Epoch 449/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7225 - val_loss: 0.5172 - val_accuracy: 0.7244\n",
            "Epoch 450/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7242 - val_loss: 0.5161 - val_accuracy: 0.7252\n",
            "Epoch 451/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5211 - accuracy: 0.7224 - val_loss: 0.5182 - val_accuracy: 0.7234\n",
            "Epoch 452/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7227 - val_loss: 0.5179 - val_accuracy: 0.7247\n",
            "Epoch 453/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7236 - val_loss: 0.5164 - val_accuracy: 0.7254\n",
            "Epoch 454/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7234 - val_loss: 0.5168 - val_accuracy: 0.7252\n",
            "Epoch 455/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5201 - accuracy: 0.7226 - val_loss: 0.5167 - val_accuracy: 0.7247\n",
            "Epoch 456/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7229 - val_loss: 0.5169 - val_accuracy: 0.7246\n",
            "Epoch 457/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7232 - val_loss: 0.5183 - val_accuracy: 0.7238\n",
            "Epoch 458/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7221 - val_loss: 0.5188 - val_accuracy: 0.7224\n",
            "Epoch 459/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5201 - accuracy: 0.7224 - val_loss: 0.5169 - val_accuracy: 0.7240\n",
            "Epoch 460/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5201 - accuracy: 0.7233 - val_loss: 0.5188 - val_accuracy: 0.7247\n",
            "Epoch 461/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7222 - val_loss: 0.5188 - val_accuracy: 0.7234\n",
            "Epoch 462/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5204 - accuracy: 0.7230 - val_loss: 0.5170 - val_accuracy: 0.7251\n",
            "Epoch 463/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7225 - val_loss: 0.5176 - val_accuracy: 0.7236\n",
            "Epoch 464/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7225 - val_loss: 0.5167 - val_accuracy: 0.7240\n",
            "Epoch 465/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7223 - val_loss: 0.5172 - val_accuracy: 0.7248\n",
            "Epoch 466/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7221 - val_loss: 0.5164 - val_accuracy: 0.7252\n",
            "Epoch 467/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7229 - val_loss: 0.5166 - val_accuracy: 0.7254\n",
            "Epoch 468/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7235 - val_loss: 0.5193 - val_accuracy: 0.7239\n",
            "Epoch 469/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7233 - val_loss: 0.5174 - val_accuracy: 0.7240\n",
            "Epoch 470/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7233 - val_loss: 0.5185 - val_accuracy: 0.7233\n",
            "Epoch 471/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7234 - val_loss: 0.5194 - val_accuracy: 0.7243\n",
            "Epoch 472/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7231 - val_loss: 0.5179 - val_accuracy: 0.7234\n",
            "Epoch 473/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7232 - val_loss: 0.5186 - val_accuracy: 0.7222\n",
            "Epoch 474/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5201 - accuracy: 0.7233 - val_loss: 0.5182 - val_accuracy: 0.7242\n",
            "Epoch 475/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7237 - val_loss: 0.5170 - val_accuracy: 0.7236\n",
            "Epoch 476/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7232 - val_loss: 0.5174 - val_accuracy: 0.7239\n",
            "Epoch 477/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7241 - val_loss: 0.5163 - val_accuracy: 0.7253\n",
            "Epoch 478/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7241 - val_loss: 0.5178 - val_accuracy: 0.7245\n",
            "Epoch 479/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5204 - accuracy: 0.7230 - val_loss: 0.5186 - val_accuracy: 0.7236\n",
            "Epoch 480/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7228 - val_loss: 0.5188 - val_accuracy: 0.7244\n",
            "Epoch 481/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7232 - val_loss: 0.5183 - val_accuracy: 0.7232\n",
            "Epoch 482/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7237 - val_loss: 0.5189 - val_accuracy: 0.7239\n",
            "Epoch 483/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7220 - val_loss: 0.5202 - val_accuracy: 0.7230\n",
            "Epoch 484/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7228 - val_loss: 0.5167 - val_accuracy: 0.7239\n",
            "Epoch 485/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7237 - val_loss: 0.5209 - val_accuracy: 0.7239\n",
            "Epoch 486/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5199 - accuracy: 0.7232 - val_loss: 0.5170 - val_accuracy: 0.7252\n",
            "Epoch 487/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7228 - val_loss: 0.5174 - val_accuracy: 0.7252\n",
            "Epoch 488/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5204 - accuracy: 0.7228 - val_loss: 0.5152 - val_accuracy: 0.7255\n",
            "Epoch 489/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7236 - val_loss: 0.5160 - val_accuracy: 0.7251\n",
            "Epoch 490/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7232 - val_loss: 0.5171 - val_accuracy: 0.7238\n",
            "Epoch 491/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5199 - accuracy: 0.7234 - val_loss: 0.5204 - val_accuracy: 0.7241\n",
            "Epoch 492/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7237 - val_loss: 0.5173 - val_accuracy: 0.7248\n",
            "Epoch 493/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7232 - val_loss: 0.5156 - val_accuracy: 0.7248\n",
            "Epoch 494/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7233 - val_loss: 0.5173 - val_accuracy: 0.7238\n",
            "Epoch 495/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7238 - val_loss: 0.5169 - val_accuracy: 0.7240\n",
            "Epoch 496/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7240 - val_loss: 0.5169 - val_accuracy: 0.7251\n",
            "Epoch 497/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7236 - val_loss: 0.5173 - val_accuracy: 0.7246\n",
            "Epoch 498/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7247 - val_loss: 0.5169 - val_accuracy: 0.7242\n",
            "Epoch 499/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7235 - val_loss: 0.5180 - val_accuracy: 0.7246\n",
            "Epoch 500/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7243 - val_loss: 0.5164 - val_accuracy: 0.7262\n",
            "Epoch 501/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7235 - val_loss: 0.5174 - val_accuracy: 0.7250\n",
            "Epoch 502/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7233 - val_loss: 0.5164 - val_accuracy: 0.7247\n",
            "Epoch 503/2500\n",
            "293/293 [==============================] - 1s 4ms/step - loss: 0.5192 - accuracy: 0.7234 - val_loss: 0.5174 - val_accuracy: 0.7247\n",
            "Epoch 504/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7243 - val_loss: 0.5157 - val_accuracy: 0.7254\n",
            "Epoch 505/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7242 - val_loss: 0.5163 - val_accuracy: 0.7252\n",
            "Epoch 506/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7242 - val_loss: 0.5160 - val_accuracy: 0.7245\n",
            "Epoch 507/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7227 - val_loss: 0.5178 - val_accuracy: 0.7252\n",
            "Epoch 508/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7237 - val_loss: 0.5186 - val_accuracy: 0.7258\n",
            "Epoch 509/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7236 - val_loss: 0.5171 - val_accuracy: 0.7246\n",
            "Epoch 510/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7235 - val_loss: 0.5171 - val_accuracy: 0.7242\n",
            "Epoch 511/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7245 - val_loss: 0.5192 - val_accuracy: 0.7245\n",
            "Epoch 512/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7242 - val_loss: 0.5184 - val_accuracy: 0.7247\n",
            "Epoch 513/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7240 - val_loss: 0.5171 - val_accuracy: 0.7234\n",
            "Epoch 514/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7229 - val_loss: 0.5166 - val_accuracy: 0.7250\n",
            "Epoch 515/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7247 - val_loss: 0.5174 - val_accuracy: 0.7249\n",
            "Epoch 516/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7239 - val_loss: 0.5180 - val_accuracy: 0.7246\n",
            "Epoch 517/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7234 - val_loss: 0.5185 - val_accuracy: 0.7254\n",
            "Epoch 518/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5199 - accuracy: 0.7228 - val_loss: 0.5172 - val_accuracy: 0.7244\n",
            "Epoch 519/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7227 - val_loss: 0.5169 - val_accuracy: 0.7252\n",
            "Epoch 520/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7241 - val_loss: 0.5175 - val_accuracy: 0.7248\n",
            "Epoch 521/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5199 - accuracy: 0.7236 - val_loss: 0.5172 - val_accuracy: 0.7239\n",
            "Epoch 522/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7240 - val_loss: 0.5174 - val_accuracy: 0.7248\n",
            "Epoch 523/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7243 - val_loss: 0.5174 - val_accuracy: 0.7254\n",
            "Epoch 524/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7243 - val_loss: 0.5177 - val_accuracy: 0.7243\n",
            "Epoch 525/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7248 - val_loss: 0.5171 - val_accuracy: 0.7250\n",
            "Epoch 526/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7235 - val_loss: 0.5175 - val_accuracy: 0.7246\n",
            "Epoch 527/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7233 - val_loss: 0.5173 - val_accuracy: 0.7246\n",
            "Epoch 528/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7233 - val_loss: 0.5170 - val_accuracy: 0.7244\n",
            "Epoch 529/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7240 - val_loss: 0.5207 - val_accuracy: 0.7239\n",
            "Epoch 530/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7244 - val_loss: 0.5160 - val_accuracy: 0.7248\n",
            "Epoch 531/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7230 - val_loss: 0.5168 - val_accuracy: 0.7251\n",
            "Epoch 532/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7233 - val_loss: 0.5171 - val_accuracy: 0.7251\n",
            "Epoch 533/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7238 - val_loss: 0.5186 - val_accuracy: 0.7233\n",
            "Epoch 534/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7236 - val_loss: 0.5175 - val_accuracy: 0.7246\n",
            "Epoch 535/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7237 - val_loss: 0.5179 - val_accuracy: 0.7255\n",
            "Epoch 536/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7233 - val_loss: 0.5167 - val_accuracy: 0.7251\n",
            "Epoch 537/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7233 - val_loss: 0.5162 - val_accuracy: 0.7248\n",
            "Epoch 538/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7239 - val_loss: 0.5168 - val_accuracy: 0.7264\n",
            "Epoch 539/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7235 - val_loss: 0.5166 - val_accuracy: 0.7263\n",
            "Epoch 540/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7245 - val_loss: 0.5184 - val_accuracy: 0.7256\n",
            "Epoch 541/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7230 - val_loss: 0.5173 - val_accuracy: 0.7248\n",
            "Epoch 542/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7236 - val_loss: 0.5170 - val_accuracy: 0.7249\n",
            "Epoch 543/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7233 - val_loss: 0.5163 - val_accuracy: 0.7250\n",
            "Epoch 544/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7238 - val_loss: 0.5167 - val_accuracy: 0.7246\n",
            "Epoch 545/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7246 - val_loss: 0.5172 - val_accuracy: 0.7254\n",
            "Epoch 546/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7234 - val_loss: 0.5164 - val_accuracy: 0.7258\n",
            "Epoch 547/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7232 - val_loss: 0.5161 - val_accuracy: 0.7248\n",
            "Epoch 548/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7235 - val_loss: 0.5169 - val_accuracy: 0.7260\n",
            "Epoch 549/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7247 - val_loss: 0.5168 - val_accuracy: 0.7254\n",
            "Epoch 550/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7235 - val_loss: 0.5163 - val_accuracy: 0.7259\n",
            "Epoch 551/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7244 - val_loss: 0.5176 - val_accuracy: 0.7257\n",
            "Epoch 552/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7240 - val_loss: 0.5171 - val_accuracy: 0.7246\n",
            "Epoch 553/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7247 - val_loss: 0.5169 - val_accuracy: 0.7263\n",
            "Epoch 554/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7232 - val_loss: 0.5167 - val_accuracy: 0.7256\n",
            "Epoch 555/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7241 - val_loss: 0.5195 - val_accuracy: 0.7244\n",
            "Epoch 556/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7245 - val_loss: 0.5180 - val_accuracy: 0.7252\n",
            "Epoch 557/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7239 - val_loss: 0.5169 - val_accuracy: 0.7250\n",
            "Epoch 558/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7238 - val_loss: 0.5157 - val_accuracy: 0.7256\n",
            "Epoch 559/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7243 - val_loss: 0.5172 - val_accuracy: 0.7253\n",
            "Epoch 560/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7246 - val_loss: 0.5168 - val_accuracy: 0.7241\n",
            "Epoch 561/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7238 - val_loss: 0.5171 - val_accuracy: 0.7252\n",
            "Epoch 562/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7243 - val_loss: 0.5163 - val_accuracy: 0.7253\n",
            "Epoch 563/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7246 - val_loss: 0.5184 - val_accuracy: 0.7252\n",
            "Epoch 564/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7244 - val_loss: 0.5199 - val_accuracy: 0.7246\n",
            "Epoch 565/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7244 - val_loss: 0.5173 - val_accuracy: 0.7243\n",
            "Epoch 566/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7230 - val_loss: 0.5165 - val_accuracy: 0.7263\n",
            "Epoch 567/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7244 - val_loss: 0.5155 - val_accuracy: 0.7255\n",
            "Epoch 568/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7238 - val_loss: 0.5163 - val_accuracy: 0.7253\n",
            "Epoch 569/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7248 - val_loss: 0.5181 - val_accuracy: 0.7253\n",
            "Epoch 570/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7242 - val_loss: 0.5169 - val_accuracy: 0.7244\n",
            "Epoch 571/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7241 - val_loss: 0.5172 - val_accuracy: 0.7250\n",
            "Epoch 572/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7246 - val_loss: 0.5170 - val_accuracy: 0.7248\n",
            "Epoch 573/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7239 - val_loss: 0.5162 - val_accuracy: 0.7251\n",
            "Epoch 574/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7239 - val_loss: 0.5157 - val_accuracy: 0.7255\n",
            "Epoch 575/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7237 - val_loss: 0.5182 - val_accuracy: 0.7245\n",
            "Epoch 576/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7239 - val_loss: 0.5179 - val_accuracy: 0.7255\n",
            "Epoch 577/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7233 - val_loss: 0.5183 - val_accuracy: 0.7231\n",
            "Epoch 578/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7251 - val_loss: 0.5180 - val_accuracy: 0.7235\n",
            "Epoch 579/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7242 - val_loss: 0.5171 - val_accuracy: 0.7255\n",
            "Epoch 580/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7241 - val_loss: 0.5160 - val_accuracy: 0.7254\n",
            "Epoch 581/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7234 - val_loss: 0.5169 - val_accuracy: 0.7259\n",
            "Epoch 582/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7244 - val_loss: 0.5166 - val_accuracy: 0.7258\n",
            "Epoch 583/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7258 - val_loss: 0.5186 - val_accuracy: 0.7252\n",
            "Epoch 584/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7243 - val_loss: 0.5166 - val_accuracy: 0.7247\n",
            "Epoch 585/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7247 - val_loss: 0.5167 - val_accuracy: 0.7252\n",
            "Epoch 586/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7244 - val_loss: 0.5164 - val_accuracy: 0.7260\n",
            "Epoch 587/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7255 - val_loss: 0.5161 - val_accuracy: 0.7259\n",
            "Epoch 588/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7228 - val_loss: 0.5182 - val_accuracy: 0.7245\n",
            "Epoch 589/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7233 - val_loss: 0.5177 - val_accuracy: 0.7247\n",
            "Epoch 590/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7248 - val_loss: 0.5160 - val_accuracy: 0.7255\n",
            "Epoch 591/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7237 - val_loss: 0.5176 - val_accuracy: 0.7243\n",
            "Epoch 592/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7248 - val_loss: 0.5172 - val_accuracy: 0.7262\n",
            "Epoch 593/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7245 - val_loss: 0.5170 - val_accuracy: 0.7252\n",
            "Epoch 594/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7239 - val_loss: 0.5174 - val_accuracy: 0.7254\n",
            "Epoch 595/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7248 - val_loss: 0.5177 - val_accuracy: 0.7254\n",
            "Epoch 596/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7239 - val_loss: 0.5170 - val_accuracy: 0.7252\n",
            "Epoch 597/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7243 - val_loss: 0.5161 - val_accuracy: 0.7265\n",
            "Epoch 598/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7248 - val_loss: 0.5168 - val_accuracy: 0.7257\n",
            "Epoch 599/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7239 - val_loss: 0.5176 - val_accuracy: 0.7247\n",
            "Epoch 600/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7241 - val_loss: 0.5164 - val_accuracy: 0.7250\n",
            "Epoch 601/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7246 - val_loss: 0.5169 - val_accuracy: 0.7239\n",
            "Epoch 602/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7240 - val_loss: 0.5175 - val_accuracy: 0.7245\n",
            "Epoch 603/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7239 - val_loss: 0.5168 - val_accuracy: 0.7253\n",
            "Epoch 604/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7241 - val_loss: 0.5180 - val_accuracy: 0.7254\n",
            "Epoch 605/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7246 - val_loss: 0.5182 - val_accuracy: 0.7247\n",
            "Epoch 606/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7248 - val_loss: 0.5168 - val_accuracy: 0.7246\n",
            "Epoch 607/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5182 - accuracy: 0.7245 - val_loss: 0.5177 - val_accuracy: 0.7253\n",
            "Epoch 608/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7247 - val_loss: 0.5173 - val_accuracy: 0.7246\n",
            "Epoch 609/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7235 - val_loss: 0.5168 - val_accuracy: 0.7248\n",
            "Epoch 610/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7245 - val_loss: 0.5171 - val_accuracy: 0.7249\n",
            "Epoch 611/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7250 - val_loss: 0.5169 - val_accuracy: 0.7253\n",
            "Epoch 612/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7252 - val_loss: 0.5165 - val_accuracy: 0.7253\n",
            "Epoch 613/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7247 - val_loss: 0.5158 - val_accuracy: 0.7263\n",
            "Epoch 614/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7242 - val_loss: 0.5163 - val_accuracy: 0.7252\n",
            "Epoch 615/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7243 - val_loss: 0.5170 - val_accuracy: 0.7248\n",
            "Epoch 616/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7241 - val_loss: 0.5159 - val_accuracy: 0.7253\n",
            "Epoch 617/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7256 - val_loss: 0.5175 - val_accuracy: 0.7245\n",
            "Epoch 618/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7251 - val_loss: 0.5176 - val_accuracy: 0.7256\n",
            "Epoch 619/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7241 - val_loss: 0.5164 - val_accuracy: 0.7247\n",
            "Epoch 620/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7232 - val_loss: 0.5175 - val_accuracy: 0.7247\n",
            "Epoch 621/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7242 - val_loss: 0.5185 - val_accuracy: 0.7256\n",
            "Epoch 622/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7238 - val_loss: 0.5172 - val_accuracy: 0.7252\n",
            "Epoch 623/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7239 - val_loss: 0.5179 - val_accuracy: 0.7241\n",
            "Epoch 624/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7247 - val_loss: 0.5165 - val_accuracy: 0.7259\n",
            "Epoch 625/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7235 - val_loss: 0.5190 - val_accuracy: 0.7255\n",
            "Epoch 626/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7235 - val_loss: 0.5170 - val_accuracy: 0.7251\n",
            "Epoch 627/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7248 - val_loss: 0.5179 - val_accuracy: 0.7246\n",
            "Epoch 628/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7253 - val_loss: 0.5173 - val_accuracy: 0.7250\n",
            "Epoch 629/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7252 - val_loss: 0.5175 - val_accuracy: 0.7266\n",
            "Epoch 630/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7244 - val_loss: 0.5158 - val_accuracy: 0.7252\n",
            "Epoch 631/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7237 - val_loss: 0.5183 - val_accuracy: 0.7257\n",
            "Epoch 632/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7252 - val_loss: 0.5164 - val_accuracy: 0.7256\n",
            "Epoch 633/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7245 - val_loss: 0.5182 - val_accuracy: 0.7261\n",
            "Epoch 634/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7252 - val_loss: 0.5175 - val_accuracy: 0.7263\n",
            "Epoch 635/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7244 - val_loss: 0.5173 - val_accuracy: 0.7257\n",
            "Epoch 636/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7240 - val_loss: 0.5174 - val_accuracy: 0.7254\n",
            "Epoch 637/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7244 - val_loss: 0.5166 - val_accuracy: 0.7255\n",
            "Epoch 638/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7241 - val_loss: 0.5178 - val_accuracy: 0.7258\n",
            "Epoch 639/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7247 - val_loss: 0.5178 - val_accuracy: 0.7252\n",
            "Epoch 640/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7249 - val_loss: 0.5171 - val_accuracy: 0.7249\n",
            "Epoch 641/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7242 - val_loss: 0.5164 - val_accuracy: 0.7262\n",
            "Epoch 642/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7249 - val_loss: 0.5176 - val_accuracy: 0.7253\n",
            "Epoch 643/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7245 - val_loss: 0.5167 - val_accuracy: 0.7257\n",
            "Epoch 644/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7249 - val_loss: 0.5168 - val_accuracy: 0.7267\n",
            "Epoch 645/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7255 - val_loss: 0.5183 - val_accuracy: 0.7256\n",
            "Epoch 646/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7251 - val_loss: 0.5162 - val_accuracy: 0.7259\n",
            "Epoch 647/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7247 - val_loss: 0.5174 - val_accuracy: 0.7256\n",
            "Epoch 648/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7243 - val_loss: 0.5166 - val_accuracy: 0.7254\n",
            "Epoch 649/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7250 - val_loss: 0.5172 - val_accuracy: 0.7270\n",
            "Epoch 650/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7250 - val_loss: 0.5162 - val_accuracy: 0.7262\n",
            "Epoch 651/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7249 - val_loss: 0.5167 - val_accuracy: 0.7254\n",
            "Epoch 652/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7248 - val_loss: 0.5165 - val_accuracy: 0.7261\n",
            "Epoch 653/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7251 - val_loss: 0.5190 - val_accuracy: 0.7254\n",
            "Epoch 654/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7250 - val_loss: 0.5182 - val_accuracy: 0.7262\n",
            "Epoch 655/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7247 - val_loss: 0.5174 - val_accuracy: 0.7264\n",
            "Epoch 656/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7249 - val_loss: 0.5184 - val_accuracy: 0.7264\n",
            "Epoch 657/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7243 - val_loss: 0.5160 - val_accuracy: 0.7261\n",
            "Epoch 658/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7238 - val_loss: 0.5161 - val_accuracy: 0.7246\n",
            "Epoch 659/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7253 - val_loss: 0.5170 - val_accuracy: 0.7258\n",
            "Epoch 660/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7246 - val_loss: 0.5173 - val_accuracy: 0.7263\n",
            "Epoch 661/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7244 - val_loss: 0.5160 - val_accuracy: 0.7246\n",
            "Epoch 662/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7252 - val_loss: 0.5176 - val_accuracy: 0.7259\n",
            "Epoch 663/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7260 - val_loss: 0.5180 - val_accuracy: 0.7241\n",
            "Epoch 664/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5182 - accuracy: 0.7247 - val_loss: 0.5166 - val_accuracy: 0.7255\n",
            "Epoch 665/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7252 - val_loss: 0.5186 - val_accuracy: 0.7258\n",
            "Epoch 666/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7242 - val_loss: 0.5175 - val_accuracy: 0.7258\n",
            "Epoch 667/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7255 - val_loss: 0.5173 - val_accuracy: 0.7254\n",
            "Epoch 668/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7249 - val_loss: 0.5165 - val_accuracy: 0.7267\n",
            "Epoch 669/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7252 - val_loss: 0.5162 - val_accuracy: 0.7256\n",
            "Epoch 670/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7251 - val_loss: 0.5179 - val_accuracy: 0.7260\n",
            "Epoch 671/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7247 - val_loss: 0.5154 - val_accuracy: 0.7264\n",
            "Epoch 672/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7253 - val_loss: 0.5170 - val_accuracy: 0.7260\n",
            "Epoch 673/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7242 - val_loss: 0.5161 - val_accuracy: 0.7258\n",
            "Epoch 674/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7257 - val_loss: 0.5165 - val_accuracy: 0.7260\n",
            "Epoch 675/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7254 - val_loss: 0.5173 - val_accuracy: 0.7262\n",
            "Epoch 676/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7256 - val_loss: 0.5194 - val_accuracy: 0.7246\n",
            "Epoch 677/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7248 - val_loss: 0.5194 - val_accuracy: 0.7259\n",
            "Epoch 678/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7247 - val_loss: 0.5182 - val_accuracy: 0.7247\n",
            "Epoch 679/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7242 - val_loss: 0.5156 - val_accuracy: 0.7265\n",
            "Epoch 680/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7251 - val_loss: 0.5163 - val_accuracy: 0.7248\n",
            "Epoch 681/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7264 - val_loss: 0.5164 - val_accuracy: 0.7255\n",
            "Epoch 682/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7250 - val_loss: 0.5163 - val_accuracy: 0.7257\n",
            "Epoch 683/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7257 - val_loss: 0.5158 - val_accuracy: 0.7262\n",
            "Epoch 684/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7246 - val_loss: 0.5164 - val_accuracy: 0.7249\n",
            "Epoch 685/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7248 - val_loss: 0.5161 - val_accuracy: 0.7250\n",
            "Epoch 686/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7252 - val_loss: 0.5192 - val_accuracy: 0.7246\n",
            "Epoch 687/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7248 - val_loss: 0.5168 - val_accuracy: 0.7243\n",
            "Epoch 688/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7245 - val_loss: 0.5165 - val_accuracy: 0.7247\n",
            "Epoch 689/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7253 - val_loss: 0.5161 - val_accuracy: 0.7262\n",
            "Epoch 690/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7244 - val_loss: 0.5185 - val_accuracy: 0.7245\n",
            "Epoch 691/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7248 - val_loss: 0.5157 - val_accuracy: 0.7251\n",
            "Epoch 692/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7256 - val_loss: 0.5166 - val_accuracy: 0.7244\n",
            "Epoch 693/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7250 - val_loss: 0.5159 - val_accuracy: 0.7263\n",
            "Epoch 694/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7252 - val_loss: 0.5170 - val_accuracy: 0.7244\n",
            "Epoch 695/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7243 - val_loss: 0.5177 - val_accuracy: 0.7251\n",
            "Epoch 696/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7253 - val_loss: 0.5174 - val_accuracy: 0.7250\n",
            "Epoch 697/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7248 - val_loss: 0.5167 - val_accuracy: 0.7267\n",
            "Epoch 698/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7246 - val_loss: 0.5164 - val_accuracy: 0.7254\n",
            "Epoch 699/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7243 - val_loss: 0.5168 - val_accuracy: 0.7252\n",
            "Epoch 700/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7244 - val_loss: 0.5159 - val_accuracy: 0.7266\n",
            "Epoch 701/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7255 - val_loss: 0.5160 - val_accuracy: 0.7260\n",
            "Epoch 702/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7244 - val_loss: 0.5162 - val_accuracy: 0.7255\n",
            "Epoch 703/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7249 - val_loss: 0.5161 - val_accuracy: 0.7258\n",
            "Epoch 704/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7244 - val_loss: 0.5165 - val_accuracy: 0.7261\n",
            "Epoch 705/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7253 - val_loss: 0.5174 - val_accuracy: 0.7253\n",
            "Epoch 706/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7250 - val_loss: 0.5167 - val_accuracy: 0.7258\n",
            "Epoch 707/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7241 - val_loss: 0.5176 - val_accuracy: 0.7258\n",
            "Epoch 708/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7250 - val_loss: 0.5167 - val_accuracy: 0.7253\n",
            "Epoch 709/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7255 - val_loss: 0.5182 - val_accuracy: 0.7263\n",
            "Epoch 710/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7241 - val_loss: 0.5169 - val_accuracy: 0.7254\n",
            "Epoch 711/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7252 - val_loss: 0.5168 - val_accuracy: 0.7261\n",
            "Epoch 712/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7250 - val_loss: 0.5165 - val_accuracy: 0.7263\n",
            "Epoch 713/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7256 - val_loss: 0.5193 - val_accuracy: 0.7249\n",
            "Epoch 714/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5182 - accuracy: 0.7243 - val_loss: 0.5162 - val_accuracy: 0.7255\n",
            "Epoch 715/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7252 - val_loss: 0.5159 - val_accuracy: 0.7263\n",
            "Epoch 716/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7247 - val_loss: 0.5163 - val_accuracy: 0.7259\n",
            "Epoch 717/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7250 - val_loss: 0.5169 - val_accuracy: 0.7253\n",
            "Epoch 718/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7254 - val_loss: 0.5166 - val_accuracy: 0.7265\n",
            "Epoch 719/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7252 - val_loss: 0.5170 - val_accuracy: 0.7257\n",
            "Epoch 720/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7246 - val_loss: 0.5182 - val_accuracy: 0.7256\n",
            "Epoch 721/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7252 - val_loss: 0.5161 - val_accuracy: 0.7253\n",
            "Epoch 722/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7247 - val_loss: 0.5163 - val_accuracy: 0.7257\n",
            "Epoch 723/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7245 - val_loss: 0.5179 - val_accuracy: 0.7261\n",
            "Epoch 724/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7256 - val_loss: 0.5180 - val_accuracy: 0.7254\n",
            "Epoch 725/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7252 - val_loss: 0.5163 - val_accuracy: 0.7260\n",
            "Epoch 726/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7245 - val_loss: 0.5170 - val_accuracy: 0.7257\n",
            "Epoch 727/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7254 - val_loss: 0.5200 - val_accuracy: 0.7245\n",
            "Epoch 728/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7247 - val_loss: 0.5164 - val_accuracy: 0.7261\n",
            "Epoch 729/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7260 - val_loss: 0.5183 - val_accuracy: 0.7251\n",
            "Epoch 730/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7248 - val_loss: 0.5180 - val_accuracy: 0.7261\n",
            "Epoch 731/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7257 - val_loss: 0.5167 - val_accuracy: 0.7243\n",
            "Epoch 732/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7245 - val_loss: 0.5156 - val_accuracy: 0.7257\n",
            "Epoch 733/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7253 - val_loss: 0.5164 - val_accuracy: 0.7244\n",
            "Epoch 734/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7261 - val_loss: 0.5160 - val_accuracy: 0.7250\n",
            "Epoch 735/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7237 - val_loss: 0.5164 - val_accuracy: 0.7252\n",
            "Epoch 736/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7254 - val_loss: 0.5166 - val_accuracy: 0.7256\n",
            "Epoch 737/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5161 - accuracy: 0.7264 - val_loss: 0.5172 - val_accuracy: 0.7260\n",
            "Epoch 738/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7254 - val_loss: 0.5161 - val_accuracy: 0.7250\n",
            "Epoch 739/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7254 - val_loss: 0.5202 - val_accuracy: 0.7251\n",
            "Epoch 740/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7250 - val_loss: 0.5164 - val_accuracy: 0.7265\n",
            "Epoch 741/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7239 - val_loss: 0.5168 - val_accuracy: 0.7257\n",
            "Epoch 742/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7258 - val_loss: 0.5180 - val_accuracy: 0.7265\n",
            "Epoch 743/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7264 - val_loss: 0.5176 - val_accuracy: 0.7246\n",
            "Epoch 744/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7240 - val_loss: 0.5177 - val_accuracy: 0.7248\n",
            "Epoch 745/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7261 - val_loss: 0.5155 - val_accuracy: 0.7252\n",
            "Epoch 746/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5160 - accuracy: 0.7258 - val_loss: 0.5173 - val_accuracy: 0.7260\n",
            "Epoch 747/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7259 - val_loss: 0.5163 - val_accuracy: 0.7266\n",
            "Epoch 748/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7242 - val_loss: 0.5177 - val_accuracy: 0.7256\n",
            "Epoch 749/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5161 - accuracy: 0.7262 - val_loss: 0.5164 - val_accuracy: 0.7248\n",
            "Epoch 750/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7259 - val_loss: 0.5162 - val_accuracy: 0.7252\n",
            "Epoch 751/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7260 - val_loss: 0.5161 - val_accuracy: 0.7263\n",
            "Epoch 752/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7255 - val_loss: 0.5183 - val_accuracy: 0.7266\n",
            "Epoch 753/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7258 - val_loss: 0.5165 - val_accuracy: 0.7255\n",
            "Epoch 754/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7255 - val_loss: 0.5168 - val_accuracy: 0.7259\n",
            "Epoch 755/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7253 - val_loss: 0.5183 - val_accuracy: 0.7252\n",
            "Epoch 756/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7256 - val_loss: 0.5170 - val_accuracy: 0.7254\n",
            "Epoch 757/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7251 - val_loss: 0.5162 - val_accuracy: 0.7266\n",
            "Epoch 758/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7254 - val_loss: 0.5171 - val_accuracy: 0.7266\n",
            "Epoch 759/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7260 - val_loss: 0.5166 - val_accuracy: 0.7252\n",
            "Epoch 760/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7251 - val_loss: 0.5172 - val_accuracy: 0.7249\n",
            "Epoch 761/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7252 - val_loss: 0.5191 - val_accuracy: 0.7257\n",
            "Epoch 762/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7256 - val_loss: 0.5168 - val_accuracy: 0.7264\n",
            "Epoch 763/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7259 - val_loss: 0.5167 - val_accuracy: 0.7258\n",
            "Epoch 764/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7249 - val_loss: 0.5167 - val_accuracy: 0.7252\n",
            "Epoch 765/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7258 - val_loss: 0.5161 - val_accuracy: 0.7262\n",
            "Epoch 766/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - accuracy: 0.7248 - val_loss: 0.5165 - val_accuracy: 0.7249\n",
            "Epoch 767/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7255 - val_loss: 0.5168 - val_accuracy: 0.7261\n",
            "Epoch 768/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7263 - val_loss: 0.5180 - val_accuracy: 0.7258\n",
            "Epoch 769/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7258 - val_loss: 0.5162 - val_accuracy: 0.7257\n",
            "Epoch 770/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7264 - val_loss: 0.5168 - val_accuracy: 0.7258\n",
            "Epoch 771/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7263 - val_loss: 0.5180 - val_accuracy: 0.7258\n",
            "Epoch 772/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7260 - val_loss: 0.5177 - val_accuracy: 0.7261\n",
            "Epoch 773/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7258 - val_loss: 0.5161 - val_accuracy: 0.7256\n",
            "Epoch 774/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7261 - val_loss: 0.5170 - val_accuracy: 0.7248\n",
            "Epoch 775/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7250 - val_loss: 0.5173 - val_accuracy: 0.7260\n",
            "Epoch 776/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7249 - val_loss: 0.5166 - val_accuracy: 0.7259\n",
            "Epoch 777/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7255 - val_loss: 0.5176 - val_accuracy: 0.7260\n",
            "Epoch 778/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7250 - val_loss: 0.5186 - val_accuracy: 0.7251\n",
            "Epoch 779/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7256 - val_loss: 0.5167 - val_accuracy: 0.7253\n",
            "Epoch 780/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7252 - val_loss: 0.5180 - val_accuracy: 0.7256\n",
            "Epoch 781/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7254 - val_loss: 0.5186 - val_accuracy: 0.7259\n",
            "Epoch 782/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7256 - val_loss: 0.5173 - val_accuracy: 0.7254\n",
            "Epoch 783/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7260 - val_loss: 0.5190 - val_accuracy: 0.7260\n",
            "Epoch 784/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7246 - val_loss: 0.5188 - val_accuracy: 0.7247\n",
            "Epoch 785/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7263 - val_loss: 0.5169 - val_accuracy: 0.7251\n",
            "Epoch 786/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7243 - val_loss: 0.5176 - val_accuracy: 0.7253\n",
            "Epoch 787/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7256 - val_loss: 0.5167 - val_accuracy: 0.7248\n",
            "Epoch 788/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7254 - val_loss: 0.5162 - val_accuracy: 0.7255\n",
            "Epoch 789/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5161 - accuracy: 0.7258 - val_loss: 0.5163 - val_accuracy: 0.7255\n",
            "Epoch 790/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7255 - val_loss: 0.5172 - val_accuracy: 0.7258\n",
            "Epoch 791/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7254 - val_loss: 0.5170 - val_accuracy: 0.7259\n",
            "Epoch 792/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7250 - val_loss: 0.5170 - val_accuracy: 0.7262\n",
            "Epoch 793/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - accuracy: 0.7256 - val_loss: 0.5158 - val_accuracy: 0.7258\n",
            "Epoch 794/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5161 - accuracy: 0.7257 - val_loss: 0.5164 - val_accuracy: 0.7263\n",
            "Epoch 795/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7260 - val_loss: 0.5172 - val_accuracy: 0.7266\n",
            "Epoch 796/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7258 - val_loss: 0.5156 - val_accuracy: 0.7261\n",
            "Epoch 797/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7262 - val_loss: 0.5157 - val_accuracy: 0.7267\n",
            "Epoch 798/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7259 - val_loss: 0.5170 - val_accuracy: 0.7265\n",
            "Epoch 799/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7259 - val_loss: 0.5187 - val_accuracy: 0.7255\n",
            "Epoch 800/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7264 - val_loss: 0.5175 - val_accuracy: 0.7266\n",
            "Epoch 801/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7255 - val_loss: 0.5181 - val_accuracy: 0.7253\n",
            "Epoch 802/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7258 - val_loss: 0.5164 - val_accuracy: 0.7255\n",
            "Epoch 803/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7241 - val_loss: 0.5164 - val_accuracy: 0.7258\n",
            "Epoch 804/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7256 - val_loss: 0.5175 - val_accuracy: 0.7247\n",
            "Epoch 805/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7251 - val_loss: 0.5165 - val_accuracy: 0.7245\n",
            "Epoch 806/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7251 - val_loss: 0.5165 - val_accuracy: 0.7271\n",
            "Epoch 807/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7253 - val_loss: 0.5165 - val_accuracy: 0.7257\n",
            "Epoch 808/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7251 - val_loss: 0.5176 - val_accuracy: 0.7252\n",
            "Epoch 809/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7251 - val_loss: 0.5188 - val_accuracy: 0.7258\n",
            "Epoch 810/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7256 - val_loss: 0.5171 - val_accuracy: 0.7278\n",
            "Epoch 811/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7252 - val_loss: 0.5183 - val_accuracy: 0.7258\n",
            "Epoch 812/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7263 - val_loss: 0.5170 - val_accuracy: 0.7253\n",
            "Epoch 813/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5160 - accuracy: 0.7260 - val_loss: 0.5159 - val_accuracy: 0.7262\n",
            "Epoch 814/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7257 - val_loss: 0.5169 - val_accuracy: 0.7273\n",
            "Epoch 815/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7263 - val_loss: 0.5160 - val_accuracy: 0.7250\n",
            "Epoch 816/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7261 - val_loss: 0.5160 - val_accuracy: 0.7260\n",
            "Epoch 817/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - accuracy: 0.7254 - val_loss: 0.5166 - val_accuracy: 0.7248\n",
            "Epoch 818/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5160 - accuracy: 0.7254 - val_loss: 0.5161 - val_accuracy: 0.7265\n",
            "Epoch 819/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7255 - val_loss: 0.5190 - val_accuracy: 0.7238\n",
            "Epoch 820/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7252 - val_loss: 0.5168 - val_accuracy: 0.7259\n",
            "Epoch 821/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7256 - val_loss: 0.5170 - val_accuracy: 0.7258\n",
            "Epoch 822/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7252 - val_loss: 0.5199 - val_accuracy: 0.7238\n",
            "Epoch 823/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7258 - val_loss: 0.5162 - val_accuracy: 0.7262\n",
            "Epoch 824/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7264 - val_loss: 0.5170 - val_accuracy: 0.7265\n",
            "Epoch 825/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7263 - val_loss: 0.5160 - val_accuracy: 0.7265\n",
            "Epoch 826/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - accuracy: 0.7254 - val_loss: 0.5156 - val_accuracy: 0.7259\n",
            "Epoch 827/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - accuracy: 0.7262 - val_loss: 0.5154 - val_accuracy: 0.7258\n",
            "Epoch 828/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7256 - val_loss: 0.5173 - val_accuracy: 0.7253\n",
            "Epoch 829/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5161 - accuracy: 0.7255 - val_loss: 0.5152 - val_accuracy: 0.7262\n",
            "Epoch 830/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5154 - accuracy: 0.7275 - val_loss: 0.5173 - val_accuracy: 0.7271\n",
            "Epoch 831/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7260 - val_loss: 0.5159 - val_accuracy: 0.7252\n",
            "Epoch 832/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5160 - accuracy: 0.7254 - val_loss: 0.5156 - val_accuracy: 0.7269\n",
            "Epoch 833/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7260 - val_loss: 0.5178 - val_accuracy: 0.7255\n",
            "Epoch 834/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7264 - val_loss: 0.5164 - val_accuracy: 0.7251\n",
            "Epoch 835/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7262 - val_loss: 0.5158 - val_accuracy: 0.7260\n",
            "Epoch 836/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7249 - val_loss: 0.5163 - val_accuracy: 0.7244\n",
            "Epoch 837/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7264 - val_loss: 0.5159 - val_accuracy: 0.7251\n",
            "Epoch 838/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7260 - val_loss: 0.5157 - val_accuracy: 0.7265\n",
            "Epoch 839/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7256 - val_loss: 0.5207 - val_accuracy: 0.7248\n",
            "Epoch 840/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7256 - val_loss: 0.5160 - val_accuracy: 0.7268\n",
            "Epoch 841/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7261 - val_loss: 0.5164 - val_accuracy: 0.7262\n",
            "Epoch 842/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - accuracy: 0.7264 - val_loss: 0.5162 - val_accuracy: 0.7256\n",
            "Epoch 843/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7257 - val_loss: 0.5155 - val_accuracy: 0.7256\n",
            "Epoch 844/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7258 - val_loss: 0.5168 - val_accuracy: 0.7261\n",
            "Epoch 845/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7268 - val_loss: 0.5163 - val_accuracy: 0.7264\n",
            "Epoch 846/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7267 - val_loss: 0.5153 - val_accuracy: 0.7260\n",
            "Epoch 847/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7257 - val_loss: 0.5165 - val_accuracy: 0.7267\n",
            "Epoch 848/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5154 - accuracy: 0.7266 - val_loss: 0.5164 - val_accuracy: 0.7263\n",
            "Epoch 849/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7255 - val_loss: 0.5160 - val_accuracy: 0.7262\n",
            "Epoch 850/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5155 - accuracy: 0.7272 - val_loss: 0.5159 - val_accuracy: 0.7257\n",
            "Epoch 851/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5150 - accuracy: 0.7272 - val_loss: 0.5169 - val_accuracy: 0.7255\n",
            "Epoch 852/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - accuracy: 0.7268 - val_loss: 0.5157 - val_accuracy: 0.7255\n",
            "Epoch 853/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5160 - accuracy: 0.7257 - val_loss: 0.5152 - val_accuracy: 0.7257\n",
            "Epoch 854/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - accuracy: 0.7257 - val_loss: 0.5168 - val_accuracy: 0.7258\n",
            "Epoch 855/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - accuracy: 0.7263 - val_loss: 0.5161 - val_accuracy: 0.7258\n",
            "Epoch 856/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - accuracy: 0.7260 - val_loss: 0.5179 - val_accuracy: 0.7257\n",
            "Epoch 857/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5160 - accuracy: 0.7262 - val_loss: 0.5175 - val_accuracy: 0.7253\n",
            "Epoch 858/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7259 - val_loss: 0.5163 - val_accuracy: 0.7267\n",
            "Epoch 859/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - accuracy: 0.7264 - val_loss: 0.5158 - val_accuracy: 0.7256\n",
            "Epoch 860/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5151 - accuracy: 0.7272 - val_loss: 0.5159 - val_accuracy: 0.7266\n",
            "Epoch 861/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5161 - accuracy: 0.7252 - val_loss: 0.5163 - val_accuracy: 0.7261\n",
            "Epoch 862/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - accuracy: 0.7269 - val_loss: 0.5161 - val_accuracy: 0.7253\n",
            "Epoch 863/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - accuracy: 0.7266 - val_loss: 0.5158 - val_accuracy: 0.7259\n",
            "Epoch 864/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5155 - accuracy: 0.7267 - val_loss: 0.5166 - val_accuracy: 0.7257\n",
            "Epoch 865/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7265 - val_loss: 0.5177 - val_accuracy: 0.7257\n",
            "Epoch 866/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7259 - val_loss: 0.5166 - val_accuracy: 0.7260\n",
            "Epoch 867/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7260 - val_loss: 0.5177 - val_accuracy: 0.7255\n",
            "Epoch 868/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7269 - val_loss: 0.5157 - val_accuracy: 0.7257\n",
            "Epoch 869/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5154 - accuracy: 0.7263 - val_loss: 0.5170 - val_accuracy: 0.7266\n",
            "Epoch 870/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - accuracy: 0.7270 - val_loss: 0.5173 - val_accuracy: 0.7264\n",
            "Epoch 871/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5152 - accuracy: 0.7269 - val_loss: 0.5144 - val_accuracy: 0.7274\n",
            "Epoch 872/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7256 - val_loss: 0.5160 - val_accuracy: 0.7265\n",
            "Epoch 873/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - accuracy: 0.7261 - val_loss: 0.5143 - val_accuracy: 0.7269\n",
            "Epoch 874/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7262 - val_loss: 0.5158 - val_accuracy: 0.7253\n",
            "Epoch 875/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5154 - accuracy: 0.7268 - val_loss: 0.5153 - val_accuracy: 0.7263\n",
            "Epoch 876/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - accuracy: 0.7263 - val_loss: 0.5165 - val_accuracy: 0.7261\n",
            "Epoch 877/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5150 - accuracy: 0.7265 - val_loss: 0.5169 - val_accuracy: 0.7263\n",
            "Epoch 878/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7266 - val_loss: 0.5170 - val_accuracy: 0.7273\n",
            "Epoch 879/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7264 - val_loss: 0.5172 - val_accuracy: 0.7266\n",
            "Epoch 880/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7262 - val_loss: 0.5162 - val_accuracy: 0.7257\n",
            "Epoch 881/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7270 - val_loss: 0.5169 - val_accuracy: 0.7269\n",
            "Epoch 882/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7278 - val_loss: 0.5173 - val_accuracy: 0.7254\n",
            "Epoch 883/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5152 - accuracy: 0.7262 - val_loss: 0.5161 - val_accuracy: 0.7274\n",
            "Epoch 884/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5154 - accuracy: 0.7266 - val_loss: 0.5153 - val_accuracy: 0.7268\n",
            "Epoch 885/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5151 - accuracy: 0.7275 - val_loss: 0.5151 - val_accuracy: 0.7270\n",
            "Epoch 886/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7261 - val_loss: 0.5174 - val_accuracy: 0.7260\n",
            "Epoch 887/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5155 - accuracy: 0.7259 - val_loss: 0.5158 - val_accuracy: 0.7255\n",
            "Epoch 888/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5155 - accuracy: 0.7277 - val_loss: 0.5167 - val_accuracy: 0.7259\n",
            "Epoch 889/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5150 - accuracy: 0.7266 - val_loss: 0.5172 - val_accuracy: 0.7264\n",
            "Epoch 890/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5155 - accuracy: 0.7268 - val_loss: 0.5165 - val_accuracy: 0.7259\n",
            "Epoch 891/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5152 - accuracy: 0.7266 - val_loss: 0.5164 - val_accuracy: 0.7265\n",
            "Epoch 892/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7255 - val_loss: 0.5158 - val_accuracy: 0.7266\n",
            "Epoch 893/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7269 - val_loss: 0.5175 - val_accuracy: 0.7263\n",
            "Epoch 894/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5152 - accuracy: 0.7261 - val_loss: 0.5178 - val_accuracy: 0.7265\n",
            "Epoch 895/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5152 - accuracy: 0.7277 - val_loss: 0.5152 - val_accuracy: 0.7268\n",
            "Epoch 896/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5151 - accuracy: 0.7274 - val_loss: 0.5170 - val_accuracy: 0.7255\n",
            "Epoch 897/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5150 - accuracy: 0.7262 - val_loss: 0.5147 - val_accuracy: 0.7269\n",
            "Epoch 898/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7270 - val_loss: 0.5150 - val_accuracy: 0.7264\n",
            "Epoch 899/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7257 - val_loss: 0.5165 - val_accuracy: 0.7261\n",
            "Epoch 900/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7263 - val_loss: 0.5163 - val_accuracy: 0.7259\n",
            "Epoch 901/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - accuracy: 0.7267 - val_loss: 0.5154 - val_accuracy: 0.7255\n",
            "Epoch 902/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7261 - val_loss: 0.5167 - val_accuracy: 0.7269\n",
            "Epoch 903/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5154 - accuracy: 0.7264 - val_loss: 0.5162 - val_accuracy: 0.7254\n",
            "Epoch 904/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5152 - accuracy: 0.7268 - val_loss: 0.5170 - val_accuracy: 0.7256\n",
            "Epoch 905/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7259 - val_loss: 0.5173 - val_accuracy: 0.7266\n",
            "Epoch 906/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5150 - accuracy: 0.7267 - val_loss: 0.5152 - val_accuracy: 0.7282\n",
            "Epoch 907/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5160 - accuracy: 0.7261 - val_loss: 0.5162 - val_accuracy: 0.7269\n",
            "Epoch 908/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - accuracy: 0.7258 - val_loss: 0.5181 - val_accuracy: 0.7242\n",
            "Epoch 909/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7265 - val_loss: 0.5148 - val_accuracy: 0.7259\n",
            "Epoch 910/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7269 - val_loss: 0.5183 - val_accuracy: 0.7252\n",
            "Epoch 911/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5156 - accuracy: 0.7264 - val_loss: 0.5152 - val_accuracy: 0.7261\n",
            "Epoch 912/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7262 - val_loss: 0.5149 - val_accuracy: 0.7272\n",
            "Epoch 913/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5158 - accuracy: 0.7262 - val_loss: 0.5170 - val_accuracy: 0.7261\n",
            "Epoch 914/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7268 - val_loss: 0.5158 - val_accuracy: 0.7268\n",
            "Epoch 915/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7275 - val_loss: 0.5161 - val_accuracy: 0.7268\n",
            "Epoch 916/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7265 - val_loss: 0.5159 - val_accuracy: 0.7264\n",
            "Epoch 917/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7263 - val_loss: 0.5179 - val_accuracy: 0.7255\n",
            "Epoch 918/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7265 - val_loss: 0.5168 - val_accuracy: 0.7263\n",
            "Epoch 919/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7271 - val_loss: 0.5163 - val_accuracy: 0.7258\n",
            "Epoch 920/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7268 - val_loss: 0.5156 - val_accuracy: 0.7265\n",
            "Epoch 921/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7264 - val_loss: 0.5161 - val_accuracy: 0.7266\n",
            "Epoch 922/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5150 - accuracy: 0.7266 - val_loss: 0.5169 - val_accuracy: 0.7252\n",
            "Epoch 923/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7282 - val_loss: 0.5172 - val_accuracy: 0.7265\n",
            "Epoch 924/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7268 - val_loss: 0.5164 - val_accuracy: 0.7268\n",
            "Epoch 925/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5151 - accuracy: 0.7270 - val_loss: 0.5171 - val_accuracy: 0.7256\n",
            "Epoch 926/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7260 - val_loss: 0.5158 - val_accuracy: 0.7262\n",
            "Epoch 927/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7263 - val_loss: 0.5150 - val_accuracy: 0.7275\n",
            "Epoch 928/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5152 - accuracy: 0.7267 - val_loss: 0.5153 - val_accuracy: 0.7262\n",
            "Epoch 929/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7265 - val_loss: 0.5153 - val_accuracy: 0.7257\n",
            "Epoch 930/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7273 - val_loss: 0.5158 - val_accuracy: 0.7275\n",
            "Epoch 931/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7280 - val_loss: 0.5172 - val_accuracy: 0.7277\n",
            "Epoch 932/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5159 - accuracy: 0.7265 - val_loss: 0.5155 - val_accuracy: 0.7283\n",
            "Epoch 933/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7276 - val_loss: 0.5173 - val_accuracy: 0.7257\n",
            "Epoch 934/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7275 - val_loss: 0.5156 - val_accuracy: 0.7268\n",
            "Epoch 935/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5150 - accuracy: 0.7271 - val_loss: 0.5169 - val_accuracy: 0.7281\n",
            "Epoch 936/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7262 - val_loss: 0.5159 - val_accuracy: 0.7273\n",
            "Epoch 937/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7270 - val_loss: 0.5174 - val_accuracy: 0.7262\n",
            "Epoch 938/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7265 - val_loss: 0.5161 - val_accuracy: 0.7256\n",
            "Epoch 939/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7267 - val_loss: 0.5173 - val_accuracy: 0.7266\n",
            "Epoch 940/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7269 - val_loss: 0.5152 - val_accuracy: 0.7269\n",
            "Epoch 941/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5154 - accuracy: 0.7265 - val_loss: 0.5159 - val_accuracy: 0.7259\n",
            "Epoch 942/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7264 - val_loss: 0.5162 - val_accuracy: 0.7270\n",
            "Epoch 943/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7271 - val_loss: 0.5164 - val_accuracy: 0.7262\n",
            "Epoch 944/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5151 - accuracy: 0.7270 - val_loss: 0.5176 - val_accuracy: 0.7258\n",
            "Epoch 945/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7278 - val_loss: 0.5169 - val_accuracy: 0.7275\n",
            "Epoch 946/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7263 - val_loss: 0.5166 - val_accuracy: 0.7264\n",
            "Epoch 947/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5155 - accuracy: 0.7261 - val_loss: 0.5153 - val_accuracy: 0.7273\n",
            "Epoch 948/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7273 - val_loss: 0.5159 - val_accuracy: 0.7277\n",
            "Epoch 949/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7268 - val_loss: 0.5149 - val_accuracy: 0.7265\n",
            "Epoch 950/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7275 - val_loss: 0.5163 - val_accuracy: 0.7276\n",
            "Epoch 951/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7269 - val_loss: 0.5165 - val_accuracy: 0.7258\n",
            "Epoch 952/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5157 - accuracy: 0.7264 - val_loss: 0.5163 - val_accuracy: 0.7257\n",
            "Epoch 953/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5151 - accuracy: 0.7261 - val_loss: 0.5165 - val_accuracy: 0.7273\n",
            "Epoch 954/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7275 - val_loss: 0.5154 - val_accuracy: 0.7266\n",
            "Epoch 955/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7263 - val_loss: 0.5170 - val_accuracy: 0.7267\n",
            "Epoch 956/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7272 - val_loss: 0.5160 - val_accuracy: 0.7275\n",
            "Epoch 957/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5154 - accuracy: 0.7271 - val_loss: 0.5172 - val_accuracy: 0.7263\n",
            "Epoch 958/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7269 - val_loss: 0.5158 - val_accuracy: 0.7263\n",
            "Epoch 959/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7271 - val_loss: 0.5164 - val_accuracy: 0.7264\n",
            "Epoch 960/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5150 - accuracy: 0.7267 - val_loss: 0.5156 - val_accuracy: 0.7275\n",
            "Epoch 961/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7269 - val_loss: 0.5166 - val_accuracy: 0.7269\n",
            "Epoch 962/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7269 - val_loss: 0.5160 - val_accuracy: 0.7263\n",
            "Epoch 963/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5151 - accuracy: 0.7269 - val_loss: 0.5152 - val_accuracy: 0.7272\n",
            "Epoch 964/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5152 - accuracy: 0.7280 - val_loss: 0.5168 - val_accuracy: 0.7269\n",
            "Epoch 965/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7274 - val_loss: 0.5148 - val_accuracy: 0.7275\n",
            "Epoch 966/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7266 - val_loss: 0.5163 - val_accuracy: 0.7279\n",
            "Epoch 967/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7276 - val_loss: 0.5226 - val_accuracy: 0.7251\n",
            "Epoch 968/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7277 - val_loss: 0.5165 - val_accuracy: 0.7270\n",
            "Epoch 969/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7279 - val_loss: 0.5168 - val_accuracy: 0.7269\n",
            "Epoch 970/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7276 - val_loss: 0.5169 - val_accuracy: 0.7277\n",
            "Epoch 971/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7268 - val_loss: 0.5183 - val_accuracy: 0.7273\n",
            "Epoch 972/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7270 - val_loss: 0.5170 - val_accuracy: 0.7251\n",
            "Epoch 973/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7272 - val_loss: 0.5159 - val_accuracy: 0.7270\n",
            "Epoch 974/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7273 - val_loss: 0.5166 - val_accuracy: 0.7272\n",
            "Epoch 975/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7281 - val_loss: 0.5148 - val_accuracy: 0.7274\n",
            "Epoch 976/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7268 - val_loss: 0.5178 - val_accuracy: 0.7264\n",
            "Epoch 977/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7271 - val_loss: 0.5153 - val_accuracy: 0.7270\n",
            "Epoch 978/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5142 - accuracy: 0.7278 - val_loss: 0.5151 - val_accuracy: 0.7276\n",
            "Epoch 979/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5141 - accuracy: 0.7272 - val_loss: 0.5160 - val_accuracy: 0.7268\n",
            "Epoch 980/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7277 - val_loss: 0.5200 - val_accuracy: 0.7262\n",
            "Epoch 981/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7275 - val_loss: 0.5144 - val_accuracy: 0.7275\n",
            "Epoch 982/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7272 - val_loss: 0.5155 - val_accuracy: 0.7264\n",
            "Epoch 983/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7276 - val_loss: 0.5158 - val_accuracy: 0.7273\n",
            "Epoch 984/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7274 - val_loss: 0.5164 - val_accuracy: 0.7268\n",
            "Epoch 985/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7273 - val_loss: 0.5155 - val_accuracy: 0.7274\n",
            "Epoch 986/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5153 - accuracy: 0.7258 - val_loss: 0.5160 - val_accuracy: 0.7269\n",
            "Epoch 987/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7271 - val_loss: 0.5164 - val_accuracy: 0.7268\n",
            "Epoch 988/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7272 - val_loss: 0.5161 - val_accuracy: 0.7278\n",
            "Epoch 989/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7267 - val_loss: 0.5158 - val_accuracy: 0.7272\n",
            "Epoch 990/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7267 - val_loss: 0.5153 - val_accuracy: 0.7272\n",
            "Epoch 991/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7270 - val_loss: 0.5154 - val_accuracy: 0.7262\n",
            "Epoch 992/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7273 - val_loss: 0.5159 - val_accuracy: 0.7271\n",
            "Epoch 993/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5150 - accuracy: 0.7280 - val_loss: 0.5174 - val_accuracy: 0.7266\n",
            "Epoch 994/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5151 - accuracy: 0.7267 - val_loss: 0.5161 - val_accuracy: 0.7267\n",
            "Epoch 995/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7265 - val_loss: 0.5155 - val_accuracy: 0.7268\n",
            "Epoch 996/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7271 - val_loss: 0.5173 - val_accuracy: 0.7273\n",
            "Epoch 997/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7275 - val_loss: 0.5165 - val_accuracy: 0.7272\n",
            "Epoch 998/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5151 - accuracy: 0.7265 - val_loss: 0.5152 - val_accuracy: 0.7277\n",
            "Epoch 999/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7265 - val_loss: 0.5167 - val_accuracy: 0.7259\n",
            "Epoch 1000/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7270 - val_loss: 0.5169 - val_accuracy: 0.7282\n",
            "Epoch 1001/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7275 - val_loss: 0.5148 - val_accuracy: 0.7273\n",
            "Epoch 1002/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7261 - val_loss: 0.5154 - val_accuracy: 0.7275\n",
            "Epoch 1003/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7272 - val_loss: 0.5162 - val_accuracy: 0.7264\n",
            "Epoch 1004/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5155 - accuracy: 0.7273 - val_loss: 0.5161 - val_accuracy: 0.7270\n",
            "Epoch 1005/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7267 - val_loss: 0.5162 - val_accuracy: 0.7271\n",
            "Epoch 1006/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7275 - val_loss: 0.5166 - val_accuracy: 0.7271\n",
            "Epoch 1007/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5142 - accuracy: 0.7275 - val_loss: 0.5174 - val_accuracy: 0.7264\n",
            "Epoch 1008/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7278 - val_loss: 0.5181 - val_accuracy: 0.7270\n",
            "Epoch 1009/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5151 - accuracy: 0.7255 - val_loss: 0.5161 - val_accuracy: 0.7269\n",
            "Epoch 1010/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5141 - accuracy: 0.7274 - val_loss: 0.5171 - val_accuracy: 0.7267\n",
            "Epoch 1011/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7270 - val_loss: 0.5155 - val_accuracy: 0.7267\n",
            "Epoch 1012/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5142 - accuracy: 0.7268 - val_loss: 0.5160 - val_accuracy: 0.7268\n",
            "Epoch 1013/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5149 - accuracy: 0.7280 - val_loss: 0.5157 - val_accuracy: 0.7267\n",
            "Epoch 1014/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7270 - val_loss: 0.5168 - val_accuracy: 0.7269\n",
            "Epoch 1015/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7278 - val_loss: 0.5165 - val_accuracy: 0.7273\n",
            "Epoch 1016/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7281 - val_loss: 0.5168 - val_accuracy: 0.7263\n",
            "Epoch 1017/2500\n",
            "293/293 [==============================] - 1s 4ms/step - loss: 0.5139 - accuracy: 0.7281 - val_loss: 0.5174 - val_accuracy: 0.7282\n",
            "Epoch 1018/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5142 - accuracy: 0.7266 - val_loss: 0.5172 - val_accuracy: 0.7259\n",
            "Epoch 1019/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5150 - accuracy: 0.7262 - val_loss: 0.5163 - val_accuracy: 0.7260\n",
            "Epoch 1020/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7270 - val_loss: 0.5177 - val_accuracy: 0.7258\n",
            "Epoch 1021/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5142 - accuracy: 0.7272 - val_loss: 0.5156 - val_accuracy: 0.7280\n",
            "Epoch 1022/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5147 - accuracy: 0.7272 - val_loss: 0.5164 - val_accuracy: 0.7264\n",
            "Epoch 1023/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7271 - val_loss: 0.5164 - val_accuracy: 0.7264\n",
            "Epoch 1024/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7270 - val_loss: 0.5166 - val_accuracy: 0.7273\n",
            "Epoch 1025/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7267 - val_loss: 0.5156 - val_accuracy: 0.7270\n",
            "Epoch 1026/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7273 - val_loss: 0.5159 - val_accuracy: 0.7276\n",
            "Epoch 1027/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7267 - val_loss: 0.5180 - val_accuracy: 0.7255\n",
            "Epoch 1028/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7283 - val_loss: 0.5163 - val_accuracy: 0.7267\n",
            "Epoch 1029/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7262 - val_loss: 0.5185 - val_accuracy: 0.7261\n",
            "Epoch 1030/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7271 - val_loss: 0.5164 - val_accuracy: 0.7249\n",
            "Epoch 1031/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7265 - val_loss: 0.5175 - val_accuracy: 0.7270\n",
            "Epoch 1032/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - accuracy: 0.7286 - val_loss: 0.5175 - val_accuracy: 0.7269\n",
            "Epoch 1033/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7276 - val_loss: 0.5171 - val_accuracy: 0.7268\n",
            "Epoch 1034/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7278 - val_loss: 0.5155 - val_accuracy: 0.7267\n",
            "Epoch 1035/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7277 - val_loss: 0.5163 - val_accuracy: 0.7267\n",
            "Epoch 1036/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7285 - val_loss: 0.5177 - val_accuracy: 0.7267\n",
            "Epoch 1037/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7274 - val_loss: 0.5163 - val_accuracy: 0.7262\n",
            "Epoch 1038/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7273 - val_loss: 0.5158 - val_accuracy: 0.7265\n",
            "Epoch 1039/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5141 - accuracy: 0.7266 - val_loss: 0.5157 - val_accuracy: 0.7274\n",
            "Epoch 1040/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7271 - val_loss: 0.5167 - val_accuracy: 0.7272\n",
            "Epoch 1041/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5152 - accuracy: 0.7267 - val_loss: 0.5175 - val_accuracy: 0.7268\n",
            "Epoch 1042/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7270 - val_loss: 0.5161 - val_accuracy: 0.7267\n",
            "Epoch 1043/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7282 - val_loss: 0.5170 - val_accuracy: 0.7274\n",
            "Epoch 1044/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7276 - val_loss: 0.5177 - val_accuracy: 0.7266\n",
            "Epoch 1045/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5141 - accuracy: 0.7274 - val_loss: 0.5150 - val_accuracy: 0.7274\n",
            "Epoch 1046/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7282 - val_loss: 0.5164 - val_accuracy: 0.7278\n",
            "Epoch 1047/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7275 - val_loss: 0.5154 - val_accuracy: 0.7267\n",
            "Epoch 1048/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7282 - val_loss: 0.5161 - val_accuracy: 0.7268\n",
            "Epoch 1049/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7274 - val_loss: 0.5181 - val_accuracy: 0.7270\n",
            "Epoch 1050/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7293 - val_loss: 0.5166 - val_accuracy: 0.7277\n",
            "Epoch 1051/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7280 - val_loss: 0.5159 - val_accuracy: 0.7277\n",
            "Epoch 1052/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7285 - val_loss: 0.5163 - val_accuracy: 0.7266\n",
            "Epoch 1053/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7278 - val_loss: 0.5168 - val_accuracy: 0.7284\n",
            "Epoch 1054/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5141 - accuracy: 0.7276 - val_loss: 0.5172 - val_accuracy: 0.7267\n",
            "Epoch 1055/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - accuracy: 0.7269 - val_loss: 0.5170 - val_accuracy: 0.7265\n",
            "Epoch 1056/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - accuracy: 0.7277 - val_loss: 0.5155 - val_accuracy: 0.7273\n",
            "Epoch 1057/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7282 - val_loss: 0.5162 - val_accuracy: 0.7282\n",
            "Epoch 1058/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7277 - val_loss: 0.5156 - val_accuracy: 0.7276\n",
            "Epoch 1059/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7273 - val_loss: 0.5153 - val_accuracy: 0.7280\n",
            "Epoch 1060/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7277 - val_loss: 0.5163 - val_accuracy: 0.7275\n",
            "Epoch 1061/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7275 - val_loss: 0.5154 - val_accuracy: 0.7280\n",
            "Epoch 1062/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7278 - val_loss: 0.5175 - val_accuracy: 0.7279\n",
            "Epoch 1063/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7263 - val_loss: 0.5167 - val_accuracy: 0.7280\n",
            "Epoch 1064/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7281 - val_loss: 0.5158 - val_accuracy: 0.7272\n",
            "Epoch 1065/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5142 - accuracy: 0.7279 - val_loss: 0.5151 - val_accuracy: 0.7282\n",
            "Epoch 1066/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7271 - val_loss: 0.5151 - val_accuracy: 0.7266\n",
            "Epoch 1067/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7267 - val_loss: 0.5166 - val_accuracy: 0.7269\n",
            "Epoch 1068/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5148 - accuracy: 0.7266 - val_loss: 0.5164 - val_accuracy: 0.7272\n",
            "Epoch 1069/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5142 - accuracy: 0.7268 - val_loss: 0.5156 - val_accuracy: 0.7267\n",
            "Epoch 1070/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5142 - accuracy: 0.7275 - val_loss: 0.5167 - val_accuracy: 0.7276\n",
            "Epoch 1071/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7280 - val_loss: 0.5185 - val_accuracy: 0.7269\n",
            "Epoch 1072/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5141 - accuracy: 0.7270 - val_loss: 0.5163 - val_accuracy: 0.7270\n",
            "Epoch 1073/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7275 - val_loss: 0.5159 - val_accuracy: 0.7283\n",
            "Epoch 1074/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7277 - val_loss: 0.5161 - val_accuracy: 0.7274\n",
            "Epoch 1075/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7280 - val_loss: 0.5161 - val_accuracy: 0.7269\n",
            "Epoch 1076/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7277 - val_loss: 0.5162 - val_accuracy: 0.7280\n",
            "Epoch 1077/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7267 - val_loss: 0.5160 - val_accuracy: 0.7264\n",
            "Epoch 1078/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7274 - val_loss: 0.5160 - val_accuracy: 0.7266\n",
            "Epoch 1079/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7276 - val_loss: 0.5156 - val_accuracy: 0.7279\n",
            "Epoch 1080/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5141 - accuracy: 0.7277 - val_loss: 0.5163 - val_accuracy: 0.7276\n",
            "Epoch 1081/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7273 - val_loss: 0.5162 - val_accuracy: 0.7275\n",
            "Epoch 1082/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7269 - val_loss: 0.5189 - val_accuracy: 0.7267\n",
            "Epoch 1083/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7271 - val_loss: 0.5176 - val_accuracy: 0.7274\n",
            "Epoch 1084/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7275 - val_loss: 0.5170 - val_accuracy: 0.7282\n",
            "Epoch 1085/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5142 - accuracy: 0.7275 - val_loss: 0.5172 - val_accuracy: 0.7282\n",
            "Epoch 1086/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7275 - val_loss: 0.5156 - val_accuracy: 0.7286\n",
            "Epoch 1087/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7281 - val_loss: 0.5183 - val_accuracy: 0.7268\n",
            "Epoch 1088/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7275 - val_loss: 0.5175 - val_accuracy: 0.7280\n",
            "Epoch 1089/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7273 - val_loss: 0.5156 - val_accuracy: 0.7280\n",
            "Epoch 1090/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - accuracy: 0.7276 - val_loss: 0.5171 - val_accuracy: 0.7266\n",
            "Epoch 1091/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7272 - val_loss: 0.5163 - val_accuracy: 0.7273\n",
            "Epoch 1092/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7289 - val_loss: 0.5161 - val_accuracy: 0.7276\n",
            "Epoch 1093/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5146 - accuracy: 0.7276 - val_loss: 0.5156 - val_accuracy: 0.7282\n",
            "Epoch 1094/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7284 - val_loss: 0.5165 - val_accuracy: 0.7274\n",
            "Epoch 1095/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7273 - val_loss: 0.5178 - val_accuracy: 0.7274\n",
            "Epoch 1096/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7274 - val_loss: 0.5158 - val_accuracy: 0.7271\n",
            "Epoch 1097/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7282 - val_loss: 0.5188 - val_accuracy: 0.7271\n",
            "Epoch 1098/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7272 - val_loss: 0.5162 - val_accuracy: 0.7274\n",
            "Epoch 1099/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7279 - val_loss: 0.5177 - val_accuracy: 0.7270\n",
            "Epoch 1100/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7277 - val_loss: 0.5156 - val_accuracy: 0.7275\n",
            "Epoch 1101/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7276 - val_loss: 0.5169 - val_accuracy: 0.7267\n",
            "Epoch 1102/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5142 - accuracy: 0.7267 - val_loss: 0.5167 - val_accuracy: 0.7274\n",
            "Epoch 1103/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - accuracy: 0.7278 - val_loss: 0.5171 - val_accuracy: 0.7266\n",
            "Epoch 1104/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7282 - val_loss: 0.5155 - val_accuracy: 0.7276\n",
            "Epoch 1105/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7280 - val_loss: 0.5174 - val_accuracy: 0.7266\n",
            "Epoch 1106/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - accuracy: 0.7281 - val_loss: 0.5169 - val_accuracy: 0.7270\n",
            "Epoch 1107/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7273 - val_loss: 0.5191 - val_accuracy: 0.7262\n",
            "Epoch 1108/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7269 - val_loss: 0.5160 - val_accuracy: 0.7273\n",
            "Epoch 1109/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7276 - val_loss: 0.5179 - val_accuracy: 0.7263\n",
            "Epoch 1110/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7282 - val_loss: 0.5154 - val_accuracy: 0.7278\n",
            "Epoch 1111/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7277 - val_loss: 0.5159 - val_accuracy: 0.7278\n",
            "Epoch 1112/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7284 - val_loss: 0.5167 - val_accuracy: 0.7272\n",
            "Epoch 1113/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7277 - val_loss: 0.5166 - val_accuracy: 0.7274\n",
            "Epoch 1114/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7295 - val_loss: 0.5157 - val_accuracy: 0.7276\n",
            "Epoch 1115/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7278 - val_loss: 0.5197 - val_accuracy: 0.7268\n",
            "Epoch 1116/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7283 - val_loss: 0.5162 - val_accuracy: 0.7275\n",
            "Epoch 1117/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7281 - val_loss: 0.5169 - val_accuracy: 0.7269\n",
            "Epoch 1118/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7271 - val_loss: 0.5178 - val_accuracy: 0.7269\n",
            "Epoch 1119/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7275 - val_loss: 0.5169 - val_accuracy: 0.7283\n",
            "Epoch 1120/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7272 - val_loss: 0.5175 - val_accuracy: 0.7265\n",
            "Epoch 1121/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5144 - accuracy: 0.7279 - val_loss: 0.5174 - val_accuracy: 0.7279\n",
            "Epoch 1122/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7282 - val_loss: 0.5163 - val_accuracy: 0.7280\n",
            "Epoch 1123/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - accuracy: 0.7278 - val_loss: 0.5166 - val_accuracy: 0.7280\n",
            "Epoch 1124/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7295 - val_loss: 0.5163 - val_accuracy: 0.7262\n",
            "Epoch 1125/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7270 - val_loss: 0.5160 - val_accuracy: 0.7286\n",
            "Epoch 1126/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7278 - val_loss: 0.5153 - val_accuracy: 0.7270\n",
            "Epoch 1127/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7276 - val_loss: 0.5162 - val_accuracy: 0.7276\n",
            "Epoch 1128/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7285 - val_loss: 0.5161 - val_accuracy: 0.7285\n",
            "Epoch 1129/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7281 - val_loss: 0.5168 - val_accuracy: 0.7273\n",
            "Epoch 1130/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7273 - val_loss: 0.5174 - val_accuracy: 0.7267\n",
            "Epoch 1131/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7281 - val_loss: 0.5172 - val_accuracy: 0.7279\n",
            "Epoch 1132/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7282 - val_loss: 0.5160 - val_accuracy: 0.7274\n",
            "Epoch 1133/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7272 - val_loss: 0.5167 - val_accuracy: 0.7279\n",
            "Epoch 1134/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5143 - accuracy: 0.7272 - val_loss: 0.5160 - val_accuracy: 0.7271\n",
            "Epoch 1135/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7274 - val_loss: 0.5166 - val_accuracy: 0.7270\n",
            "Epoch 1136/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7278 - val_loss: 0.5174 - val_accuracy: 0.7267\n",
            "Epoch 1137/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7278 - val_loss: 0.5169 - val_accuracy: 0.7283\n",
            "Epoch 1138/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - accuracy: 0.7285 - val_loss: 0.5163 - val_accuracy: 0.7285\n",
            "Epoch 1139/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7283 - val_loss: 0.5159 - val_accuracy: 0.7270\n",
            "Epoch 1140/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7273 - val_loss: 0.5184 - val_accuracy: 0.7264\n",
            "Epoch 1141/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7274 - val_loss: 0.5170 - val_accuracy: 0.7267\n",
            "Epoch 1142/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7282 - val_loss: 0.5172 - val_accuracy: 0.7282\n",
            "Epoch 1143/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7269 - val_loss: 0.5178 - val_accuracy: 0.7270\n",
            "Epoch 1144/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7282 - val_loss: 0.5170 - val_accuracy: 0.7266\n",
            "Epoch 1145/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7283 - val_loss: 0.5159 - val_accuracy: 0.7278\n",
            "Epoch 1146/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7283 - val_loss: 0.5156 - val_accuracy: 0.7287\n",
            "Epoch 1147/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7290 - val_loss: 0.5173 - val_accuracy: 0.7278\n",
            "Epoch 1148/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7282 - val_loss: 0.5162 - val_accuracy: 0.7284\n",
            "Epoch 1149/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7279 - val_loss: 0.5172 - val_accuracy: 0.7277\n",
            "Epoch 1150/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7281 - val_loss: 0.5159 - val_accuracy: 0.7287\n",
            "Epoch 1151/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7280 - val_loss: 0.5186 - val_accuracy: 0.7278\n",
            "Epoch 1152/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7283 - val_loss: 0.5172 - val_accuracy: 0.7266\n",
            "Epoch 1153/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7277 - val_loss: 0.5162 - val_accuracy: 0.7284\n",
            "Epoch 1154/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7285 - val_loss: 0.5189 - val_accuracy: 0.7269\n",
            "Epoch 1155/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7284 - val_loss: 0.5157 - val_accuracy: 0.7281\n",
            "Epoch 1156/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7282 - val_loss: 0.5169 - val_accuracy: 0.7274\n",
            "Epoch 1157/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7273 - val_loss: 0.5158 - val_accuracy: 0.7288\n",
            "Epoch 1158/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7272 - val_loss: 0.5161 - val_accuracy: 0.7280\n",
            "Epoch 1159/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7284 - val_loss: 0.5163 - val_accuracy: 0.7277\n",
            "Epoch 1160/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7283 - val_loss: 0.5171 - val_accuracy: 0.7279\n",
            "Epoch 1161/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7289 - val_loss: 0.5171 - val_accuracy: 0.7275\n",
            "Epoch 1162/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7278 - val_loss: 0.5181 - val_accuracy: 0.7261\n",
            "Epoch 1163/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7283 - val_loss: 0.5165 - val_accuracy: 0.7282\n",
            "Epoch 1164/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7280 - val_loss: 0.5182 - val_accuracy: 0.7275\n",
            "Epoch 1165/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7279 - val_loss: 0.5161 - val_accuracy: 0.7274\n",
            "Epoch 1166/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7281 - val_loss: 0.5167 - val_accuracy: 0.7267\n",
            "Epoch 1167/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7282 - val_loss: 0.5156 - val_accuracy: 0.7272\n",
            "Epoch 1168/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7287 - val_loss: 0.5157 - val_accuracy: 0.7284\n",
            "Epoch 1169/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7277 - val_loss: 0.5174 - val_accuracy: 0.7269\n",
            "Epoch 1170/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7284 - val_loss: 0.5158 - val_accuracy: 0.7280\n",
            "Epoch 1171/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7282 - val_loss: 0.5174 - val_accuracy: 0.7278\n",
            "Epoch 1172/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7288 - val_loss: 0.5167 - val_accuracy: 0.7285\n",
            "Epoch 1173/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7285 - val_loss: 0.5169 - val_accuracy: 0.7286\n",
            "Epoch 1174/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7289 - val_loss: 0.5175 - val_accuracy: 0.7279\n",
            "Epoch 1175/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7287 - val_loss: 0.5161 - val_accuracy: 0.7268\n",
            "Epoch 1176/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7278 - val_loss: 0.5153 - val_accuracy: 0.7274\n",
            "Epoch 1177/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7275 - val_loss: 0.5167 - val_accuracy: 0.7282\n",
            "Epoch 1178/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7277 - val_loss: 0.5165 - val_accuracy: 0.7284\n",
            "Epoch 1179/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7281 - val_loss: 0.5170 - val_accuracy: 0.7275\n",
            "Epoch 1180/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7287 - val_loss: 0.5162 - val_accuracy: 0.7270\n",
            "Epoch 1181/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7281 - val_loss: 0.5165 - val_accuracy: 0.7270\n",
            "Epoch 1182/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7282 - val_loss: 0.5159 - val_accuracy: 0.7272\n",
            "Epoch 1183/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7279 - val_loss: 0.5165 - val_accuracy: 0.7286\n",
            "Epoch 1184/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7278 - val_loss: 0.5161 - val_accuracy: 0.7287\n",
            "Epoch 1185/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7274 - val_loss: 0.5159 - val_accuracy: 0.7274\n",
            "Epoch 1186/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7273 - val_loss: 0.5160 - val_accuracy: 0.7272\n",
            "Epoch 1187/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7277 - val_loss: 0.5160 - val_accuracy: 0.7281\n",
            "Epoch 1188/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7282 - val_loss: 0.5161 - val_accuracy: 0.7281\n",
            "Epoch 1189/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7286 - val_loss: 0.5155 - val_accuracy: 0.7286\n",
            "Epoch 1190/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7278 - val_loss: 0.5158 - val_accuracy: 0.7285\n",
            "Epoch 1191/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7288 - val_loss: 0.5165 - val_accuracy: 0.7279\n",
            "Epoch 1192/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7285 - val_loss: 0.5169 - val_accuracy: 0.7287\n",
            "Epoch 1193/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7281 - val_loss: 0.5164 - val_accuracy: 0.7283\n",
            "Epoch 1194/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7278 - val_loss: 0.5159 - val_accuracy: 0.7288\n",
            "Epoch 1195/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7292 - val_loss: 0.5165 - val_accuracy: 0.7289\n",
            "Epoch 1196/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7281 - val_loss: 0.5157 - val_accuracy: 0.7281\n",
            "Epoch 1197/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7281 - val_loss: 0.5163 - val_accuracy: 0.7279\n",
            "Epoch 1198/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7287 - val_loss: 0.5152 - val_accuracy: 0.7295\n",
            "Epoch 1199/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5140 - accuracy: 0.7274 - val_loss: 0.5165 - val_accuracy: 0.7287\n",
            "Epoch 1200/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7278 - val_loss: 0.5154 - val_accuracy: 0.7295\n",
            "Epoch 1201/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7278 - val_loss: 0.5165 - val_accuracy: 0.7276\n",
            "Epoch 1202/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7282 - val_loss: 0.5157 - val_accuracy: 0.7291\n",
            "Epoch 1203/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7284 - val_loss: 0.5174 - val_accuracy: 0.7272\n",
            "Epoch 1204/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7272 - val_loss: 0.5176 - val_accuracy: 0.7274\n",
            "Epoch 1205/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7276 - val_loss: 0.5176 - val_accuracy: 0.7279\n",
            "Epoch 1206/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7278 - val_loss: 0.5157 - val_accuracy: 0.7292\n",
            "Epoch 1207/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7279 - val_loss: 0.5164 - val_accuracy: 0.7269\n",
            "Epoch 1208/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7280 - val_loss: 0.5171 - val_accuracy: 0.7281\n",
            "Epoch 1209/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7275 - val_loss: 0.5159 - val_accuracy: 0.7277\n",
            "Epoch 1210/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7283 - val_loss: 0.5168 - val_accuracy: 0.7272\n",
            "Epoch 1211/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7276 - val_loss: 0.5157 - val_accuracy: 0.7292\n",
            "Epoch 1212/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7279 - val_loss: 0.5159 - val_accuracy: 0.7282\n",
            "Epoch 1213/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7276 - val_loss: 0.5163 - val_accuracy: 0.7282\n",
            "Epoch 1214/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7284 - val_loss: 0.5178 - val_accuracy: 0.7274\n",
            "Epoch 1215/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7292 - val_loss: 0.5194 - val_accuracy: 0.7281\n",
            "Epoch 1216/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7281 - val_loss: 0.5170 - val_accuracy: 0.7286\n",
            "Epoch 1217/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5142 - accuracy: 0.7261 - val_loss: 0.5166 - val_accuracy: 0.7270\n",
            "Epoch 1218/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7287 - val_loss: 0.5169 - val_accuracy: 0.7274\n",
            "Epoch 1219/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7273 - val_loss: 0.5169 - val_accuracy: 0.7274\n",
            "Epoch 1220/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7283 - val_loss: 0.5164 - val_accuracy: 0.7282\n",
            "Epoch 1221/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7289 - val_loss: 0.5150 - val_accuracy: 0.7284\n",
            "Epoch 1222/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7288 - val_loss: 0.5175 - val_accuracy: 0.7274\n",
            "Epoch 1223/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7276 - val_loss: 0.5158 - val_accuracy: 0.7289\n",
            "Epoch 1224/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7284 - val_loss: 0.5157 - val_accuracy: 0.7279\n",
            "Epoch 1225/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7282 - val_loss: 0.5148 - val_accuracy: 0.7291\n",
            "Epoch 1226/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7282 - val_loss: 0.5157 - val_accuracy: 0.7286\n",
            "Epoch 1227/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7287 - val_loss: 0.5148 - val_accuracy: 0.7299\n",
            "Epoch 1228/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7278 - val_loss: 0.5175 - val_accuracy: 0.7285\n",
            "Epoch 1229/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7288 - val_loss: 0.5171 - val_accuracy: 0.7276\n",
            "Epoch 1230/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7283 - val_loss: 0.5169 - val_accuracy: 0.7290\n",
            "Epoch 1231/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7293 - val_loss: 0.5160 - val_accuracy: 0.7286\n",
            "Epoch 1232/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7287 - val_loss: 0.5168 - val_accuracy: 0.7280\n",
            "Epoch 1233/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7279 - val_loss: 0.5161 - val_accuracy: 0.7281\n",
            "Epoch 1234/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5138 - accuracy: 0.7282 - val_loss: 0.5161 - val_accuracy: 0.7274\n",
            "Epoch 1235/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7293 - val_loss: 0.5169 - val_accuracy: 0.7279\n",
            "Epoch 1236/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7290 - val_loss: 0.5162 - val_accuracy: 0.7297\n",
            "Epoch 1237/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7286 - val_loss: 0.5151 - val_accuracy: 0.7293\n",
            "Epoch 1238/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7276 - val_loss: 0.5173 - val_accuracy: 0.7291\n",
            "Epoch 1239/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7285 - val_loss: 0.5152 - val_accuracy: 0.7284\n",
            "Epoch 1240/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7295 - val_loss: 0.5172 - val_accuracy: 0.7274\n",
            "Epoch 1241/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7278 - val_loss: 0.5177 - val_accuracy: 0.7279\n",
            "Epoch 1242/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7288 - val_loss: 0.5163 - val_accuracy: 0.7283\n",
            "Epoch 1243/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7284 - val_loss: 0.5182 - val_accuracy: 0.7275\n",
            "Epoch 1244/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7293 - val_loss: 0.5170 - val_accuracy: 0.7275\n",
            "Epoch 1245/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7282 - val_loss: 0.5165 - val_accuracy: 0.7293\n",
            "Epoch 1246/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7279 - val_loss: 0.5165 - val_accuracy: 0.7282\n",
            "Epoch 1247/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7292 - val_loss: 0.5162 - val_accuracy: 0.7287\n",
            "Epoch 1248/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7276 - val_loss: 0.5166 - val_accuracy: 0.7277\n",
            "Epoch 1249/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7284 - val_loss: 0.5173 - val_accuracy: 0.7281\n",
            "Epoch 1250/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7284 - val_loss: 0.5169 - val_accuracy: 0.7285\n",
            "Epoch 1251/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7294 - val_loss: 0.5164 - val_accuracy: 0.7291\n",
            "Epoch 1252/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7277 - val_loss: 0.5170 - val_accuracy: 0.7287\n",
            "Epoch 1253/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5137 - accuracy: 0.7272 - val_loss: 0.5183 - val_accuracy: 0.7281\n",
            "Epoch 1254/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7286 - val_loss: 0.5167 - val_accuracy: 0.7286\n",
            "Epoch 1255/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7286 - val_loss: 0.5163 - val_accuracy: 0.7286\n",
            "Epoch 1256/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7293 - val_loss: 0.5158 - val_accuracy: 0.7286\n",
            "Epoch 1257/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7278 - val_loss: 0.5166 - val_accuracy: 0.7272\n",
            "Epoch 1258/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7282 - val_loss: 0.5180 - val_accuracy: 0.7277\n",
            "Epoch 1259/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7286 - val_loss: 0.5171 - val_accuracy: 0.7265\n",
            "Epoch 1260/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7274 - val_loss: 0.5178 - val_accuracy: 0.7282\n",
            "Epoch 1261/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5135 - accuracy: 0.7277 - val_loss: 0.5184 - val_accuracy: 0.7275\n",
            "Epoch 1262/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7290 - val_loss: 0.5170 - val_accuracy: 0.7287\n",
            "Epoch 1263/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7281 - val_loss: 0.5147 - val_accuracy: 0.7289\n",
            "Epoch 1264/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7291 - val_loss: 0.5172 - val_accuracy: 0.7285\n",
            "Epoch 1265/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7280 - val_loss: 0.5158 - val_accuracy: 0.7274\n",
            "Epoch 1266/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7289 - val_loss: 0.5172 - val_accuracy: 0.7284\n",
            "Epoch 1267/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7289 - val_loss: 0.5168 - val_accuracy: 0.7284\n",
            "Epoch 1268/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7279 - val_loss: 0.5169 - val_accuracy: 0.7283\n",
            "Epoch 1269/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7283 - val_loss: 0.5162 - val_accuracy: 0.7285\n",
            "Epoch 1270/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7283 - val_loss: 0.5188 - val_accuracy: 0.7272\n",
            "Epoch 1271/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7290 - val_loss: 0.5171 - val_accuracy: 0.7282\n",
            "Epoch 1272/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7291 - val_loss: 0.5165 - val_accuracy: 0.7278\n",
            "Epoch 1273/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7271 - val_loss: 0.5191 - val_accuracy: 0.7274\n",
            "Epoch 1274/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7283 - val_loss: 0.5166 - val_accuracy: 0.7282\n",
            "Epoch 1275/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7276 - val_loss: 0.5160 - val_accuracy: 0.7294\n",
            "Epoch 1276/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7280 - val_loss: 0.5162 - val_accuracy: 0.7292\n",
            "Epoch 1277/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7285 - val_loss: 0.5155 - val_accuracy: 0.7286\n",
            "Epoch 1278/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7290 - val_loss: 0.5165 - val_accuracy: 0.7283\n",
            "Epoch 1279/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7277 - val_loss: 0.5173 - val_accuracy: 0.7289\n",
            "Epoch 1280/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7282 - val_loss: 0.5179 - val_accuracy: 0.7279\n",
            "Epoch 1281/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7278 - val_loss: 0.5176 - val_accuracy: 0.7278\n",
            "Epoch 1282/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7289 - val_loss: 0.5182 - val_accuracy: 0.7282\n",
            "Epoch 1283/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7287 - val_loss: 0.5179 - val_accuracy: 0.7276\n",
            "Epoch 1284/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7277 - val_loss: 0.5177 - val_accuracy: 0.7276\n",
            "Epoch 1285/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7284 - val_loss: 0.5179 - val_accuracy: 0.7278\n",
            "Epoch 1286/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7290 - val_loss: 0.5160 - val_accuracy: 0.7282\n",
            "Epoch 1287/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7292 - val_loss: 0.5172 - val_accuracy: 0.7272\n",
            "Epoch 1288/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7288 - val_loss: 0.5176 - val_accuracy: 0.7279\n",
            "Epoch 1289/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7283 - val_loss: 0.5163 - val_accuracy: 0.7285\n",
            "Epoch 1290/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7290 - val_loss: 0.5183 - val_accuracy: 0.7278\n",
            "Epoch 1291/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7289 - val_loss: 0.5170 - val_accuracy: 0.7279\n",
            "Epoch 1292/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7284 - val_loss: 0.5179 - val_accuracy: 0.7281\n",
            "Epoch 1293/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7302 - val_loss: 0.5185 - val_accuracy: 0.7277\n",
            "Epoch 1294/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7280 - val_loss: 0.5187 - val_accuracy: 0.7279\n",
            "Epoch 1295/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7278 - val_loss: 0.5172 - val_accuracy: 0.7279\n",
            "Epoch 1296/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7275 - val_loss: 0.5190 - val_accuracy: 0.7287\n",
            "Epoch 1297/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7296 - val_loss: 0.5180 - val_accuracy: 0.7280\n",
            "Epoch 1298/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5131 - accuracy: 0.7278 - val_loss: 0.5173 - val_accuracy: 0.7287\n",
            "Epoch 1299/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7287 - val_loss: 0.5161 - val_accuracy: 0.7286\n",
            "Epoch 1300/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7291 - val_loss: 0.5172 - val_accuracy: 0.7277\n",
            "Epoch 1301/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7284 - val_loss: 0.5168 - val_accuracy: 0.7284\n",
            "Epoch 1302/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7285 - val_loss: 0.5204 - val_accuracy: 0.7273\n",
            "Epoch 1303/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7288 - val_loss: 0.5175 - val_accuracy: 0.7290\n",
            "Epoch 1304/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7293 - val_loss: 0.5174 - val_accuracy: 0.7284\n",
            "Epoch 1305/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7292 - val_loss: 0.5170 - val_accuracy: 0.7285\n",
            "Epoch 1306/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.7287 - val_loss: 0.5166 - val_accuracy: 0.7287\n",
            "Epoch 1307/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7287 - val_loss: 0.5187 - val_accuracy: 0.7283\n",
            "Epoch 1308/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7280 - val_loss: 0.5164 - val_accuracy: 0.7282\n",
            "Epoch 1309/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7293 - val_loss: 0.5169 - val_accuracy: 0.7275\n",
            "Epoch 1310/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7284 - val_loss: 0.5165 - val_accuracy: 0.7293\n",
            "Epoch 1311/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7291 - val_loss: 0.5162 - val_accuracy: 0.7287\n",
            "Epoch 1312/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7280 - val_loss: 0.5174 - val_accuracy: 0.7291\n",
            "Epoch 1313/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7278 - val_loss: 0.5169 - val_accuracy: 0.7289\n",
            "Epoch 1314/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7293 - val_loss: 0.5158 - val_accuracy: 0.7282\n",
            "Epoch 1315/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7287 - val_loss: 0.5169 - val_accuracy: 0.7285\n",
            "Epoch 1316/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7281 - val_loss: 0.5179 - val_accuracy: 0.7283\n",
            "Epoch 1317/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7282 - val_loss: 0.5169 - val_accuracy: 0.7283\n",
            "Epoch 1318/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7286 - val_loss: 0.5165 - val_accuracy: 0.7272\n",
            "Epoch 1319/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7283 - val_loss: 0.5169 - val_accuracy: 0.7284\n",
            "Epoch 1320/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7288 - val_loss: 0.5195 - val_accuracy: 0.7281\n",
            "Epoch 1321/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7295 - val_loss: 0.5166 - val_accuracy: 0.7287\n",
            "Epoch 1322/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7290 - val_loss: 0.5187 - val_accuracy: 0.7278\n",
            "Epoch 1323/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7293 - val_loss: 0.5165 - val_accuracy: 0.7285\n",
            "Epoch 1324/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7293 - val_loss: 0.5183 - val_accuracy: 0.7273\n",
            "Epoch 1325/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7285 - val_loss: 0.5176 - val_accuracy: 0.7287\n",
            "Epoch 1326/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7286 - val_loss: 0.5166 - val_accuracy: 0.7290\n",
            "Epoch 1327/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7290 - val_loss: 0.5170 - val_accuracy: 0.7287\n",
            "Epoch 1328/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7284 - val_loss: 0.5163 - val_accuracy: 0.7284\n",
            "Epoch 1329/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7292 - val_loss: 0.5174 - val_accuracy: 0.7288\n",
            "Epoch 1330/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7286 - val_loss: 0.5171 - val_accuracy: 0.7281\n",
            "Epoch 1331/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7284 - val_loss: 0.5172 - val_accuracy: 0.7282\n",
            "Epoch 1332/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7288 - val_loss: 0.5183 - val_accuracy: 0.7284\n",
            "Epoch 1333/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7275 - val_loss: 0.5164 - val_accuracy: 0.7288\n",
            "Epoch 1334/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7286 - val_loss: 0.5173 - val_accuracy: 0.7284\n",
            "Epoch 1335/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7282 - val_loss: 0.5167 - val_accuracy: 0.7280\n",
            "Epoch 1336/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7273 - val_loss: 0.5176 - val_accuracy: 0.7279\n",
            "Epoch 1337/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7287 - val_loss: 0.5166 - val_accuracy: 0.7286\n",
            "Epoch 1338/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7285 - val_loss: 0.5165 - val_accuracy: 0.7275\n",
            "Epoch 1339/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7288 - val_loss: 0.5181 - val_accuracy: 0.7282\n",
            "Epoch 1340/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7287 - val_loss: 0.5159 - val_accuracy: 0.7295\n",
            "Epoch 1341/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7280 - val_loss: 0.5186 - val_accuracy: 0.7277\n",
            "Epoch 1342/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7278 - val_loss: 0.5195 - val_accuracy: 0.7267\n",
            "Epoch 1343/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7292 - val_loss: 0.5175 - val_accuracy: 0.7283\n",
            "Epoch 1344/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7283 - val_loss: 0.5173 - val_accuracy: 0.7279\n",
            "Epoch 1345/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7282 - val_loss: 0.5173 - val_accuracy: 0.7286\n",
            "Epoch 1346/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7286 - val_loss: 0.5179 - val_accuracy: 0.7283\n",
            "Epoch 1347/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7289 - val_loss: 0.5174 - val_accuracy: 0.7281\n",
            "Epoch 1348/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7287 - val_loss: 0.5169 - val_accuracy: 0.7292\n",
            "Epoch 1349/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7285 - val_loss: 0.5189 - val_accuracy: 0.7282\n",
            "Epoch 1350/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7287 - val_loss: 0.5170 - val_accuracy: 0.7286\n",
            "Epoch 1351/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7280 - val_loss: 0.5180 - val_accuracy: 0.7284\n",
            "Epoch 1352/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7293 - val_loss: 0.5168 - val_accuracy: 0.7272\n",
            "Epoch 1353/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5133 - accuracy: 0.7287 - val_loss: 0.5181 - val_accuracy: 0.7285\n",
            "Epoch 1354/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7282 - val_loss: 0.5155 - val_accuracy: 0.7293\n",
            "Epoch 1355/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7280 - val_loss: 0.5174 - val_accuracy: 0.7286\n",
            "Epoch 1356/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7277 - val_loss: 0.5178 - val_accuracy: 0.7286\n",
            "Epoch 1357/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7294 - val_loss: 0.5175 - val_accuracy: 0.7286\n",
            "Epoch 1358/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7287 - val_loss: 0.5182 - val_accuracy: 0.7276\n",
            "Epoch 1359/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7278 - val_loss: 0.5179 - val_accuracy: 0.7282\n",
            "Epoch 1360/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7290 - val_loss: 0.5167 - val_accuracy: 0.7288\n",
            "Epoch 1361/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5132 - accuracy: 0.7291 - val_loss: 0.5162 - val_accuracy: 0.7283\n",
            "Epoch 1362/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7284 - val_loss: 0.5154 - val_accuracy: 0.7296\n",
            "Epoch 1363/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7277 - val_loss: 0.5186 - val_accuracy: 0.7272\n",
            "Epoch 1364/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7287 - val_loss: 0.5171 - val_accuracy: 0.7290\n",
            "Epoch 1365/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7289 - val_loss: 0.5186 - val_accuracy: 0.7285\n",
            "Epoch 1366/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7282 - val_loss: 0.5173 - val_accuracy: 0.7287\n",
            "Epoch 1367/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7283 - val_loss: 0.5176 - val_accuracy: 0.7280\n",
            "Epoch 1368/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7285 - val_loss: 0.5179 - val_accuracy: 0.7282\n",
            "Epoch 1369/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7285 - val_loss: 0.5188 - val_accuracy: 0.7284\n",
            "Epoch 1370/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7292 - val_loss: 0.5167 - val_accuracy: 0.7290\n",
            "Epoch 1371/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7278 - val_loss: 0.5166 - val_accuracy: 0.7282\n",
            "Epoch 1372/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7298 - val_loss: 0.5203 - val_accuracy: 0.7261\n",
            "Epoch 1373/2500\n",
            "293/293 [==============================] - 1s 4ms/step - loss: 0.5123 - accuracy: 0.7287 - val_loss: 0.5184 - val_accuracy: 0.7277\n",
            "Epoch 1374/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7295 - val_loss: 0.5186 - val_accuracy: 0.7272\n",
            "Epoch 1375/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7288 - val_loss: 0.5171 - val_accuracy: 0.7292\n",
            "Epoch 1376/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7290 - val_loss: 0.5195 - val_accuracy: 0.7282\n",
            "Epoch 1377/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7289 - val_loss: 0.5184 - val_accuracy: 0.7277\n",
            "Epoch 1378/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7286 - val_loss: 0.5169 - val_accuracy: 0.7288\n",
            "Epoch 1379/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7288 - val_loss: 0.5167 - val_accuracy: 0.7280\n",
            "Epoch 1380/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7284 - val_loss: 0.5183 - val_accuracy: 0.7273\n",
            "Epoch 1381/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7300 - val_loss: 0.5189 - val_accuracy: 0.7282\n",
            "Epoch 1382/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7285 - val_loss: 0.5168 - val_accuracy: 0.7284\n",
            "Epoch 1383/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7288 - val_loss: 0.5193 - val_accuracy: 0.7286\n",
            "Epoch 1384/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7283 - val_loss: 0.5176 - val_accuracy: 0.7292\n",
            "Epoch 1385/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7285 - val_loss: 0.5164 - val_accuracy: 0.7269\n",
            "Epoch 1386/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7277 - val_loss: 0.5175 - val_accuracy: 0.7285\n",
            "Epoch 1387/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7288 - val_loss: 0.5182 - val_accuracy: 0.7278\n",
            "Epoch 1388/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7283 - val_loss: 0.5179 - val_accuracy: 0.7280\n",
            "Epoch 1389/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7279 - val_loss: 0.5173 - val_accuracy: 0.7276\n",
            "Epoch 1390/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7293 - val_loss: 0.5171 - val_accuracy: 0.7286\n",
            "Epoch 1391/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7295 - val_loss: 0.5166 - val_accuracy: 0.7286\n",
            "Epoch 1392/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7286 - val_loss: 0.5184 - val_accuracy: 0.7290\n",
            "Epoch 1393/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7301 - val_loss: 0.5196 - val_accuracy: 0.7274\n",
            "Epoch 1394/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5130 - accuracy: 0.7280 - val_loss: 0.5166 - val_accuracy: 0.7295\n",
            "Epoch 1395/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7291 - val_loss: 0.5166 - val_accuracy: 0.7291\n",
            "Epoch 1396/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7286 - val_loss: 0.5169 - val_accuracy: 0.7283\n",
            "Epoch 1397/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7292 - val_loss: 0.5175 - val_accuracy: 0.7285\n",
            "Epoch 1398/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7297 - val_loss: 0.5181 - val_accuracy: 0.7298\n",
            "Epoch 1399/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7286 - val_loss: 0.5169 - val_accuracy: 0.7281\n",
            "Epoch 1400/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7280 - val_loss: 0.5171 - val_accuracy: 0.7287\n",
            "Epoch 1401/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7301 - val_loss: 0.5170 - val_accuracy: 0.7289\n",
            "Epoch 1402/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7295 - val_loss: 0.5169 - val_accuracy: 0.7283\n",
            "Epoch 1403/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7285 - val_loss: 0.5169 - val_accuracy: 0.7292\n",
            "Epoch 1404/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7291 - val_loss: 0.5167 - val_accuracy: 0.7286\n",
            "Epoch 1405/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7284 - val_loss: 0.5172 - val_accuracy: 0.7290\n",
            "Epoch 1406/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7282 - val_loss: 0.5193 - val_accuracy: 0.7267\n",
            "Epoch 1407/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5129 - accuracy: 0.7276 - val_loss: 0.5174 - val_accuracy: 0.7288\n",
            "Epoch 1408/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7281 - val_loss: 0.5186 - val_accuracy: 0.7278\n",
            "Epoch 1409/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7289 - val_loss: 0.5182 - val_accuracy: 0.7281\n",
            "Epoch 1410/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7284 - val_loss: 0.5170 - val_accuracy: 0.7285\n",
            "Epoch 1411/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7291 - val_loss: 0.5168 - val_accuracy: 0.7291\n",
            "Epoch 1412/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5128 - accuracy: 0.7281 - val_loss: 0.5166 - val_accuracy: 0.7284\n",
            "Epoch 1413/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7303 - val_loss: 0.5175 - val_accuracy: 0.7291\n",
            "Epoch 1414/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7295 - val_loss: 0.5186 - val_accuracy: 0.7276\n",
            "Epoch 1415/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7288 - val_loss: 0.5161 - val_accuracy: 0.7287\n",
            "Epoch 1416/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7310 - val_loss: 0.5160 - val_accuracy: 0.7297\n",
            "Epoch 1417/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7290 - val_loss: 0.5171 - val_accuracy: 0.7295\n",
            "Epoch 1418/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7298 - val_loss: 0.5179 - val_accuracy: 0.7288\n",
            "Epoch 1419/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7291 - val_loss: 0.5179 - val_accuracy: 0.7292\n",
            "Epoch 1420/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7289 - val_loss: 0.5180 - val_accuracy: 0.7277\n",
            "Epoch 1421/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7297 - val_loss: 0.5179 - val_accuracy: 0.7279\n",
            "Epoch 1422/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7305 - val_loss: 0.5171 - val_accuracy: 0.7287\n",
            "Epoch 1423/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7287 - val_loss: 0.5170 - val_accuracy: 0.7285\n",
            "Epoch 1424/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7301 - val_loss: 0.5172 - val_accuracy: 0.7283\n",
            "Epoch 1425/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7284 - val_loss: 0.5178 - val_accuracy: 0.7282\n",
            "Epoch 1426/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7285 - val_loss: 0.5163 - val_accuracy: 0.7287\n",
            "Epoch 1427/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7288 - val_loss: 0.5176 - val_accuracy: 0.7288\n",
            "Epoch 1428/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7278 - val_loss: 0.5164 - val_accuracy: 0.7288\n",
            "Epoch 1429/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7281 - val_loss: 0.5180 - val_accuracy: 0.7287\n",
            "Epoch 1430/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7293 - val_loss: 0.5191 - val_accuracy: 0.7284\n",
            "Epoch 1431/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7301 - val_loss: 0.5165 - val_accuracy: 0.7291\n",
            "Epoch 1432/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7285 - val_loss: 0.5165 - val_accuracy: 0.7294\n",
            "Epoch 1433/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7300 - val_loss: 0.5181 - val_accuracy: 0.7279\n",
            "Epoch 1434/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7283 - val_loss: 0.5170 - val_accuracy: 0.7298\n",
            "Epoch 1435/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7289 - val_loss: 0.5167 - val_accuracy: 0.7291\n",
            "Epoch 1436/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7295 - val_loss: 0.5174 - val_accuracy: 0.7285\n",
            "Epoch 1437/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7293 - val_loss: 0.5170 - val_accuracy: 0.7295\n",
            "Epoch 1438/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7299 - val_loss: 0.5167 - val_accuracy: 0.7285\n",
            "Epoch 1439/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7301 - val_loss: 0.5160 - val_accuracy: 0.7294\n",
            "Epoch 1440/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7283 - val_loss: 0.5162 - val_accuracy: 0.7287\n",
            "Epoch 1441/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7284 - val_loss: 0.5173 - val_accuracy: 0.7289\n",
            "Epoch 1442/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7296 - val_loss: 0.5158 - val_accuracy: 0.7294\n",
            "Epoch 1443/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7289 - val_loss: 0.5167 - val_accuracy: 0.7299\n",
            "Epoch 1444/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7285 - val_loss: 0.5179 - val_accuracy: 0.7284\n",
            "Epoch 1445/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7282 - val_loss: 0.5181 - val_accuracy: 0.7289\n",
            "Epoch 1446/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7286 - val_loss: 0.5167 - val_accuracy: 0.7292\n",
            "Epoch 1447/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7289 - val_loss: 0.5163 - val_accuracy: 0.7296\n",
            "Epoch 1448/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7281 - val_loss: 0.5170 - val_accuracy: 0.7296\n",
            "Epoch 1449/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7286 - val_loss: 0.5172 - val_accuracy: 0.7289\n",
            "Epoch 1450/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7290 - val_loss: 0.5184 - val_accuracy: 0.7283\n",
            "Epoch 1451/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7291 - val_loss: 0.5166 - val_accuracy: 0.7295\n",
            "Epoch 1452/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7289 - val_loss: 0.5177 - val_accuracy: 0.7291\n",
            "Epoch 1453/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7294 - val_loss: 0.5180 - val_accuracy: 0.7285\n",
            "Epoch 1454/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7302 - val_loss: 0.5184 - val_accuracy: 0.7285\n",
            "Epoch 1455/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7292 - val_loss: 0.5169 - val_accuracy: 0.7279\n",
            "Epoch 1456/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7296 - val_loss: 0.5179 - val_accuracy: 0.7293\n",
            "Epoch 1457/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7290 - val_loss: 0.5176 - val_accuracy: 0.7286\n",
            "Epoch 1458/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7288 - val_loss: 0.5184 - val_accuracy: 0.7276\n",
            "Epoch 1459/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7284 - val_loss: 0.5180 - val_accuracy: 0.7282\n",
            "Epoch 1460/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7296 - val_loss: 0.5180 - val_accuracy: 0.7279\n",
            "Epoch 1461/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7299 - val_loss: 0.5185 - val_accuracy: 0.7281\n",
            "Epoch 1462/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7286 - val_loss: 0.5177 - val_accuracy: 0.7293\n",
            "Epoch 1463/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7302 - val_loss: 0.5169 - val_accuracy: 0.7284\n",
            "Epoch 1464/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7290 - val_loss: 0.5173 - val_accuracy: 0.7281\n",
            "Epoch 1465/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7296 - val_loss: 0.5176 - val_accuracy: 0.7296\n",
            "Epoch 1466/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7290 - val_loss: 0.5189 - val_accuracy: 0.7296\n",
            "Epoch 1467/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7290 - val_loss: 0.5177 - val_accuracy: 0.7284\n",
            "Epoch 1468/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7292 - val_loss: 0.5170 - val_accuracy: 0.7287\n",
            "Epoch 1469/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7281 - val_loss: 0.5170 - val_accuracy: 0.7292\n",
            "Epoch 1470/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7286 - val_loss: 0.5172 - val_accuracy: 0.7279\n",
            "Epoch 1471/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7294 - val_loss: 0.5178 - val_accuracy: 0.7273\n",
            "Epoch 1472/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7294 - val_loss: 0.5189 - val_accuracy: 0.7281\n",
            "Epoch 1473/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7292 - val_loss: 0.5191 - val_accuracy: 0.7285\n",
            "Epoch 1474/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7286 - val_loss: 0.5186 - val_accuracy: 0.7272\n",
            "Epoch 1475/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7289 - val_loss: 0.5189 - val_accuracy: 0.7279\n",
            "Epoch 1476/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7289 - val_loss: 0.5166 - val_accuracy: 0.7290\n",
            "Epoch 1477/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7279 - val_loss: 0.5168 - val_accuracy: 0.7282\n",
            "Epoch 1478/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7286 - val_loss: 0.5172 - val_accuracy: 0.7291\n",
            "Epoch 1479/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7299 - val_loss: 0.5172 - val_accuracy: 0.7292\n",
            "Epoch 1480/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7296 - val_loss: 0.5168 - val_accuracy: 0.7275\n",
            "Epoch 1481/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7288 - val_loss: 0.5170 - val_accuracy: 0.7293\n",
            "Epoch 1482/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7292 - val_loss: 0.5172 - val_accuracy: 0.7287\n",
            "Epoch 1483/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7292 - val_loss: 0.5184 - val_accuracy: 0.7284\n",
            "Epoch 1484/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7285 - val_loss: 0.5166 - val_accuracy: 0.7287\n",
            "Epoch 1485/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7293 - val_loss: 0.5169 - val_accuracy: 0.7295\n",
            "Epoch 1486/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7290 - val_loss: 0.5176 - val_accuracy: 0.7300\n",
            "Epoch 1487/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7300 - val_loss: 0.5201 - val_accuracy: 0.7278\n",
            "Epoch 1488/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7298 - val_loss: 0.5174 - val_accuracy: 0.7283\n",
            "Epoch 1489/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7287 - val_loss: 0.5190 - val_accuracy: 0.7287\n",
            "Epoch 1490/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7286 - val_loss: 0.5187 - val_accuracy: 0.7283\n",
            "Epoch 1491/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7294 - val_loss: 0.5174 - val_accuracy: 0.7291\n",
            "Epoch 1492/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7281 - val_loss: 0.5186 - val_accuracy: 0.7283\n",
            "Epoch 1493/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7292 - val_loss: 0.5188 - val_accuracy: 0.7278\n",
            "Epoch 1494/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7295 - val_loss: 0.5169 - val_accuracy: 0.7298\n",
            "Epoch 1495/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7297 - val_loss: 0.5181 - val_accuracy: 0.7288\n",
            "Epoch 1496/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7292 - val_loss: 0.5194 - val_accuracy: 0.7285\n",
            "Epoch 1497/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7286 - val_loss: 0.5182 - val_accuracy: 0.7286\n",
            "Epoch 1498/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7282 - val_loss: 0.5202 - val_accuracy: 0.7279\n",
            "Epoch 1499/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7297 - val_loss: 0.5178 - val_accuracy: 0.7284\n",
            "Epoch 1500/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7291 - val_loss: 0.5191 - val_accuracy: 0.7272\n",
            "Epoch 1501/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7299 - val_loss: 0.5167 - val_accuracy: 0.7296\n",
            "Epoch 1502/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7285 - val_loss: 0.5176 - val_accuracy: 0.7278\n",
            "Epoch 1503/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7298 - val_loss: 0.5178 - val_accuracy: 0.7292\n",
            "Epoch 1504/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7291 - val_loss: 0.5199 - val_accuracy: 0.7268\n",
            "Epoch 1505/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7298 - val_loss: 0.5169 - val_accuracy: 0.7298\n",
            "Epoch 1506/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7300 - val_loss: 0.5182 - val_accuracy: 0.7289\n",
            "Epoch 1507/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7302 - val_loss: 0.5186 - val_accuracy: 0.7278\n",
            "Epoch 1508/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7296 - val_loss: 0.5178 - val_accuracy: 0.7293\n",
            "Epoch 1509/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5125 - accuracy: 0.7285 - val_loss: 0.5187 - val_accuracy: 0.7290\n",
            "Epoch 1510/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7288 - val_loss: 0.5192 - val_accuracy: 0.7280\n",
            "Epoch 1511/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7292 - val_loss: 0.5217 - val_accuracy: 0.7268\n",
            "Epoch 1512/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7299 - val_loss: 0.5190 - val_accuracy: 0.7291\n",
            "Epoch 1513/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7294 - val_loss: 0.5187 - val_accuracy: 0.7286\n",
            "Epoch 1514/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7288 - val_loss: 0.5168 - val_accuracy: 0.7300\n",
            "Epoch 1515/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7296 - val_loss: 0.5169 - val_accuracy: 0.7292\n",
            "Epoch 1516/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7288 - val_loss: 0.5180 - val_accuracy: 0.7289\n",
            "Epoch 1517/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7295 - val_loss: 0.5189 - val_accuracy: 0.7297\n",
            "Epoch 1518/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7294 - val_loss: 0.5169 - val_accuracy: 0.7296\n",
            "Epoch 1519/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7285 - val_loss: 0.5184 - val_accuracy: 0.7281\n",
            "Epoch 1520/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7300 - val_loss: 0.5188 - val_accuracy: 0.7287\n",
            "Epoch 1521/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7282 - val_loss: 0.5175 - val_accuracy: 0.7285\n",
            "Epoch 1522/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7296 - val_loss: 0.5191 - val_accuracy: 0.7282\n",
            "Epoch 1523/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7294 - val_loss: 0.5210 - val_accuracy: 0.7267\n",
            "Epoch 1524/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7287 - val_loss: 0.5183 - val_accuracy: 0.7287\n",
            "Epoch 1525/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7301 - val_loss: 0.5182 - val_accuracy: 0.7298\n",
            "Epoch 1526/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7284 - val_loss: 0.5172 - val_accuracy: 0.7290\n",
            "Epoch 1527/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7284 - val_loss: 0.5166 - val_accuracy: 0.7295\n",
            "Epoch 1528/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7295 - val_loss: 0.5173 - val_accuracy: 0.7299\n",
            "Epoch 1529/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7297 - val_loss: 0.5187 - val_accuracy: 0.7280\n",
            "Epoch 1530/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7284 - val_loss: 0.5167 - val_accuracy: 0.7279\n",
            "Epoch 1531/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7296 - val_loss: 0.5196 - val_accuracy: 0.7277\n",
            "Epoch 1532/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7292 - val_loss: 0.5178 - val_accuracy: 0.7287\n",
            "Epoch 1533/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7288 - val_loss: 0.5206 - val_accuracy: 0.7288\n",
            "Epoch 1534/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7296 - val_loss: 0.5168 - val_accuracy: 0.7295\n",
            "Epoch 1535/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7283 - val_loss: 0.5174 - val_accuracy: 0.7286\n",
            "Epoch 1536/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7286 - val_loss: 0.5186 - val_accuracy: 0.7286\n",
            "Epoch 1537/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7288 - val_loss: 0.5187 - val_accuracy: 0.7286\n",
            "Epoch 1538/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7297 - val_loss: 0.5172 - val_accuracy: 0.7287\n",
            "Epoch 1539/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7290 - val_loss: 0.5176 - val_accuracy: 0.7294\n",
            "Epoch 1540/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7287 - val_loss: 0.5176 - val_accuracy: 0.7282\n",
            "Epoch 1541/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7290 - val_loss: 0.5176 - val_accuracy: 0.7286\n",
            "Epoch 1542/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7289 - val_loss: 0.5180 - val_accuracy: 0.7285\n",
            "Epoch 1543/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7297 - val_loss: 0.5204 - val_accuracy: 0.7278\n",
            "Epoch 1544/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7286 - val_loss: 0.5175 - val_accuracy: 0.7280\n",
            "Epoch 1545/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7296 - val_loss: 0.5187 - val_accuracy: 0.7279\n",
            "Epoch 1546/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7294 - val_loss: 0.5176 - val_accuracy: 0.7291\n",
            "Epoch 1547/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7291 - val_loss: 0.5192 - val_accuracy: 0.7280\n",
            "Epoch 1548/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7301 - val_loss: 0.5164 - val_accuracy: 0.7283\n",
            "Epoch 1549/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7293 - val_loss: 0.5167 - val_accuracy: 0.7294\n",
            "Epoch 1550/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7290 - val_loss: 0.5161 - val_accuracy: 0.7292\n",
            "Epoch 1551/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7299 - val_loss: 0.5200 - val_accuracy: 0.7271\n",
            "Epoch 1552/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7286 - val_loss: 0.5171 - val_accuracy: 0.7301\n",
            "Epoch 1553/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7296 - val_loss: 0.5182 - val_accuracy: 0.7284\n",
            "Epoch 1554/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7291 - val_loss: 0.5186 - val_accuracy: 0.7288\n",
            "Epoch 1555/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7298 - val_loss: 0.5176 - val_accuracy: 0.7287\n",
            "Epoch 1556/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7300 - val_loss: 0.5180 - val_accuracy: 0.7294\n",
            "Epoch 1557/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7291 - val_loss: 0.5190 - val_accuracy: 0.7287\n",
            "Epoch 1558/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7309 - val_loss: 0.5183 - val_accuracy: 0.7286\n",
            "Epoch 1559/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7293 - val_loss: 0.5172 - val_accuracy: 0.7289\n",
            "Epoch 1560/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7295 - val_loss: 0.5199 - val_accuracy: 0.7284\n",
            "Epoch 1561/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7291 - val_loss: 0.5183 - val_accuracy: 0.7277\n",
            "Epoch 1562/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7292 - val_loss: 0.5170 - val_accuracy: 0.7297\n",
            "Epoch 1563/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7301 - val_loss: 0.5190 - val_accuracy: 0.7290\n",
            "Epoch 1564/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7296 - val_loss: 0.5186 - val_accuracy: 0.7282\n",
            "Epoch 1565/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7293 - val_loss: 0.5189 - val_accuracy: 0.7290\n",
            "Epoch 1566/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7291 - val_loss: 0.5196 - val_accuracy: 0.7282\n",
            "Epoch 1567/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7301 - val_loss: 0.5176 - val_accuracy: 0.7298\n",
            "Epoch 1568/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7289 - val_loss: 0.5187 - val_accuracy: 0.7286\n",
            "Epoch 1569/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7299 - val_loss: 0.5174 - val_accuracy: 0.7289\n",
            "Epoch 1570/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7299 - val_loss: 0.5188 - val_accuracy: 0.7283\n",
            "Epoch 1571/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7289 - val_loss: 0.5177 - val_accuracy: 0.7294\n",
            "Epoch 1572/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7304 - val_loss: 0.5188 - val_accuracy: 0.7285\n",
            "Epoch 1573/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7298 - val_loss: 0.5179 - val_accuracy: 0.7291\n",
            "Epoch 1574/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7293 - val_loss: 0.5177 - val_accuracy: 0.7288\n",
            "Epoch 1575/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7300 - val_loss: 0.5178 - val_accuracy: 0.7285\n",
            "Epoch 1576/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7286 - val_loss: 0.5180 - val_accuracy: 0.7292\n",
            "Epoch 1577/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7287 - val_loss: 0.5192 - val_accuracy: 0.7286\n",
            "Epoch 1578/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7282 - val_loss: 0.5179 - val_accuracy: 0.7289\n",
            "Epoch 1579/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7298 - val_loss: 0.5179 - val_accuracy: 0.7287\n",
            "Epoch 1580/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7290 - val_loss: 0.5171 - val_accuracy: 0.7291\n",
            "Epoch 1581/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7301 - val_loss: 0.5178 - val_accuracy: 0.7287\n",
            "Epoch 1582/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7292 - val_loss: 0.5172 - val_accuracy: 0.7288\n",
            "Epoch 1583/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7294 - val_loss: 0.5166 - val_accuracy: 0.7291\n",
            "Epoch 1584/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7299 - val_loss: 0.5165 - val_accuracy: 0.7299\n",
            "Epoch 1585/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7296 - val_loss: 0.5189 - val_accuracy: 0.7283\n",
            "Epoch 1586/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7285 - val_loss: 0.5188 - val_accuracy: 0.7286\n",
            "Epoch 1587/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7293 - val_loss: 0.5184 - val_accuracy: 0.7287\n",
            "Epoch 1588/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7301 - val_loss: 0.5190 - val_accuracy: 0.7279\n",
            "Epoch 1589/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7290 - val_loss: 0.5193 - val_accuracy: 0.7275\n",
            "Epoch 1590/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7298 - val_loss: 0.5202 - val_accuracy: 0.7284\n",
            "Epoch 1591/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7294 - val_loss: 0.5196 - val_accuracy: 0.7286\n",
            "Epoch 1592/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7299 - val_loss: 0.5187 - val_accuracy: 0.7284\n",
            "Epoch 1593/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7298 - val_loss: 0.5193 - val_accuracy: 0.7278\n",
            "Epoch 1594/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7298 - val_loss: 0.5190 - val_accuracy: 0.7280\n",
            "Epoch 1595/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7297 - val_loss: 0.5186 - val_accuracy: 0.7285\n",
            "Epoch 1596/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7282 - val_loss: 0.5184 - val_accuracy: 0.7281\n",
            "Epoch 1597/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7300 - val_loss: 0.5176 - val_accuracy: 0.7284\n",
            "Epoch 1598/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7301 - val_loss: 0.5197 - val_accuracy: 0.7283\n",
            "Epoch 1599/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7290 - val_loss: 0.5176 - val_accuracy: 0.7298\n",
            "Epoch 1600/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7301 - val_loss: 0.5177 - val_accuracy: 0.7291\n",
            "Epoch 1601/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7300 - val_loss: 0.5204 - val_accuracy: 0.7283\n",
            "Epoch 1602/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7289 - val_loss: 0.5180 - val_accuracy: 0.7282\n",
            "Epoch 1603/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7288 - val_loss: 0.5176 - val_accuracy: 0.7289\n",
            "Epoch 1604/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7299 - val_loss: 0.5190 - val_accuracy: 0.7288\n",
            "Epoch 1605/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7293 - val_loss: 0.5185 - val_accuracy: 0.7283\n",
            "Epoch 1606/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7298 - val_loss: 0.5185 - val_accuracy: 0.7290\n",
            "Epoch 1607/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7296 - val_loss: 0.5199 - val_accuracy: 0.7283\n",
            "Epoch 1608/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7300 - val_loss: 0.5190 - val_accuracy: 0.7275\n",
            "Epoch 1609/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7289 - val_loss: 0.5193 - val_accuracy: 0.7287\n",
            "Epoch 1610/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7301 - val_loss: 0.5192 - val_accuracy: 0.7289\n",
            "Epoch 1611/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7296 - val_loss: 0.5206 - val_accuracy: 0.7282\n",
            "Epoch 1612/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7295 - val_loss: 0.5189 - val_accuracy: 0.7293\n",
            "Epoch 1613/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7294 - val_loss: 0.5202 - val_accuracy: 0.7286\n",
            "Epoch 1614/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7298 - val_loss: 0.5181 - val_accuracy: 0.7293\n",
            "Epoch 1615/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7293 - val_loss: 0.5184 - val_accuracy: 0.7294\n",
            "Epoch 1616/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7295 - val_loss: 0.5187 - val_accuracy: 0.7287\n",
            "Epoch 1617/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7291 - val_loss: 0.5188 - val_accuracy: 0.7290\n",
            "Epoch 1618/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7296 - val_loss: 0.5208 - val_accuracy: 0.7288\n",
            "Epoch 1619/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7289 - val_loss: 0.5184 - val_accuracy: 0.7288\n",
            "Epoch 1620/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7301 - val_loss: 0.5192 - val_accuracy: 0.7288\n",
            "Epoch 1621/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7304 - val_loss: 0.5176 - val_accuracy: 0.7286\n",
            "Epoch 1622/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7296 - val_loss: 0.5203 - val_accuracy: 0.7285\n",
            "Epoch 1623/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7297 - val_loss: 0.5204 - val_accuracy: 0.7283\n",
            "Epoch 1624/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5123 - accuracy: 0.7294 - val_loss: 0.5196 - val_accuracy: 0.7285\n",
            "Epoch 1625/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7291 - val_loss: 0.5184 - val_accuracy: 0.7301\n",
            "Epoch 1626/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7302 - val_loss: 0.5180 - val_accuracy: 0.7279\n",
            "Epoch 1627/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7299 - val_loss: 0.5185 - val_accuracy: 0.7285\n",
            "Epoch 1628/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7294 - val_loss: 0.5174 - val_accuracy: 0.7289\n",
            "Epoch 1629/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7294 - val_loss: 0.5183 - val_accuracy: 0.7283\n",
            "Epoch 1630/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7297 - val_loss: 0.5182 - val_accuracy: 0.7301\n",
            "Epoch 1631/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7289 - val_loss: 0.5199 - val_accuracy: 0.7282\n",
            "Epoch 1632/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7297 - val_loss: 0.5184 - val_accuracy: 0.7297\n",
            "Epoch 1633/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7298 - val_loss: 0.5186 - val_accuracy: 0.7303\n",
            "Epoch 1634/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7297 - val_loss: 0.5170 - val_accuracy: 0.7290\n",
            "Epoch 1635/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7300 - val_loss: 0.5196 - val_accuracy: 0.7295\n",
            "Epoch 1636/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7287 - val_loss: 0.5171 - val_accuracy: 0.7298\n",
            "Epoch 1637/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7299 - val_loss: 0.5180 - val_accuracy: 0.7289\n",
            "Epoch 1638/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7300 - val_loss: 0.5187 - val_accuracy: 0.7291\n",
            "Epoch 1639/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7295 - val_loss: 0.5188 - val_accuracy: 0.7298\n",
            "Epoch 1640/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7293 - val_loss: 0.5193 - val_accuracy: 0.7292\n",
            "Epoch 1641/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7301 - val_loss: 0.5187 - val_accuracy: 0.7287\n",
            "Epoch 1642/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7293 - val_loss: 0.5185 - val_accuracy: 0.7299\n",
            "Epoch 1643/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7290 - val_loss: 0.5183 - val_accuracy: 0.7300\n",
            "Epoch 1644/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7297 - val_loss: 0.5191 - val_accuracy: 0.7288\n",
            "Epoch 1645/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7294 - val_loss: 0.5197 - val_accuracy: 0.7285\n",
            "Epoch 1646/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7288 - val_loss: 0.5189 - val_accuracy: 0.7287\n",
            "Epoch 1647/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7282 - val_loss: 0.5198 - val_accuracy: 0.7274\n",
            "Epoch 1648/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7295 - val_loss: 0.5188 - val_accuracy: 0.7282\n",
            "Epoch 1649/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7291 - val_loss: 0.5229 - val_accuracy: 0.7261\n",
            "Epoch 1650/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7300 - val_loss: 0.5171 - val_accuracy: 0.7292\n",
            "Epoch 1651/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7303 - val_loss: 0.5168 - val_accuracy: 0.7293\n",
            "Epoch 1652/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7284 - val_loss: 0.5180 - val_accuracy: 0.7279\n",
            "Epoch 1653/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7285 - val_loss: 0.5183 - val_accuracy: 0.7290\n",
            "Epoch 1654/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7292 - val_loss: 0.5189 - val_accuracy: 0.7292\n",
            "Epoch 1655/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5120 - accuracy: 0.7292 - val_loss: 0.5180 - val_accuracy: 0.7294\n",
            "Epoch 1656/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7290 - val_loss: 0.5193 - val_accuracy: 0.7288\n",
            "Epoch 1657/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7300 - val_loss: 0.5185 - val_accuracy: 0.7295\n",
            "Epoch 1658/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7300 - val_loss: 0.5188 - val_accuracy: 0.7295\n",
            "Epoch 1659/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.7288 - val_loss: 0.5186 - val_accuracy: 0.7294\n",
            "Epoch 1660/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7305 - val_loss: 0.5198 - val_accuracy: 0.7289\n",
            "Epoch 1661/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7284 - val_loss: 0.5196 - val_accuracy: 0.7288\n",
            "Epoch 1662/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7300 - val_loss: 0.5191 - val_accuracy: 0.7286\n",
            "Epoch 1663/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7296 - val_loss: 0.5191 - val_accuracy: 0.7298\n",
            "Epoch 1664/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7299 - val_loss: 0.5205 - val_accuracy: 0.7277\n",
            "Epoch 1665/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7290 - val_loss: 0.5204 - val_accuracy: 0.7286\n",
            "Epoch 1666/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7298 - val_loss: 0.5176 - val_accuracy: 0.7281\n",
            "Epoch 1667/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7310 - val_loss: 0.5213 - val_accuracy: 0.7288\n",
            "Epoch 1668/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7304 - val_loss: 0.5192 - val_accuracy: 0.7286\n",
            "Epoch 1669/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7286 - val_loss: 0.5182 - val_accuracy: 0.7284\n",
            "Epoch 1670/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7295 - val_loss: 0.5185 - val_accuracy: 0.7295\n",
            "Epoch 1671/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7285 - val_loss: 0.5201 - val_accuracy: 0.7278\n",
            "Epoch 1672/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7305 - val_loss: 0.5179 - val_accuracy: 0.7284\n",
            "Epoch 1673/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7297 - val_loss: 0.5199 - val_accuracy: 0.7291\n",
            "Epoch 1674/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7293 - val_loss: 0.5185 - val_accuracy: 0.7305\n",
            "Epoch 1675/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7309 - val_loss: 0.5186 - val_accuracy: 0.7294\n",
            "Epoch 1676/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7298 - val_loss: 0.5192 - val_accuracy: 0.7302\n",
            "Epoch 1677/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7297 - val_loss: 0.5181 - val_accuracy: 0.7291\n",
            "Epoch 1678/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7303 - val_loss: 0.5178 - val_accuracy: 0.7302\n",
            "Epoch 1679/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7296 - val_loss: 0.5184 - val_accuracy: 0.7299\n",
            "Epoch 1680/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7303 - val_loss: 0.5177 - val_accuracy: 0.7298\n",
            "Epoch 1681/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7299 - val_loss: 0.5194 - val_accuracy: 0.7288\n",
            "Epoch 1682/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7297 - val_loss: 0.5189 - val_accuracy: 0.7290\n",
            "Epoch 1683/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7304 - val_loss: 0.5166 - val_accuracy: 0.7302\n",
            "Epoch 1684/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7303 - val_loss: 0.5171 - val_accuracy: 0.7299\n",
            "Epoch 1685/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7291 - val_loss: 0.5193 - val_accuracy: 0.7284\n",
            "Epoch 1686/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7295 - val_loss: 0.5170 - val_accuracy: 0.7287\n",
            "Epoch 1687/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7294 - val_loss: 0.5205 - val_accuracy: 0.7280\n",
            "Epoch 1688/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7284 - val_loss: 0.5186 - val_accuracy: 0.7285\n",
            "Epoch 1689/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7294 - val_loss: 0.5181 - val_accuracy: 0.7295\n",
            "Epoch 1690/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7280 - val_loss: 0.5179 - val_accuracy: 0.7287\n",
            "Epoch 1691/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7302 - val_loss: 0.5196 - val_accuracy: 0.7289\n",
            "Epoch 1692/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7294 - val_loss: 0.5180 - val_accuracy: 0.7289\n",
            "Epoch 1693/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7293 - val_loss: 0.5194 - val_accuracy: 0.7296\n",
            "Epoch 1694/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7303 - val_loss: 0.5193 - val_accuracy: 0.7293\n",
            "Epoch 1695/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7296 - val_loss: 0.5188 - val_accuracy: 0.7293\n",
            "Epoch 1696/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7302 - val_loss: 0.5184 - val_accuracy: 0.7291\n",
            "Epoch 1697/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7293 - val_loss: 0.5192 - val_accuracy: 0.7288\n",
            "Epoch 1698/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7296 - val_loss: 0.5186 - val_accuracy: 0.7285\n",
            "Epoch 1699/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7300 - val_loss: 0.5184 - val_accuracy: 0.7293\n",
            "Epoch 1700/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7299 - val_loss: 0.5180 - val_accuracy: 0.7296\n",
            "Epoch 1701/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7286 - val_loss: 0.5187 - val_accuracy: 0.7280\n",
            "Epoch 1702/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7293 - val_loss: 0.5202 - val_accuracy: 0.7278\n",
            "Epoch 1703/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7299 - val_loss: 0.5186 - val_accuracy: 0.7279\n",
            "Epoch 1704/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7296 - val_loss: 0.5189 - val_accuracy: 0.7292\n",
            "Epoch 1705/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7296 - val_loss: 0.5195 - val_accuracy: 0.7291\n",
            "Epoch 1706/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7289 - val_loss: 0.5197 - val_accuracy: 0.7285\n",
            "Epoch 1707/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7302 - val_loss: 0.5183 - val_accuracy: 0.7293\n",
            "Epoch 1708/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7294 - val_loss: 0.5179 - val_accuracy: 0.7307\n",
            "Epoch 1709/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7296 - val_loss: 0.5197 - val_accuracy: 0.7286\n",
            "Epoch 1710/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7293 - val_loss: 0.5177 - val_accuracy: 0.7284\n",
            "Epoch 1711/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7295 - val_loss: 0.5201 - val_accuracy: 0.7280\n",
            "Epoch 1712/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7293 - val_loss: 0.5193 - val_accuracy: 0.7283\n",
            "Epoch 1713/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7298 - val_loss: 0.5187 - val_accuracy: 0.7288\n",
            "Epoch 1714/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5126 - accuracy: 0.7293 - val_loss: 0.5186 - val_accuracy: 0.7284\n",
            "Epoch 1715/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7285 - val_loss: 0.5189 - val_accuracy: 0.7291\n",
            "Epoch 1716/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7296 - val_loss: 0.5188 - val_accuracy: 0.7299\n",
            "Epoch 1717/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7296 - val_loss: 0.5182 - val_accuracy: 0.7277\n",
            "Epoch 1718/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7302 - val_loss: 0.5211 - val_accuracy: 0.7288\n",
            "Epoch 1719/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7306 - val_loss: 0.5192 - val_accuracy: 0.7292\n",
            "Epoch 1720/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7302 - val_loss: 0.5195 - val_accuracy: 0.7279\n",
            "Epoch 1721/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7297 - val_loss: 0.5195 - val_accuracy: 0.7297\n",
            "Epoch 1722/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7300 - val_loss: 0.5191 - val_accuracy: 0.7287\n",
            "Epoch 1723/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7294 - val_loss: 0.5177 - val_accuracy: 0.7296\n",
            "Epoch 1724/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7300 - val_loss: 0.5194 - val_accuracy: 0.7290\n",
            "Epoch 1725/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7297 - val_loss: 0.5179 - val_accuracy: 0.7291\n",
            "Epoch 1726/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7296 - val_loss: 0.5189 - val_accuracy: 0.7283\n",
            "Epoch 1727/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7305 - val_loss: 0.5193 - val_accuracy: 0.7289\n",
            "Epoch 1728/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7302 - val_loss: 0.5202 - val_accuracy: 0.7295\n",
            "Epoch 1729/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7297 - val_loss: 0.5188 - val_accuracy: 0.7301\n",
            "Epoch 1730/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7292 - val_loss: 0.5197 - val_accuracy: 0.7295\n",
            "Epoch 1731/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7297 - val_loss: 0.5192 - val_accuracy: 0.7281\n",
            "Epoch 1732/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7284 - val_loss: 0.5186 - val_accuracy: 0.7297\n",
            "Epoch 1733/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7291 - val_loss: 0.5200 - val_accuracy: 0.7293\n",
            "Epoch 1734/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7291 - val_loss: 0.5179 - val_accuracy: 0.7294\n",
            "Epoch 1735/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7301 - val_loss: 0.5198 - val_accuracy: 0.7293\n",
            "Epoch 1736/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7293 - val_loss: 0.5184 - val_accuracy: 0.7300\n",
            "Epoch 1737/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7298 - val_loss: 0.5191 - val_accuracy: 0.7298\n",
            "Epoch 1738/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7297 - val_loss: 0.5195 - val_accuracy: 0.7285\n",
            "Epoch 1739/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7302 - val_loss: 0.5177 - val_accuracy: 0.7292\n",
            "Epoch 1740/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7310 - val_loss: 0.5185 - val_accuracy: 0.7302\n",
            "Epoch 1741/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7295 - val_loss: 0.5198 - val_accuracy: 0.7280\n",
            "Epoch 1742/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7293 - val_loss: 0.5184 - val_accuracy: 0.7293\n",
            "Epoch 1743/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5127 - accuracy: 0.7288 - val_loss: 0.5174 - val_accuracy: 0.7293\n",
            "Epoch 1744/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7291 - val_loss: 0.5190 - val_accuracy: 0.7299\n",
            "Epoch 1745/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7298 - val_loss: 0.5188 - val_accuracy: 0.7302\n",
            "Epoch 1746/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7295 - val_loss: 0.5186 - val_accuracy: 0.7297\n",
            "Epoch 1747/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7290 - val_loss: 0.5183 - val_accuracy: 0.7291\n",
            "Epoch 1748/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7299 - val_loss: 0.5188 - val_accuracy: 0.7295\n",
            "Epoch 1749/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7292 - val_loss: 0.5191 - val_accuracy: 0.7288\n",
            "Epoch 1750/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7292 - val_loss: 0.5195 - val_accuracy: 0.7284\n",
            "Epoch 1751/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7285 - val_loss: 0.5188 - val_accuracy: 0.7284\n",
            "Epoch 1752/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7299 - val_loss: 0.5195 - val_accuracy: 0.7292\n",
            "Epoch 1753/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7308 - val_loss: 0.5199 - val_accuracy: 0.7295\n",
            "Epoch 1754/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7286 - val_loss: 0.5202 - val_accuracy: 0.7286\n",
            "Epoch 1755/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7312 - val_loss: 0.5190 - val_accuracy: 0.7294\n",
            "Epoch 1756/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7298 - val_loss: 0.5190 - val_accuracy: 0.7301\n",
            "Epoch 1757/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7295 - val_loss: 0.5194 - val_accuracy: 0.7292\n",
            "Epoch 1758/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7299 - val_loss: 0.5195 - val_accuracy: 0.7285\n",
            "Epoch 1759/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7297 - val_loss: 0.5195 - val_accuracy: 0.7296\n",
            "Epoch 1760/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7297 - val_loss: 0.5184 - val_accuracy: 0.7294\n",
            "Epoch 1761/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7304 - val_loss: 0.5192 - val_accuracy: 0.7297\n",
            "Epoch 1762/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7296 - val_loss: 0.5212 - val_accuracy: 0.7293\n",
            "Epoch 1763/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7303 - val_loss: 0.5198 - val_accuracy: 0.7303\n",
            "Epoch 1764/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7304 - val_loss: 0.5178 - val_accuracy: 0.7290\n",
            "Epoch 1765/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7297 - val_loss: 0.5232 - val_accuracy: 0.7271\n",
            "Epoch 1766/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7297 - val_loss: 0.5193 - val_accuracy: 0.7293\n",
            "Epoch 1767/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7290 - val_loss: 0.5193 - val_accuracy: 0.7287\n",
            "Epoch 1768/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7306 - val_loss: 0.5195 - val_accuracy: 0.7289\n",
            "Epoch 1769/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7310 - val_loss: 0.5199 - val_accuracy: 0.7288\n",
            "Epoch 1770/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7295 - val_loss: 0.5199 - val_accuracy: 0.7296\n",
            "Epoch 1771/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7299 - val_loss: 0.5207 - val_accuracy: 0.7287\n",
            "Epoch 1772/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7301 - val_loss: 0.5198 - val_accuracy: 0.7299\n",
            "Epoch 1773/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7298 - val_loss: 0.5199 - val_accuracy: 0.7290\n",
            "Epoch 1774/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7303 - val_loss: 0.5219 - val_accuracy: 0.7293\n",
            "Epoch 1775/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7301 - val_loss: 0.5196 - val_accuracy: 0.7290\n",
            "Epoch 1776/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7297 - val_loss: 0.5200 - val_accuracy: 0.7288\n",
            "Epoch 1777/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7301 - val_loss: 0.5201 - val_accuracy: 0.7289\n",
            "Epoch 1778/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7299 - val_loss: 0.5204 - val_accuracy: 0.7288\n",
            "Epoch 1779/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7301 - val_loss: 0.5202 - val_accuracy: 0.7302\n",
            "Epoch 1780/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7304 - val_loss: 0.5197 - val_accuracy: 0.7285\n",
            "Epoch 1781/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7297 - val_loss: 0.5194 - val_accuracy: 0.7295\n",
            "Epoch 1782/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7299 - val_loss: 0.5198 - val_accuracy: 0.7303\n",
            "Epoch 1783/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7295 - val_loss: 0.5197 - val_accuracy: 0.7293\n",
            "Epoch 1784/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7300 - val_loss: 0.5193 - val_accuracy: 0.7287\n",
            "Epoch 1785/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7311 - val_loss: 0.5194 - val_accuracy: 0.7297\n",
            "Epoch 1786/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7291 - val_loss: 0.5194 - val_accuracy: 0.7293\n",
            "Epoch 1787/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7294 - val_loss: 0.5186 - val_accuracy: 0.7292\n",
            "Epoch 1788/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7298 - val_loss: 0.5205 - val_accuracy: 0.7290\n",
            "Epoch 1789/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7305 - val_loss: 0.5196 - val_accuracy: 0.7298\n",
            "Epoch 1790/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7295 - val_loss: 0.5207 - val_accuracy: 0.7294\n",
            "Epoch 1791/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7297 - val_loss: 0.5208 - val_accuracy: 0.7292\n",
            "Epoch 1792/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7298 - val_loss: 0.5212 - val_accuracy: 0.7301\n",
            "Epoch 1793/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7288 - val_loss: 0.5192 - val_accuracy: 0.7294\n",
            "Epoch 1794/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7303 - val_loss: 0.5217 - val_accuracy: 0.7280\n",
            "Epoch 1795/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7294 - val_loss: 0.5184 - val_accuracy: 0.7297\n",
            "Epoch 1796/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7304 - val_loss: 0.5194 - val_accuracy: 0.7296\n",
            "Epoch 1797/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7301 - val_loss: 0.5187 - val_accuracy: 0.7300\n",
            "Epoch 1798/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5118 - accuracy: 0.7298 - val_loss: 0.5193 - val_accuracy: 0.7294\n",
            "Epoch 1799/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7297 - val_loss: 0.5206 - val_accuracy: 0.7285\n",
            "Epoch 1800/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7302 - val_loss: 0.5188 - val_accuracy: 0.7294\n",
            "Epoch 1801/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7295 - val_loss: 0.5207 - val_accuracy: 0.7293\n",
            "Epoch 1802/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7302 - val_loss: 0.5192 - val_accuracy: 0.7297\n",
            "Epoch 1803/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7300 - val_loss: 0.5216 - val_accuracy: 0.7280\n",
            "Epoch 1804/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7297 - val_loss: 0.5222 - val_accuracy: 0.7284\n",
            "Epoch 1805/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7304 - val_loss: 0.5203 - val_accuracy: 0.7300\n",
            "Epoch 1806/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7310 - val_loss: 0.5200 - val_accuracy: 0.7294\n",
            "Epoch 1807/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7307 - val_loss: 0.5192 - val_accuracy: 0.7291\n",
            "Epoch 1808/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7286 - val_loss: 0.5193 - val_accuracy: 0.7288\n",
            "Epoch 1809/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7298 - val_loss: 0.5192 - val_accuracy: 0.7294\n",
            "Epoch 1810/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7305 - val_loss: 0.5193 - val_accuracy: 0.7293\n",
            "Epoch 1811/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7301 - val_loss: 0.5200 - val_accuracy: 0.7276\n",
            "Epoch 1812/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7297 - val_loss: 0.5189 - val_accuracy: 0.7302\n",
            "Epoch 1813/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7290 - val_loss: 0.5196 - val_accuracy: 0.7298\n",
            "Epoch 1814/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7284 - val_loss: 0.5199 - val_accuracy: 0.7294\n",
            "Epoch 1815/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7298 - val_loss: 0.5210 - val_accuracy: 0.7299\n",
            "Epoch 1816/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7303 - val_loss: 0.5195 - val_accuracy: 0.7294\n",
            "Epoch 1817/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7298 - val_loss: 0.5194 - val_accuracy: 0.7305\n",
            "Epoch 1818/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7300 - val_loss: 0.5190 - val_accuracy: 0.7299\n",
            "Epoch 1819/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7294 - val_loss: 0.5188 - val_accuracy: 0.7296\n",
            "Epoch 1820/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7305 - val_loss: 0.5185 - val_accuracy: 0.7296\n",
            "Epoch 1821/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7301 - val_loss: 0.5213 - val_accuracy: 0.7286\n",
            "Epoch 1822/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7292 - val_loss: 0.5197 - val_accuracy: 0.7290\n",
            "Epoch 1823/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7306 - val_loss: 0.5210 - val_accuracy: 0.7289\n",
            "Epoch 1824/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7305 - val_loss: 0.5175 - val_accuracy: 0.7296\n",
            "Epoch 1825/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7294 - val_loss: 0.5191 - val_accuracy: 0.7295\n",
            "Epoch 1826/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7298 - val_loss: 0.5189 - val_accuracy: 0.7298\n",
            "Epoch 1827/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7295 - val_loss: 0.5190 - val_accuracy: 0.7297\n",
            "Epoch 1828/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7299 - val_loss: 0.5197 - val_accuracy: 0.7295\n",
            "Epoch 1829/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7306 - val_loss: 0.5188 - val_accuracy: 0.7293\n",
            "Epoch 1830/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7296 - val_loss: 0.5197 - val_accuracy: 0.7305\n",
            "Epoch 1831/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7311 - val_loss: 0.5195 - val_accuracy: 0.7296\n",
            "Epoch 1832/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7300 - val_loss: 0.5192 - val_accuracy: 0.7286\n",
            "Epoch 1833/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7297 - val_loss: 0.5182 - val_accuracy: 0.7292\n",
            "Epoch 1834/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7301 - val_loss: 0.5185 - val_accuracy: 0.7300\n",
            "Epoch 1835/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7298 - val_loss: 0.5183 - val_accuracy: 0.7291\n",
            "Epoch 1836/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7308 - val_loss: 0.5205 - val_accuracy: 0.7298\n",
            "Epoch 1837/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7296 - val_loss: 0.5203 - val_accuracy: 0.7296\n",
            "Epoch 1838/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7301 - val_loss: 0.5206 - val_accuracy: 0.7300\n",
            "Epoch 1839/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7312 - val_loss: 0.5217 - val_accuracy: 0.7293\n",
            "Epoch 1840/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7303 - val_loss: 0.5196 - val_accuracy: 0.7289\n",
            "Epoch 1841/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7310 - val_loss: 0.5224 - val_accuracy: 0.7278\n",
            "Epoch 1842/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7304 - val_loss: 0.5227 - val_accuracy: 0.7268\n",
            "Epoch 1843/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7300 - val_loss: 0.5195 - val_accuracy: 0.7302\n",
            "Epoch 1844/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7291 - val_loss: 0.5186 - val_accuracy: 0.7295\n",
            "Epoch 1845/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7303 - val_loss: 0.5199 - val_accuracy: 0.7306\n",
            "Epoch 1846/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7308 - val_loss: 0.5201 - val_accuracy: 0.7292\n",
            "Epoch 1847/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7305 - val_loss: 0.5201 - val_accuracy: 0.7290\n",
            "Epoch 1848/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7295 - val_loss: 0.5197 - val_accuracy: 0.7291\n",
            "Epoch 1849/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7305 - val_loss: 0.5196 - val_accuracy: 0.7281\n",
            "Epoch 1850/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7303 - val_loss: 0.5206 - val_accuracy: 0.7299\n",
            "Epoch 1851/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7304 - val_loss: 0.5185 - val_accuracy: 0.7298\n",
            "Epoch 1852/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7295 - val_loss: 0.5200 - val_accuracy: 0.7301\n",
            "Epoch 1853/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7294 - val_loss: 0.5197 - val_accuracy: 0.7291\n",
            "Epoch 1854/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7295 - val_loss: 0.5198 - val_accuracy: 0.7289\n",
            "Epoch 1855/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7297 - val_loss: 0.5211 - val_accuracy: 0.7288\n",
            "Epoch 1856/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7295 - val_loss: 0.5197 - val_accuracy: 0.7304\n",
            "Epoch 1857/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7306 - val_loss: 0.5201 - val_accuracy: 0.7285\n",
            "Epoch 1858/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7302 - val_loss: 0.5215 - val_accuracy: 0.7300\n",
            "Epoch 1859/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7299 - val_loss: 0.5207 - val_accuracy: 0.7301\n",
            "Epoch 1860/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7302 - val_loss: 0.5203 - val_accuracy: 0.7297\n",
            "Epoch 1861/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7299 - val_loss: 0.5199 - val_accuracy: 0.7300\n",
            "Epoch 1862/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7296 - val_loss: 0.5215 - val_accuracy: 0.7284\n",
            "Epoch 1863/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7300 - val_loss: 0.5206 - val_accuracy: 0.7290\n",
            "Epoch 1864/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7285 - val_loss: 0.5211 - val_accuracy: 0.7283\n",
            "Epoch 1865/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7290 - val_loss: 0.5198 - val_accuracy: 0.7291\n",
            "Epoch 1866/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7308 - val_loss: 0.5193 - val_accuracy: 0.7286\n",
            "Epoch 1867/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7296 - val_loss: 0.5188 - val_accuracy: 0.7293\n",
            "Epoch 1868/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7293 - val_loss: 0.5200 - val_accuracy: 0.7302\n",
            "Epoch 1869/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7299 - val_loss: 0.5199 - val_accuracy: 0.7282\n",
            "Epoch 1870/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7301 - val_loss: 0.5182 - val_accuracy: 0.7288\n",
            "Epoch 1871/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7297 - val_loss: 0.5193 - val_accuracy: 0.7293\n",
            "Epoch 1872/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7305 - val_loss: 0.5190 - val_accuracy: 0.7280\n",
            "Epoch 1873/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7296 - val_loss: 0.5200 - val_accuracy: 0.7295\n",
            "Epoch 1874/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7298 - val_loss: 0.5214 - val_accuracy: 0.7293\n",
            "Epoch 1875/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7295 - val_loss: 0.5201 - val_accuracy: 0.7290\n",
            "Epoch 1876/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7300 - val_loss: 0.5193 - val_accuracy: 0.7300\n",
            "Epoch 1877/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7312 - val_loss: 0.5206 - val_accuracy: 0.7303\n",
            "Epoch 1878/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7318 - val_loss: 0.5187 - val_accuracy: 0.7297\n",
            "Epoch 1879/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7299 - val_loss: 0.5210 - val_accuracy: 0.7296\n",
            "Epoch 1880/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7303 - val_loss: 0.5191 - val_accuracy: 0.7288\n",
            "Epoch 1881/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7295 - val_loss: 0.5193 - val_accuracy: 0.7301\n",
            "Epoch 1882/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7304 - val_loss: 0.5195 - val_accuracy: 0.7299\n",
            "Epoch 1883/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7306 - val_loss: 0.5203 - val_accuracy: 0.7290\n",
            "Epoch 1884/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7295 - val_loss: 0.5202 - val_accuracy: 0.7286\n",
            "Epoch 1885/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7306 - val_loss: 0.5198 - val_accuracy: 0.7296\n",
            "Epoch 1886/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7311 - val_loss: 0.5200 - val_accuracy: 0.7300\n",
            "Epoch 1887/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7295 - val_loss: 0.5199 - val_accuracy: 0.7293\n",
            "Epoch 1888/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7302 - val_loss: 0.5199 - val_accuracy: 0.7274\n",
            "Epoch 1889/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7291 - val_loss: 0.5221 - val_accuracy: 0.7287\n",
            "Epoch 1890/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7292 - val_loss: 0.5211 - val_accuracy: 0.7296\n",
            "Epoch 1891/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5111 - accuracy: 0.7294 - val_loss: 0.5197 - val_accuracy: 0.7298\n",
            "Epoch 1892/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7295 - val_loss: 0.5200 - val_accuracy: 0.7289\n",
            "Epoch 1893/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7297 - val_loss: 0.5205 - val_accuracy: 0.7287\n",
            "Epoch 1894/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7290 - val_loss: 0.5214 - val_accuracy: 0.7291\n",
            "Epoch 1895/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7303 - val_loss: 0.5197 - val_accuracy: 0.7295\n",
            "Epoch 1896/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7317 - val_loss: 0.5203 - val_accuracy: 0.7297\n",
            "Epoch 1897/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7313 - val_loss: 0.5200 - val_accuracy: 0.7285\n",
            "Epoch 1898/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7307 - val_loss: 0.5206 - val_accuracy: 0.7296\n",
            "Epoch 1899/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7308 - val_loss: 0.5212 - val_accuracy: 0.7287\n",
            "Epoch 1900/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7292 - val_loss: 0.5198 - val_accuracy: 0.7307\n",
            "Epoch 1901/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7309 - val_loss: 0.5193 - val_accuracy: 0.7301\n",
            "Epoch 1902/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7303 - val_loss: 0.5204 - val_accuracy: 0.7292\n",
            "Epoch 1903/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7305 - val_loss: 0.5201 - val_accuracy: 0.7292\n",
            "Epoch 1904/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7305 - val_loss: 0.5207 - val_accuracy: 0.7289\n",
            "Epoch 1905/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7292 - val_loss: 0.5232 - val_accuracy: 0.7285\n",
            "Epoch 1906/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7306 - val_loss: 0.5197 - val_accuracy: 0.7300\n",
            "Epoch 1907/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7312 - val_loss: 0.5229 - val_accuracy: 0.7288\n",
            "Epoch 1908/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7298 - val_loss: 0.5199 - val_accuracy: 0.7306\n",
            "Epoch 1909/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7301 - val_loss: 0.5187 - val_accuracy: 0.7305\n",
            "Epoch 1910/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7304 - val_loss: 0.5201 - val_accuracy: 0.7296\n",
            "Epoch 1911/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7288 - val_loss: 0.5204 - val_accuracy: 0.7288\n",
            "Epoch 1912/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7292 - val_loss: 0.5181 - val_accuracy: 0.7305\n",
            "Epoch 1913/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5116 - accuracy: 0.7297 - val_loss: 0.5211 - val_accuracy: 0.7293\n",
            "Epoch 1914/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7300 - val_loss: 0.5210 - val_accuracy: 0.7303\n",
            "Epoch 1915/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7304 - val_loss: 0.5208 - val_accuracy: 0.7304\n",
            "Epoch 1916/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7294 - val_loss: 0.5192 - val_accuracy: 0.7301\n",
            "Epoch 1917/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7313 - val_loss: 0.5185 - val_accuracy: 0.7295\n",
            "Epoch 1918/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7301 - val_loss: 0.5204 - val_accuracy: 0.7299\n",
            "Epoch 1919/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7297 - val_loss: 0.5199 - val_accuracy: 0.7293\n",
            "Epoch 1920/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7302 - val_loss: 0.5204 - val_accuracy: 0.7297\n",
            "Epoch 1921/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7301 - val_loss: 0.5199 - val_accuracy: 0.7294\n",
            "Epoch 1922/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7290 - val_loss: 0.5184 - val_accuracy: 0.7299\n",
            "Epoch 1923/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7305 - val_loss: 0.5210 - val_accuracy: 0.7272\n",
            "Epoch 1924/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7307 - val_loss: 0.5214 - val_accuracy: 0.7284\n",
            "Epoch 1925/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7287 - val_loss: 0.5194 - val_accuracy: 0.7309\n",
            "Epoch 1926/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7305 - val_loss: 0.5212 - val_accuracy: 0.7293\n",
            "Epoch 1927/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7303 - val_loss: 0.5200 - val_accuracy: 0.7295\n",
            "Epoch 1928/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7295 - val_loss: 0.5188 - val_accuracy: 0.7295\n",
            "Epoch 1929/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7292 - val_loss: 0.5183 - val_accuracy: 0.7306\n",
            "Epoch 1930/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7298 - val_loss: 0.5198 - val_accuracy: 0.7294\n",
            "Epoch 1931/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7304 - val_loss: 0.5218 - val_accuracy: 0.7298\n",
            "Epoch 1932/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7301 - val_loss: 0.5224 - val_accuracy: 0.7283\n",
            "Epoch 1933/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7305 - val_loss: 0.5200 - val_accuracy: 0.7294\n",
            "Epoch 1934/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7299 - val_loss: 0.5206 - val_accuracy: 0.7292\n",
            "Epoch 1935/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7309 - val_loss: 0.5212 - val_accuracy: 0.7290\n",
            "Epoch 1936/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7303 - val_loss: 0.5202 - val_accuracy: 0.7305\n",
            "Epoch 1937/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7310 - val_loss: 0.5229 - val_accuracy: 0.7285\n",
            "Epoch 1938/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7303 - val_loss: 0.5207 - val_accuracy: 0.7292\n",
            "Epoch 1939/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7307 - val_loss: 0.5206 - val_accuracy: 0.7278\n",
            "Epoch 1940/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7289 - val_loss: 0.5214 - val_accuracy: 0.7286\n",
            "Epoch 1941/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7309 - val_loss: 0.5204 - val_accuracy: 0.7287\n",
            "Epoch 1942/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7312 - val_loss: 0.5204 - val_accuracy: 0.7287\n",
            "Epoch 1943/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7312 - val_loss: 0.5201 - val_accuracy: 0.7291\n",
            "Epoch 1944/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7308 - val_loss: 0.5220 - val_accuracy: 0.7287\n",
            "Epoch 1945/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7302 - val_loss: 0.5202 - val_accuracy: 0.7295\n",
            "Epoch 1946/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7300 - val_loss: 0.5200 - val_accuracy: 0.7296\n",
            "Epoch 1947/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7291 - val_loss: 0.5198 - val_accuracy: 0.7294\n",
            "Epoch 1948/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7303 - val_loss: 0.5230 - val_accuracy: 0.7288\n",
            "Epoch 1949/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7293 - val_loss: 0.5196 - val_accuracy: 0.7291\n",
            "Epoch 1950/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7294 - val_loss: 0.5204 - val_accuracy: 0.7298\n",
            "Epoch 1951/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7303 - val_loss: 0.5205 - val_accuracy: 0.7283\n",
            "Epoch 1952/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7304 - val_loss: 0.5207 - val_accuracy: 0.7296\n",
            "Epoch 1953/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7308 - val_loss: 0.5214 - val_accuracy: 0.7289\n",
            "Epoch 1954/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7307 - val_loss: 0.5202 - val_accuracy: 0.7291\n",
            "Epoch 1955/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7297 - val_loss: 0.5198 - val_accuracy: 0.7298\n",
            "Epoch 1956/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7312 - val_loss: 0.5202 - val_accuracy: 0.7294\n",
            "Epoch 1957/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7296 - val_loss: 0.5215 - val_accuracy: 0.7288\n",
            "Epoch 1958/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7301 - val_loss: 0.5191 - val_accuracy: 0.7294\n",
            "Epoch 1959/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7309 - val_loss: 0.5223 - val_accuracy: 0.7291\n",
            "Epoch 1960/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7311 - val_loss: 0.5206 - val_accuracy: 0.7301\n",
            "Epoch 1961/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7309 - val_loss: 0.5223 - val_accuracy: 0.7284\n",
            "Epoch 1962/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7299 - val_loss: 0.5211 - val_accuracy: 0.7301\n",
            "Epoch 1963/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7300 - val_loss: 0.5218 - val_accuracy: 0.7303\n",
            "Epoch 1964/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7305 - val_loss: 0.5203 - val_accuracy: 0.7295\n",
            "Epoch 1965/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7301 - val_loss: 0.5196 - val_accuracy: 0.7305\n",
            "Epoch 1966/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7291 - val_loss: 0.5200 - val_accuracy: 0.7298\n",
            "Epoch 1967/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7299 - val_loss: 0.5199 - val_accuracy: 0.7298\n",
            "Epoch 1968/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7302 - val_loss: 0.5186 - val_accuracy: 0.7304\n",
            "Epoch 1969/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7296 - val_loss: 0.5194 - val_accuracy: 0.7296\n",
            "Epoch 1970/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7307 - val_loss: 0.5204 - val_accuracy: 0.7307\n",
            "Epoch 1971/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7294 - val_loss: 0.5207 - val_accuracy: 0.7299\n",
            "Epoch 1972/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5115 - accuracy: 0.7288 - val_loss: 0.5207 - val_accuracy: 0.7280\n",
            "Epoch 1973/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7302 - val_loss: 0.5199 - val_accuracy: 0.7293\n",
            "Epoch 1974/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7293 - val_loss: 0.5208 - val_accuracy: 0.7292\n",
            "Epoch 1975/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7308 - val_loss: 0.5216 - val_accuracy: 0.7290\n",
            "Epoch 1976/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7302 - val_loss: 0.5206 - val_accuracy: 0.7284\n",
            "Epoch 1977/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7292 - val_loss: 0.5229 - val_accuracy: 0.7287\n",
            "Epoch 1978/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7295 - val_loss: 0.5214 - val_accuracy: 0.7290\n",
            "Epoch 1979/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7302 - val_loss: 0.5212 - val_accuracy: 0.7286\n",
            "Epoch 1980/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7295 - val_loss: 0.5192 - val_accuracy: 0.7302\n",
            "Epoch 1981/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7315 - val_loss: 0.5198 - val_accuracy: 0.7298\n",
            "Epoch 1982/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7315 - val_loss: 0.5214 - val_accuracy: 0.7281\n",
            "Epoch 1983/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7303 - val_loss: 0.5206 - val_accuracy: 0.7291\n",
            "Epoch 1984/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7302 - val_loss: 0.5204 - val_accuracy: 0.7292\n",
            "Epoch 1985/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7298 - val_loss: 0.5208 - val_accuracy: 0.7282\n",
            "Epoch 1986/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7299 - val_loss: 0.5218 - val_accuracy: 0.7293\n",
            "Epoch 1987/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7298 - val_loss: 0.5211 - val_accuracy: 0.7290\n",
            "Epoch 1988/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7298 - val_loss: 0.5201 - val_accuracy: 0.7300\n",
            "Epoch 1989/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7305 - val_loss: 0.5202 - val_accuracy: 0.7297\n",
            "Epoch 1990/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7300 - val_loss: 0.5207 - val_accuracy: 0.7295\n",
            "Epoch 1991/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7299 - val_loss: 0.5181 - val_accuracy: 0.7287\n",
            "Epoch 1992/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7302 - val_loss: 0.5207 - val_accuracy: 0.7289\n",
            "Epoch 1993/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7313 - val_loss: 0.5198 - val_accuracy: 0.7293\n",
            "Epoch 1994/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5114 - accuracy: 0.7303 - val_loss: 0.5209 - val_accuracy: 0.7297\n",
            "Epoch 1995/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7308 - val_loss: 0.5210 - val_accuracy: 0.7298\n",
            "Epoch 1996/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7305 - val_loss: 0.5209 - val_accuracy: 0.7285\n",
            "Epoch 1997/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7308 - val_loss: 0.5203 - val_accuracy: 0.7298\n",
            "Epoch 1998/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7305 - val_loss: 0.5185 - val_accuracy: 0.7306\n",
            "Epoch 1999/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7300 - val_loss: 0.5188 - val_accuracy: 0.7314\n",
            "Epoch 2000/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7302 - val_loss: 0.5209 - val_accuracy: 0.7301\n",
            "Epoch 2001/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7305 - val_loss: 0.5182 - val_accuracy: 0.7301\n",
            "Epoch 2002/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7304 - val_loss: 0.5202 - val_accuracy: 0.7297\n",
            "Epoch 2003/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7302 - val_loss: 0.5188 - val_accuracy: 0.7299\n",
            "Epoch 2004/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7308 - val_loss: 0.5211 - val_accuracy: 0.7288\n",
            "Epoch 2005/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7301 - val_loss: 0.5215 - val_accuracy: 0.7296\n",
            "Epoch 2006/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7308 - val_loss: 0.5190 - val_accuracy: 0.7291\n",
            "Epoch 2007/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7300 - val_loss: 0.5194 - val_accuracy: 0.7287\n",
            "Epoch 2008/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5113 - accuracy: 0.7293 - val_loss: 0.5196 - val_accuracy: 0.7300\n",
            "Epoch 2009/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7302 - val_loss: 0.5198 - val_accuracy: 0.7291\n",
            "Epoch 2010/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7303 - val_loss: 0.5194 - val_accuracy: 0.7296\n",
            "Epoch 2011/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7306 - val_loss: 0.5209 - val_accuracy: 0.7287\n",
            "Epoch 2012/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7304 - val_loss: 0.5210 - val_accuracy: 0.7295\n",
            "Epoch 2013/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7308 - val_loss: 0.5209 - val_accuracy: 0.7293\n",
            "Epoch 2014/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7303 - val_loss: 0.5212 - val_accuracy: 0.7292\n",
            "Epoch 2015/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7301 - val_loss: 0.5203 - val_accuracy: 0.7278\n",
            "Epoch 2016/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7291 - val_loss: 0.5202 - val_accuracy: 0.7293\n",
            "Epoch 2017/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7314 - val_loss: 0.5194 - val_accuracy: 0.7310\n",
            "Epoch 2018/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7299 - val_loss: 0.5203 - val_accuracy: 0.7303\n",
            "Epoch 2019/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7304 - val_loss: 0.5215 - val_accuracy: 0.7284\n",
            "Epoch 2020/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7296 - val_loss: 0.5205 - val_accuracy: 0.7306\n",
            "Epoch 2021/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7300 - val_loss: 0.5217 - val_accuracy: 0.7299\n",
            "Epoch 2022/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7303 - val_loss: 0.5208 - val_accuracy: 0.7300\n",
            "Epoch 2023/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7306 - val_loss: 0.5192 - val_accuracy: 0.7300\n",
            "Epoch 2024/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7292 - val_loss: 0.5190 - val_accuracy: 0.7297\n",
            "Epoch 2025/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7301 - val_loss: 0.5224 - val_accuracy: 0.7286\n",
            "Epoch 2026/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7297 - val_loss: 0.5195 - val_accuracy: 0.7305\n",
            "Epoch 2027/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7309 - val_loss: 0.5212 - val_accuracy: 0.7305\n",
            "Epoch 2028/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7312 - val_loss: 0.5233 - val_accuracy: 0.7299\n",
            "Epoch 2029/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7312 - val_loss: 0.5210 - val_accuracy: 0.7293\n",
            "Epoch 2030/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7298 - val_loss: 0.5198 - val_accuracy: 0.7313\n",
            "Epoch 2031/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7310 - val_loss: 0.5217 - val_accuracy: 0.7301\n",
            "Epoch 2032/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7314 - val_loss: 0.5200 - val_accuracy: 0.7301\n",
            "Epoch 2033/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7306 - val_loss: 0.5237 - val_accuracy: 0.7281\n",
            "Epoch 2034/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7316 - val_loss: 0.5190 - val_accuracy: 0.7302\n",
            "Epoch 2035/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7304 - val_loss: 0.5185 - val_accuracy: 0.7288\n",
            "Epoch 2036/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7307 - val_loss: 0.5203 - val_accuracy: 0.7296\n",
            "Epoch 2037/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7301 - val_loss: 0.5203 - val_accuracy: 0.7289\n",
            "Epoch 2038/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7307 - val_loss: 0.5194 - val_accuracy: 0.7300\n",
            "Epoch 2039/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7303 - val_loss: 0.5211 - val_accuracy: 0.7286\n",
            "Epoch 2040/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7303 - val_loss: 0.5208 - val_accuracy: 0.7294\n",
            "Epoch 2041/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7302 - val_loss: 0.5200 - val_accuracy: 0.7302\n",
            "Epoch 2042/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7289 - val_loss: 0.5205 - val_accuracy: 0.7281\n",
            "Epoch 2043/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7300 - val_loss: 0.5214 - val_accuracy: 0.7289\n",
            "Epoch 2044/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7310 - val_loss: 0.5220 - val_accuracy: 0.7288\n",
            "Epoch 2045/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7305 - val_loss: 0.5201 - val_accuracy: 0.7294\n",
            "Epoch 2046/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7295 - val_loss: 0.5205 - val_accuracy: 0.7292\n",
            "Epoch 2047/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7312 - val_loss: 0.5207 - val_accuracy: 0.7305\n",
            "Epoch 2048/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7296 - val_loss: 0.5208 - val_accuracy: 0.7287\n",
            "Epoch 2049/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7312 - val_loss: 0.5186 - val_accuracy: 0.7305\n",
            "Epoch 2050/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7304 - val_loss: 0.5217 - val_accuracy: 0.7293\n",
            "Epoch 2051/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7307 - val_loss: 0.5202 - val_accuracy: 0.7297\n",
            "Epoch 2052/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7300 - val_loss: 0.5207 - val_accuracy: 0.7295\n",
            "Epoch 2053/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7301 - val_loss: 0.5222 - val_accuracy: 0.7292\n",
            "Epoch 2054/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7308 - val_loss: 0.5196 - val_accuracy: 0.7293\n",
            "Epoch 2055/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7308 - val_loss: 0.5218 - val_accuracy: 0.7297\n",
            "Epoch 2056/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7312 - val_loss: 0.5212 - val_accuracy: 0.7294\n",
            "Epoch 2057/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7298 - val_loss: 0.5207 - val_accuracy: 0.7297\n",
            "Epoch 2058/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7310 - val_loss: 0.5238 - val_accuracy: 0.7283\n",
            "Epoch 2059/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7302 - val_loss: 0.5209 - val_accuracy: 0.7295\n",
            "Epoch 2060/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5108 - accuracy: 0.7294 - val_loss: 0.5219 - val_accuracy: 0.7286\n",
            "Epoch 2061/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7309 - val_loss: 0.5202 - val_accuracy: 0.7303\n",
            "Epoch 2062/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7311 - val_loss: 0.5216 - val_accuracy: 0.7291\n",
            "Epoch 2063/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7309 - val_loss: 0.5235 - val_accuracy: 0.7278\n",
            "Epoch 2064/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7305 - val_loss: 0.5192 - val_accuracy: 0.7295\n",
            "Epoch 2065/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7305 - val_loss: 0.5208 - val_accuracy: 0.7297\n",
            "Epoch 2066/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7307 - val_loss: 0.5206 - val_accuracy: 0.7311\n",
            "Epoch 2067/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7312 - val_loss: 0.5202 - val_accuracy: 0.7295\n",
            "Epoch 2068/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7308 - val_loss: 0.5200 - val_accuracy: 0.7308\n",
            "Epoch 2069/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7311 - val_loss: 0.5213 - val_accuracy: 0.7294\n",
            "Epoch 2070/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7305 - val_loss: 0.5192 - val_accuracy: 0.7289\n",
            "Epoch 2071/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7298 - val_loss: 0.5200 - val_accuracy: 0.7297\n",
            "Epoch 2072/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7307 - val_loss: 0.5193 - val_accuracy: 0.7295\n",
            "Epoch 2073/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7299 - val_loss: 0.5187 - val_accuracy: 0.7312\n",
            "Epoch 2074/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7309 - val_loss: 0.5221 - val_accuracy: 0.7282\n",
            "Epoch 2075/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7308 - val_loss: 0.5215 - val_accuracy: 0.7294\n",
            "Epoch 2076/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7293 - val_loss: 0.5198 - val_accuracy: 0.7295\n",
            "Epoch 2077/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7307 - val_loss: 0.5226 - val_accuracy: 0.7287\n",
            "Epoch 2078/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5110 - accuracy: 0.7307 - val_loss: 0.5196 - val_accuracy: 0.7299\n",
            "Epoch 2079/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7300 - val_loss: 0.5230 - val_accuracy: 0.7290\n",
            "Epoch 2080/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7301 - val_loss: 0.5213 - val_accuracy: 0.7297\n",
            "Epoch 2081/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7298 - val_loss: 0.5203 - val_accuracy: 0.7296\n",
            "Epoch 2082/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7311 - val_loss: 0.5212 - val_accuracy: 0.7291\n",
            "Epoch 2083/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7308 - val_loss: 0.5225 - val_accuracy: 0.7298\n",
            "Epoch 2084/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7307 - val_loss: 0.5216 - val_accuracy: 0.7304\n",
            "Epoch 2085/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7295 - val_loss: 0.5218 - val_accuracy: 0.7289\n",
            "Epoch 2086/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7303 - val_loss: 0.5201 - val_accuracy: 0.7283\n",
            "Epoch 2087/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7308 - val_loss: 0.5194 - val_accuracy: 0.7298\n",
            "Epoch 2088/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7315 - val_loss: 0.5205 - val_accuracy: 0.7296\n",
            "Epoch 2089/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7311 - val_loss: 0.5209 - val_accuracy: 0.7301\n",
            "Epoch 2090/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7310 - val_loss: 0.5187 - val_accuracy: 0.7309\n",
            "Epoch 2091/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7308 - val_loss: 0.5194 - val_accuracy: 0.7302\n",
            "Epoch 2092/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7303 - val_loss: 0.5203 - val_accuracy: 0.7296\n",
            "Epoch 2093/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7305 - val_loss: 0.5189 - val_accuracy: 0.7300\n",
            "Epoch 2094/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7309 - val_loss: 0.5207 - val_accuracy: 0.7274\n",
            "Epoch 2095/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7303 - val_loss: 0.5207 - val_accuracy: 0.7291\n",
            "Epoch 2096/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7295 - val_loss: 0.5200 - val_accuracy: 0.7300\n",
            "Epoch 2097/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7299 - val_loss: 0.5190 - val_accuracy: 0.7305\n",
            "Epoch 2098/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7310 - val_loss: 0.5197 - val_accuracy: 0.7306\n",
            "Epoch 2099/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7301 - val_loss: 0.5205 - val_accuracy: 0.7294\n",
            "Epoch 2100/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7306 - val_loss: 0.5179 - val_accuracy: 0.7300\n",
            "Epoch 2101/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7297 - val_loss: 0.5210 - val_accuracy: 0.7309\n",
            "Epoch 2102/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7304 - val_loss: 0.5209 - val_accuracy: 0.7287\n",
            "Epoch 2103/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7291 - val_loss: 0.5209 - val_accuracy: 0.7302\n",
            "Epoch 2104/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7307 - val_loss: 0.5185 - val_accuracy: 0.7300\n",
            "Epoch 2105/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7291 - val_loss: 0.5219 - val_accuracy: 0.7297\n",
            "Epoch 2106/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7300 - val_loss: 0.5197 - val_accuracy: 0.7305\n",
            "Epoch 2107/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7298 - val_loss: 0.5183 - val_accuracy: 0.7291\n",
            "Epoch 2108/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7312 - val_loss: 0.5194 - val_accuracy: 0.7305\n",
            "Epoch 2109/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7308 - val_loss: 0.5203 - val_accuracy: 0.7306\n",
            "Epoch 2110/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7313 - val_loss: 0.5189 - val_accuracy: 0.7300\n",
            "Epoch 2111/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5112 - accuracy: 0.7303 - val_loss: 0.5210 - val_accuracy: 0.7298\n",
            "Epoch 2112/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7307 - val_loss: 0.5214 - val_accuracy: 0.7296\n",
            "Epoch 2113/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7308 - val_loss: 0.5210 - val_accuracy: 0.7285\n",
            "Epoch 2114/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7310 - val_loss: 0.5208 - val_accuracy: 0.7291\n",
            "Epoch 2115/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7309 - val_loss: 0.5204 - val_accuracy: 0.7301\n",
            "Epoch 2116/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7301 - val_loss: 0.5202 - val_accuracy: 0.7313\n",
            "Epoch 2117/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7309 - val_loss: 0.5196 - val_accuracy: 0.7303\n",
            "Epoch 2118/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7300 - val_loss: 0.5198 - val_accuracy: 0.7304\n",
            "Epoch 2119/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7297 - val_loss: 0.5197 - val_accuracy: 0.7297\n",
            "Epoch 2120/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7293 - val_loss: 0.5189 - val_accuracy: 0.7305\n",
            "Epoch 2121/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7310 - val_loss: 0.5190 - val_accuracy: 0.7305\n",
            "Epoch 2122/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7305 - val_loss: 0.5201 - val_accuracy: 0.7302\n",
            "Epoch 2123/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7294 - val_loss: 0.5198 - val_accuracy: 0.7308\n",
            "Epoch 2124/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7311 - val_loss: 0.5192 - val_accuracy: 0.7309\n",
            "Epoch 2125/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7305 - val_loss: 0.5209 - val_accuracy: 0.7303\n",
            "Epoch 2126/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7307 - val_loss: 0.5206 - val_accuracy: 0.7303\n",
            "Epoch 2127/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7309 - val_loss: 0.5238 - val_accuracy: 0.7284\n",
            "Epoch 2128/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7299 - val_loss: 0.5193 - val_accuracy: 0.7303\n",
            "Epoch 2129/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5109 - accuracy: 0.7300 - val_loss: 0.5196 - val_accuracy: 0.7305\n",
            "Epoch 2130/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7301 - val_loss: 0.5200 - val_accuracy: 0.7308\n",
            "Epoch 2131/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7318 - val_loss: 0.5179 - val_accuracy: 0.7305\n",
            "Epoch 2132/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7304 - val_loss: 0.5191 - val_accuracy: 0.7301\n",
            "Epoch 2133/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7301 - val_loss: 0.5218 - val_accuracy: 0.7306\n",
            "Epoch 2134/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7311 - val_loss: 0.5209 - val_accuracy: 0.7307\n",
            "Epoch 2135/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7314 - val_loss: 0.5205 - val_accuracy: 0.7299\n",
            "Epoch 2136/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7307 - val_loss: 0.5202 - val_accuracy: 0.7293\n",
            "Epoch 2137/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7303 - val_loss: 0.5208 - val_accuracy: 0.7300\n",
            "Epoch 2138/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7295 - val_loss: 0.5211 - val_accuracy: 0.7289\n",
            "Epoch 2139/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7314 - val_loss: 0.5204 - val_accuracy: 0.7291\n",
            "Epoch 2140/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7321 - val_loss: 0.5224 - val_accuracy: 0.7296\n",
            "Epoch 2141/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7314 - val_loss: 0.5193 - val_accuracy: 0.7296\n",
            "Epoch 2142/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7306 - val_loss: 0.5184 - val_accuracy: 0.7294\n",
            "Epoch 2143/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7298 - val_loss: 0.5208 - val_accuracy: 0.7291\n",
            "Epoch 2144/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7308 - val_loss: 0.5202 - val_accuracy: 0.7301\n",
            "Epoch 2145/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7313 - val_loss: 0.5212 - val_accuracy: 0.7308\n",
            "Epoch 2146/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7298 - val_loss: 0.5198 - val_accuracy: 0.7302\n",
            "Epoch 2147/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7304 - val_loss: 0.5200 - val_accuracy: 0.7312\n",
            "Epoch 2148/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7310 - val_loss: 0.5210 - val_accuracy: 0.7294\n",
            "Epoch 2149/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7313 - val_loss: 0.5193 - val_accuracy: 0.7309\n",
            "Epoch 2150/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7313 - val_loss: 0.5209 - val_accuracy: 0.7297\n",
            "Epoch 2151/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7316 - val_loss: 0.5199 - val_accuracy: 0.7302\n",
            "Epoch 2152/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7301 - val_loss: 0.5201 - val_accuracy: 0.7294\n",
            "Epoch 2153/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7308 - val_loss: 0.5199 - val_accuracy: 0.7305\n",
            "Epoch 2154/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7302 - val_loss: 0.5194 - val_accuracy: 0.7305\n",
            "Epoch 2155/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7310 - val_loss: 0.5205 - val_accuracy: 0.7291\n",
            "Epoch 2156/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7303 - val_loss: 0.5194 - val_accuracy: 0.7298\n",
            "Epoch 2157/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7310 - val_loss: 0.5201 - val_accuracy: 0.7296\n",
            "Epoch 2158/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7313 - val_loss: 0.5199 - val_accuracy: 0.7309\n",
            "Epoch 2159/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7310 - val_loss: 0.5198 - val_accuracy: 0.7298\n",
            "Epoch 2160/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7305 - val_loss: 0.5207 - val_accuracy: 0.7297\n",
            "Epoch 2161/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7307 - val_loss: 0.5206 - val_accuracy: 0.7299\n",
            "Epoch 2162/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7313 - val_loss: 0.5225 - val_accuracy: 0.7286\n",
            "Epoch 2163/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7302 - val_loss: 0.5214 - val_accuracy: 0.7289\n",
            "Epoch 2164/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7305 - val_loss: 0.5213 - val_accuracy: 0.7282\n",
            "Epoch 2165/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7307 - val_loss: 0.5214 - val_accuracy: 0.7300\n",
            "Epoch 2166/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7303 - val_loss: 0.5192 - val_accuracy: 0.7306\n",
            "Epoch 2167/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7306 - val_loss: 0.5220 - val_accuracy: 0.7298\n",
            "Epoch 2168/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7304 - val_loss: 0.5200 - val_accuracy: 0.7290\n",
            "Epoch 2169/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7302 - val_loss: 0.5210 - val_accuracy: 0.7287\n",
            "Epoch 2170/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7305 - val_loss: 0.5204 - val_accuracy: 0.7300\n",
            "Epoch 2171/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7312 - val_loss: 0.5207 - val_accuracy: 0.7286\n",
            "Epoch 2172/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7306 - val_loss: 0.5210 - val_accuracy: 0.7302\n",
            "Epoch 2173/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7309 - val_loss: 0.5196 - val_accuracy: 0.7304\n",
            "Epoch 2174/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7305 - val_loss: 0.5214 - val_accuracy: 0.7290\n",
            "Epoch 2175/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7308 - val_loss: 0.5192 - val_accuracy: 0.7299\n",
            "Epoch 2176/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7303 - val_loss: 0.5210 - val_accuracy: 0.7291\n",
            "Epoch 2177/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7303 - val_loss: 0.5218 - val_accuracy: 0.7295\n",
            "Epoch 2178/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7300 - val_loss: 0.5212 - val_accuracy: 0.7298\n",
            "Epoch 2179/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7307 - val_loss: 0.5221 - val_accuracy: 0.7287\n",
            "Epoch 2180/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7314 - val_loss: 0.5222 - val_accuracy: 0.7287\n",
            "Epoch 2181/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7308 - val_loss: 0.5213 - val_accuracy: 0.7293\n",
            "Epoch 2182/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7317 - val_loss: 0.5225 - val_accuracy: 0.7281\n",
            "Epoch 2183/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7323 - val_loss: 0.5210 - val_accuracy: 0.7296\n",
            "Epoch 2184/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7308 - val_loss: 0.5189 - val_accuracy: 0.7293\n",
            "Epoch 2185/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7312 - val_loss: 0.5184 - val_accuracy: 0.7299\n",
            "Epoch 2186/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7300 - val_loss: 0.5209 - val_accuracy: 0.7296\n",
            "Epoch 2187/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7309 - val_loss: 0.5199 - val_accuracy: 0.7299\n",
            "Epoch 2188/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7309 - val_loss: 0.5204 - val_accuracy: 0.7300\n",
            "Epoch 2189/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7314 - val_loss: 0.5202 - val_accuracy: 0.7307\n",
            "Epoch 2190/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7310 - val_loss: 0.5213 - val_accuracy: 0.7288\n",
            "Epoch 2191/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7304 - val_loss: 0.5196 - val_accuracy: 0.7290\n",
            "Epoch 2192/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7304 - val_loss: 0.5199 - val_accuracy: 0.7310\n",
            "Epoch 2193/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7309 - val_loss: 0.5199 - val_accuracy: 0.7310\n",
            "Epoch 2194/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7302 - val_loss: 0.5215 - val_accuracy: 0.7295\n",
            "Epoch 2195/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7321 - val_loss: 0.5206 - val_accuracy: 0.7290\n",
            "Epoch 2196/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7312 - val_loss: 0.5202 - val_accuracy: 0.7301\n",
            "Epoch 2197/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7315 - val_loss: 0.5240 - val_accuracy: 0.7277\n",
            "Epoch 2198/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7316 - val_loss: 0.5225 - val_accuracy: 0.7288\n",
            "Epoch 2199/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - accuracy: 0.7315 - val_loss: 0.5214 - val_accuracy: 0.7289\n",
            "Epoch 2200/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7301 - val_loss: 0.5211 - val_accuracy: 0.7291\n",
            "Epoch 2201/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7306 - val_loss: 0.5209 - val_accuracy: 0.7296\n",
            "Epoch 2202/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7307 - val_loss: 0.5194 - val_accuracy: 0.7290\n",
            "Epoch 2203/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7301 - val_loss: 0.5207 - val_accuracy: 0.7300\n",
            "Epoch 2204/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7310 - val_loss: 0.5209 - val_accuracy: 0.7288\n",
            "Epoch 2205/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7308 - val_loss: 0.5199 - val_accuracy: 0.7303\n",
            "Epoch 2206/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7309 - val_loss: 0.5214 - val_accuracy: 0.7294\n",
            "Epoch 2207/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7310 - val_loss: 0.5196 - val_accuracy: 0.7303\n",
            "Epoch 2208/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7304 - val_loss: 0.5203 - val_accuracy: 0.7295\n",
            "Epoch 2209/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7309 - val_loss: 0.5209 - val_accuracy: 0.7299\n",
            "Epoch 2210/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7310 - val_loss: 0.5208 - val_accuracy: 0.7303\n",
            "Epoch 2211/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7310 - val_loss: 0.5207 - val_accuracy: 0.7299\n",
            "Epoch 2212/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7321 - val_loss: 0.5233 - val_accuracy: 0.7289\n",
            "Epoch 2213/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7315 - val_loss: 0.5214 - val_accuracy: 0.7303\n",
            "Epoch 2214/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7310 - val_loss: 0.5219 - val_accuracy: 0.7284\n",
            "Epoch 2215/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7310 - val_loss: 0.5216 - val_accuracy: 0.7291\n",
            "Epoch 2216/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7297 - val_loss: 0.5210 - val_accuracy: 0.7293\n",
            "Epoch 2217/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7302 - val_loss: 0.5204 - val_accuracy: 0.7295\n",
            "Epoch 2218/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7308 - val_loss: 0.5210 - val_accuracy: 0.7296\n",
            "Epoch 2219/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7299 - val_loss: 0.5206 - val_accuracy: 0.7304\n",
            "Epoch 2220/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7305 - val_loss: 0.5205 - val_accuracy: 0.7309\n",
            "Epoch 2221/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5106 - accuracy: 0.7306 - val_loss: 0.5203 - val_accuracy: 0.7288\n",
            "Epoch 2222/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7305 - val_loss: 0.5210 - val_accuracy: 0.7308\n",
            "Epoch 2223/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7309 - val_loss: 0.5199 - val_accuracy: 0.7305\n",
            "Epoch 2224/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7308 - val_loss: 0.5202 - val_accuracy: 0.7297\n",
            "Epoch 2225/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7300 - val_loss: 0.5224 - val_accuracy: 0.7298\n",
            "Epoch 2226/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7306 - val_loss: 0.5215 - val_accuracy: 0.7303\n",
            "Epoch 2227/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7310 - val_loss: 0.5218 - val_accuracy: 0.7294\n",
            "Epoch 2228/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7306 - val_loss: 0.5203 - val_accuracy: 0.7295\n",
            "Epoch 2229/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7311 - val_loss: 0.5198 - val_accuracy: 0.7298\n",
            "Epoch 2230/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7302 - val_loss: 0.5229 - val_accuracy: 0.7302\n",
            "Epoch 2231/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7304 - val_loss: 0.5222 - val_accuracy: 0.7311\n",
            "Epoch 2232/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7313 - val_loss: 0.5233 - val_accuracy: 0.7282\n",
            "Epoch 2233/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7310 - val_loss: 0.5229 - val_accuracy: 0.7295\n",
            "Epoch 2234/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7313 - val_loss: 0.5228 - val_accuracy: 0.7289\n",
            "Epoch 2235/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7307 - val_loss: 0.5218 - val_accuracy: 0.7304\n",
            "Epoch 2236/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7319 - val_loss: 0.5211 - val_accuracy: 0.7291\n",
            "Epoch 2237/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7311 - val_loss: 0.5219 - val_accuracy: 0.7286\n",
            "Epoch 2238/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7306 - val_loss: 0.5206 - val_accuracy: 0.7298\n",
            "Epoch 2239/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7306 - val_loss: 0.5216 - val_accuracy: 0.7294\n",
            "Epoch 2240/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7299 - val_loss: 0.5210 - val_accuracy: 0.7299\n",
            "Epoch 2241/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7308 - val_loss: 0.5219 - val_accuracy: 0.7294\n",
            "Epoch 2242/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7310 - val_loss: 0.5217 - val_accuracy: 0.7298\n",
            "Epoch 2243/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - accuracy: 0.7323 - val_loss: 0.5210 - val_accuracy: 0.7300\n",
            "Epoch 2244/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7302 - val_loss: 0.5214 - val_accuracy: 0.7287\n",
            "Epoch 2245/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7303 - val_loss: 0.5205 - val_accuracy: 0.7304\n",
            "Epoch 2246/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7301 - val_loss: 0.5208 - val_accuracy: 0.7299\n",
            "Epoch 2247/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7304 - val_loss: 0.5221 - val_accuracy: 0.7294\n",
            "Epoch 2248/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7311 - val_loss: 0.5221 - val_accuracy: 0.7303\n",
            "Epoch 2249/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7314 - val_loss: 0.5231 - val_accuracy: 0.7293\n",
            "Epoch 2250/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7300 - val_loss: 0.5198 - val_accuracy: 0.7300\n",
            "Epoch 2251/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7317 - val_loss: 0.5212 - val_accuracy: 0.7296\n",
            "Epoch 2252/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7303 - val_loss: 0.5208 - val_accuracy: 0.7309\n",
            "Epoch 2253/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7308 - val_loss: 0.5202 - val_accuracy: 0.7305\n",
            "Epoch 2254/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7303 - val_loss: 0.5217 - val_accuracy: 0.7302\n",
            "Epoch 2255/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7309 - val_loss: 0.5225 - val_accuracy: 0.7297\n",
            "Epoch 2256/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7313 - val_loss: 0.5216 - val_accuracy: 0.7305\n",
            "Epoch 2257/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7311 - val_loss: 0.5203 - val_accuracy: 0.7299\n",
            "Epoch 2258/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7304 - val_loss: 0.5209 - val_accuracy: 0.7299\n",
            "Epoch 2259/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7306 - val_loss: 0.5221 - val_accuracy: 0.7299\n",
            "Epoch 2260/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7309 - val_loss: 0.5201 - val_accuracy: 0.7295\n",
            "Epoch 2261/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7301 - val_loss: 0.5232 - val_accuracy: 0.7294\n",
            "Epoch 2262/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5084 - accuracy: 0.7318 - val_loss: 0.5227 - val_accuracy: 0.7303\n",
            "Epoch 2263/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7312 - val_loss: 0.5213 - val_accuracy: 0.7299\n",
            "Epoch 2264/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - accuracy: 0.7308 - val_loss: 0.5235 - val_accuracy: 0.7296\n",
            "Epoch 2265/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7307 - val_loss: 0.5201 - val_accuracy: 0.7303\n",
            "Epoch 2266/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7310 - val_loss: 0.5220 - val_accuracy: 0.7300\n",
            "Epoch 2267/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7321 - val_loss: 0.5229 - val_accuracy: 0.7303\n",
            "Epoch 2268/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7313 - val_loss: 0.5214 - val_accuracy: 0.7297\n",
            "Epoch 2269/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7309 - val_loss: 0.5199 - val_accuracy: 0.7302\n",
            "Epoch 2270/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7305 - val_loss: 0.5199 - val_accuracy: 0.7291\n",
            "Epoch 2271/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5087 - accuracy: 0.7307 - val_loss: 0.5217 - val_accuracy: 0.7292\n",
            "Epoch 2272/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7300 - val_loss: 0.5239 - val_accuracy: 0.7286\n",
            "Epoch 2273/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7312 - val_loss: 0.5201 - val_accuracy: 0.7292\n",
            "Epoch 2274/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7305 - val_loss: 0.5212 - val_accuracy: 0.7287\n",
            "Epoch 2275/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7304 - val_loss: 0.5219 - val_accuracy: 0.7299\n",
            "Epoch 2276/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7314 - val_loss: 0.5243 - val_accuracy: 0.7286\n",
            "Epoch 2277/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7308 - val_loss: 0.5238 - val_accuracy: 0.7286\n",
            "Epoch 2278/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7313 - val_loss: 0.5205 - val_accuracy: 0.7292\n",
            "Epoch 2279/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7311 - val_loss: 0.5228 - val_accuracy: 0.7278\n",
            "Epoch 2280/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7306 - val_loss: 0.5223 - val_accuracy: 0.7302\n",
            "Epoch 2281/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7311 - val_loss: 0.5217 - val_accuracy: 0.7305\n",
            "Epoch 2282/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7303 - val_loss: 0.5244 - val_accuracy: 0.7295\n",
            "Epoch 2283/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7314 - val_loss: 0.5199 - val_accuracy: 0.7307\n",
            "Epoch 2284/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5105 - accuracy: 0.7312 - val_loss: 0.5225 - val_accuracy: 0.7296\n",
            "Epoch 2285/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7306 - val_loss: 0.5213 - val_accuracy: 0.7286\n",
            "Epoch 2286/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7306 - val_loss: 0.5213 - val_accuracy: 0.7296\n",
            "Epoch 2287/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7311 - val_loss: 0.5235 - val_accuracy: 0.7278\n",
            "Epoch 2288/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7315 - val_loss: 0.5245 - val_accuracy: 0.7292\n",
            "Epoch 2289/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7318 - val_loss: 0.5227 - val_accuracy: 0.7286\n",
            "Epoch 2290/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7309 - val_loss: 0.5213 - val_accuracy: 0.7294\n",
            "Epoch 2291/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7303 - val_loss: 0.5245 - val_accuracy: 0.7306\n",
            "Epoch 2292/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7311 - val_loss: 0.5201 - val_accuracy: 0.7304\n",
            "Epoch 2293/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7311 - val_loss: 0.5205 - val_accuracy: 0.7303\n",
            "Epoch 2294/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5104 - accuracy: 0.7304 - val_loss: 0.5211 - val_accuracy: 0.7307\n",
            "Epoch 2295/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7304 - val_loss: 0.5200 - val_accuracy: 0.7301\n",
            "Epoch 2296/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7310 - val_loss: 0.5215 - val_accuracy: 0.7302\n",
            "Epoch 2297/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7302 - val_loss: 0.5209 - val_accuracy: 0.7291\n",
            "Epoch 2298/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7301 - val_loss: 0.5195 - val_accuracy: 0.7305\n",
            "Epoch 2299/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7302 - val_loss: 0.5205 - val_accuracy: 0.7302\n",
            "Epoch 2300/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7308 - val_loss: 0.5203 - val_accuracy: 0.7299\n",
            "Epoch 2301/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7308 - val_loss: 0.5192 - val_accuracy: 0.7309\n",
            "Epoch 2302/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7310 - val_loss: 0.5205 - val_accuracy: 0.7302\n",
            "Epoch 2303/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.7301 - val_loss: 0.5219 - val_accuracy: 0.7297\n",
            "Epoch 2304/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7308 - val_loss: 0.5223 - val_accuracy: 0.7300\n",
            "Epoch 2305/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7318 - val_loss: 0.5234 - val_accuracy: 0.7291\n",
            "Epoch 2306/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7305 - val_loss: 0.5217 - val_accuracy: 0.7299\n",
            "Epoch 2307/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7314 - val_loss: 0.5212 - val_accuracy: 0.7294\n",
            "Epoch 2308/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7300 - val_loss: 0.5200 - val_accuracy: 0.7299\n",
            "Epoch 2309/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7311 - val_loss: 0.5218 - val_accuracy: 0.7296\n",
            "Epoch 2310/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7302 - val_loss: 0.5225 - val_accuracy: 0.7298\n",
            "Epoch 2311/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7304 - val_loss: 0.5212 - val_accuracy: 0.7300\n",
            "Epoch 2312/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7309 - val_loss: 0.5207 - val_accuracy: 0.7301\n",
            "Epoch 2313/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7306 - val_loss: 0.5208 - val_accuracy: 0.7295\n",
            "Epoch 2314/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7309 - val_loss: 0.5221 - val_accuracy: 0.7295\n",
            "Epoch 2315/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7310 - val_loss: 0.5223 - val_accuracy: 0.7295\n",
            "Epoch 2316/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - accuracy: 0.7318 - val_loss: 0.5214 - val_accuracy: 0.7305\n",
            "Epoch 2317/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7316 - val_loss: 0.5225 - val_accuracy: 0.7307\n",
            "Epoch 2318/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7308 - val_loss: 0.5213 - val_accuracy: 0.7310\n",
            "Epoch 2319/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7309 - val_loss: 0.5215 - val_accuracy: 0.7293\n",
            "Epoch 2320/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7302 - val_loss: 0.5211 - val_accuracy: 0.7304\n",
            "Epoch 2321/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7311 - val_loss: 0.5202 - val_accuracy: 0.7296\n",
            "Epoch 2322/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7308 - val_loss: 0.5203 - val_accuracy: 0.7297\n",
            "Epoch 2323/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7305 - val_loss: 0.5230 - val_accuracy: 0.7289\n",
            "Epoch 2324/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7316 - val_loss: 0.5207 - val_accuracy: 0.7292\n",
            "Epoch 2325/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7302 - val_loss: 0.5208 - val_accuracy: 0.7303\n",
            "Epoch 2326/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7306 - val_loss: 0.5232 - val_accuracy: 0.7284\n",
            "Epoch 2327/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7313 - val_loss: 0.5209 - val_accuracy: 0.7285\n",
            "Epoch 2328/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7307 - val_loss: 0.5207 - val_accuracy: 0.7294\n",
            "Epoch 2329/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7308 - val_loss: 0.5203 - val_accuracy: 0.7304\n",
            "Epoch 2330/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7313 - val_loss: 0.5210 - val_accuracy: 0.7309\n",
            "Epoch 2331/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7310 - val_loss: 0.5193 - val_accuracy: 0.7312\n",
            "Epoch 2332/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7318 - val_loss: 0.5206 - val_accuracy: 0.7301\n",
            "Epoch 2333/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7306 - val_loss: 0.5211 - val_accuracy: 0.7287\n",
            "Epoch 2334/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5086 - accuracy: 0.7320 - val_loss: 0.5197 - val_accuracy: 0.7311\n",
            "Epoch 2335/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7312 - val_loss: 0.5211 - val_accuracy: 0.7295\n",
            "Epoch 2336/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - accuracy: 0.7307 - val_loss: 0.5217 - val_accuracy: 0.7293\n",
            "Epoch 2337/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7309 - val_loss: 0.5222 - val_accuracy: 0.7303\n",
            "Epoch 2338/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7311 - val_loss: 0.5194 - val_accuracy: 0.7303\n",
            "Epoch 2339/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7314 - val_loss: 0.5223 - val_accuracy: 0.7297\n",
            "Epoch 2340/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7303 - val_loss: 0.5220 - val_accuracy: 0.7298\n",
            "Epoch 2341/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7314 - val_loss: 0.5224 - val_accuracy: 0.7288\n",
            "Epoch 2342/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7312 - val_loss: 0.5222 - val_accuracy: 0.7281\n",
            "Epoch 2343/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7312 - val_loss: 0.5220 - val_accuracy: 0.7293\n",
            "Epoch 2344/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7303 - val_loss: 0.5207 - val_accuracy: 0.7296\n",
            "Epoch 2345/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7315 - val_loss: 0.5219 - val_accuracy: 0.7300\n",
            "Epoch 2346/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7315 - val_loss: 0.5220 - val_accuracy: 0.7300\n",
            "Epoch 2347/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7314 - val_loss: 0.5211 - val_accuracy: 0.7310\n",
            "Epoch 2348/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7310 - val_loss: 0.5214 - val_accuracy: 0.7306\n",
            "Epoch 2349/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7306 - val_loss: 0.5222 - val_accuracy: 0.7295\n",
            "Epoch 2350/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7317 - val_loss: 0.5211 - val_accuracy: 0.7291\n",
            "Epoch 2351/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7318 - val_loss: 0.5220 - val_accuracy: 0.7303\n",
            "Epoch 2352/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7312 - val_loss: 0.5208 - val_accuracy: 0.7293\n",
            "Epoch 2353/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7315 - val_loss: 0.5196 - val_accuracy: 0.7302\n",
            "Epoch 2354/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7302 - val_loss: 0.5215 - val_accuracy: 0.7308\n",
            "Epoch 2355/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7303 - val_loss: 0.5234 - val_accuracy: 0.7281\n",
            "Epoch 2356/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7300 - val_loss: 0.5217 - val_accuracy: 0.7290\n",
            "Epoch 2357/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7302 - val_loss: 0.5215 - val_accuracy: 0.7305\n",
            "Epoch 2358/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.7302 - val_loss: 0.5225 - val_accuracy: 0.7290\n",
            "Epoch 2359/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7306 - val_loss: 0.5221 - val_accuracy: 0.7301\n",
            "Epoch 2360/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7309 - val_loss: 0.5232 - val_accuracy: 0.7291\n",
            "Epoch 2361/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7319 - val_loss: 0.5220 - val_accuracy: 0.7297\n",
            "Epoch 2362/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.7303 - val_loss: 0.5232 - val_accuracy: 0.7294\n",
            "Epoch 2363/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7307 - val_loss: 0.5203 - val_accuracy: 0.7298\n",
            "Epoch 2364/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7305 - val_loss: 0.5216 - val_accuracy: 0.7296\n",
            "Epoch 2365/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5087 - accuracy: 0.7307 - val_loss: 0.5214 - val_accuracy: 0.7305\n",
            "Epoch 2366/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7309 - val_loss: 0.5228 - val_accuracy: 0.7303\n",
            "Epoch 2367/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7305 - val_loss: 0.5228 - val_accuracy: 0.7300\n",
            "Epoch 2368/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7313 - val_loss: 0.5236 - val_accuracy: 0.7300\n",
            "Epoch 2369/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7302 - val_loss: 0.5226 - val_accuracy: 0.7301\n",
            "Epoch 2370/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7308 - val_loss: 0.5221 - val_accuracy: 0.7304\n",
            "Epoch 2371/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7304 - val_loss: 0.5223 - val_accuracy: 0.7309\n",
            "Epoch 2372/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7316 - val_loss: 0.5232 - val_accuracy: 0.7302\n",
            "Epoch 2373/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7306 - val_loss: 0.5208 - val_accuracy: 0.7311\n",
            "Epoch 2374/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7295 - val_loss: 0.5213 - val_accuracy: 0.7297\n",
            "Epoch 2375/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7317 - val_loss: 0.5230 - val_accuracy: 0.7308\n",
            "Epoch 2376/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7308 - val_loss: 0.5238 - val_accuracy: 0.7305\n",
            "Epoch 2377/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5085 - accuracy: 0.7320 - val_loss: 0.5220 - val_accuracy: 0.7285\n",
            "Epoch 2378/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7304 - val_loss: 0.5223 - val_accuracy: 0.7307\n",
            "Epoch 2379/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7314 - val_loss: 0.5216 - val_accuracy: 0.7312\n",
            "Epoch 2380/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7304 - val_loss: 0.5218 - val_accuracy: 0.7306\n",
            "Epoch 2381/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7312 - val_loss: 0.5224 - val_accuracy: 0.7302\n",
            "Epoch 2382/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7317 - val_loss: 0.5210 - val_accuracy: 0.7305\n",
            "Epoch 2383/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7307 - val_loss: 0.5218 - val_accuracy: 0.7299\n",
            "Epoch 2384/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5085 - accuracy: 0.7322 - val_loss: 0.5216 - val_accuracy: 0.7299\n",
            "Epoch 2385/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5086 - accuracy: 0.7320 - val_loss: 0.5222 - val_accuracy: 0.7299\n",
            "Epoch 2386/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7320 - val_loss: 0.5205 - val_accuracy: 0.7308\n",
            "Epoch 2387/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5086 - accuracy: 0.7310 - val_loss: 0.5205 - val_accuracy: 0.7301\n",
            "Epoch 2388/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7296 - val_loss: 0.5207 - val_accuracy: 0.7304\n",
            "Epoch 2389/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7307 - val_loss: 0.5206 - val_accuracy: 0.7296\n",
            "Epoch 2390/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7314 - val_loss: 0.5219 - val_accuracy: 0.7304\n",
            "Epoch 2391/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7304 - val_loss: 0.5213 - val_accuracy: 0.7300\n",
            "Epoch 2392/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7301 - val_loss: 0.5227 - val_accuracy: 0.7293\n",
            "Epoch 2393/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7305 - val_loss: 0.5215 - val_accuracy: 0.7304\n",
            "Epoch 2394/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7298 - val_loss: 0.5223 - val_accuracy: 0.7300\n",
            "Epoch 2395/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5086 - accuracy: 0.7318 - val_loss: 0.5225 - val_accuracy: 0.7287\n",
            "Epoch 2396/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7308 - val_loss: 0.5228 - val_accuracy: 0.7293\n",
            "Epoch 2397/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7297 - val_loss: 0.5214 - val_accuracy: 0.7291\n",
            "Epoch 2398/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7305 - val_loss: 0.5221 - val_accuracy: 0.7292\n",
            "Epoch 2399/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7304 - val_loss: 0.5209 - val_accuracy: 0.7302\n",
            "Epoch 2400/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7308 - val_loss: 0.5217 - val_accuracy: 0.7294\n",
            "Epoch 2401/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7308 - val_loss: 0.5218 - val_accuracy: 0.7303\n",
            "Epoch 2402/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7308 - val_loss: 0.5240 - val_accuracy: 0.7284\n",
            "Epoch 2403/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7308 - val_loss: 0.5218 - val_accuracy: 0.7287\n",
            "Epoch 2404/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7309 - val_loss: 0.5198 - val_accuracy: 0.7310\n",
            "Epoch 2405/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7301 - val_loss: 0.5221 - val_accuracy: 0.7299\n",
            "Epoch 2406/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7305 - val_loss: 0.5232 - val_accuracy: 0.7290\n",
            "Epoch 2407/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7304 - val_loss: 0.5225 - val_accuracy: 0.7302\n",
            "Epoch 2408/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7314 - val_loss: 0.5232 - val_accuracy: 0.7293\n",
            "Epoch 2409/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7310 - val_loss: 0.5234 - val_accuracy: 0.7301\n",
            "Epoch 2410/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7301 - val_loss: 0.5218 - val_accuracy: 0.7302\n",
            "Epoch 2411/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5085 - accuracy: 0.7314 - val_loss: 0.5228 - val_accuracy: 0.7299\n",
            "Epoch 2412/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7304 - val_loss: 0.5233 - val_accuracy: 0.7300\n",
            "Epoch 2413/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7310 - val_loss: 0.5259 - val_accuracy: 0.7302\n",
            "Epoch 2414/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5087 - accuracy: 0.7315 - val_loss: 0.5236 - val_accuracy: 0.7282\n",
            "Epoch 2415/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5087 - accuracy: 0.7313 - val_loss: 0.5227 - val_accuracy: 0.7296\n",
            "Epoch 2416/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7307 - val_loss: 0.5246 - val_accuracy: 0.7289\n",
            "Epoch 2417/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7315 - val_loss: 0.5225 - val_accuracy: 0.7292\n",
            "Epoch 2418/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7309 - val_loss: 0.5219 - val_accuracy: 0.7294\n",
            "Epoch 2419/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7315 - val_loss: 0.5208 - val_accuracy: 0.7299\n",
            "Epoch 2420/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7316 - val_loss: 0.5218 - val_accuracy: 0.7303\n",
            "Epoch 2421/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - accuracy: 0.7313 - val_loss: 0.5229 - val_accuracy: 0.7294\n",
            "Epoch 2422/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7320 - val_loss: 0.5208 - val_accuracy: 0.7308\n",
            "Epoch 2423/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7303 - val_loss: 0.5226 - val_accuracy: 0.7295\n",
            "Epoch 2424/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7320 - val_loss: 0.5227 - val_accuracy: 0.7301\n",
            "Epoch 2425/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7313 - val_loss: 0.5220 - val_accuracy: 0.7301\n",
            "Epoch 2426/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7312 - val_loss: 0.5213 - val_accuracy: 0.7307\n",
            "Epoch 2427/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7311 - val_loss: 0.5235 - val_accuracy: 0.7316\n",
            "Epoch 2428/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7298 - val_loss: 0.5223 - val_accuracy: 0.7300\n",
            "Epoch 2429/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7313 - val_loss: 0.5208 - val_accuracy: 0.7301\n",
            "Epoch 2430/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7311 - val_loss: 0.5237 - val_accuracy: 0.7307\n",
            "Epoch 2431/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7324 - val_loss: 0.5216 - val_accuracy: 0.7293\n",
            "Epoch 2432/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7310 - val_loss: 0.5237 - val_accuracy: 0.7291\n",
            "Epoch 2433/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7315 - val_loss: 0.5215 - val_accuracy: 0.7306\n",
            "Epoch 2434/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - accuracy: 0.7320 - val_loss: 0.5234 - val_accuracy: 0.7295\n",
            "Epoch 2435/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7312 - val_loss: 0.5233 - val_accuracy: 0.7297\n",
            "Epoch 2436/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7308 - val_loss: 0.5229 - val_accuracy: 0.7300\n",
            "Epoch 2437/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5087 - accuracy: 0.7306 - val_loss: 0.5213 - val_accuracy: 0.7300\n",
            "Epoch 2438/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7315 - val_loss: 0.5204 - val_accuracy: 0.7302\n",
            "Epoch 2439/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7303 - val_loss: 0.5249 - val_accuracy: 0.7278\n",
            "Epoch 2440/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7314 - val_loss: 0.5231 - val_accuracy: 0.7285\n",
            "Epoch 2441/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5102 - accuracy: 0.7302 - val_loss: 0.5244 - val_accuracy: 0.7297\n",
            "Epoch 2442/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7317 - val_loss: 0.5212 - val_accuracy: 0.7301\n",
            "Epoch 2443/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7315 - val_loss: 0.5215 - val_accuracy: 0.7297\n",
            "Epoch 2444/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7307 - val_loss: 0.5222 - val_accuracy: 0.7294\n",
            "Epoch 2445/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7318 - val_loss: 0.5238 - val_accuracy: 0.7296\n",
            "Epoch 2446/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7302 - val_loss: 0.5225 - val_accuracy: 0.7301\n",
            "Epoch 2447/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7298 - val_loss: 0.5218 - val_accuracy: 0.7299\n",
            "Epoch 2448/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7307 - val_loss: 0.5217 - val_accuracy: 0.7295\n",
            "Epoch 2449/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5085 - accuracy: 0.7314 - val_loss: 0.5209 - val_accuracy: 0.7296\n",
            "Epoch 2450/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - accuracy: 0.7308 - val_loss: 0.5207 - val_accuracy: 0.7303\n",
            "Epoch 2451/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7304 - val_loss: 0.5221 - val_accuracy: 0.7300\n",
            "Epoch 2452/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7303 - val_loss: 0.5230 - val_accuracy: 0.7302\n",
            "Epoch 2453/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7315 - val_loss: 0.5217 - val_accuracy: 0.7306\n",
            "Epoch 2454/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7309 - val_loss: 0.5224 - val_accuracy: 0.7293\n",
            "Epoch 2455/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5083 - accuracy: 0.7318 - val_loss: 0.5222 - val_accuracy: 0.7305\n",
            "Epoch 2456/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7312 - val_loss: 0.5225 - val_accuracy: 0.7296\n",
            "Epoch 2457/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7307 - val_loss: 0.5211 - val_accuracy: 0.7298\n",
            "Epoch 2458/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7310 - val_loss: 0.5202 - val_accuracy: 0.7306\n",
            "Epoch 2459/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7308 - val_loss: 0.5228 - val_accuracy: 0.7306\n",
            "Epoch 2460/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7316 - val_loss: 0.5212 - val_accuracy: 0.7288\n",
            "Epoch 2461/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7320 - val_loss: 0.5235 - val_accuracy: 0.7308\n",
            "Epoch 2462/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7304 - val_loss: 0.5226 - val_accuracy: 0.7302\n",
            "Epoch 2463/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7308 - val_loss: 0.5218 - val_accuracy: 0.7297\n",
            "Epoch 2464/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7310 - val_loss: 0.5226 - val_accuracy: 0.7293\n",
            "Epoch 2465/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7319 - val_loss: 0.5249 - val_accuracy: 0.7286\n",
            "Epoch 2466/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7301 - val_loss: 0.5230 - val_accuracy: 0.7305\n",
            "Epoch 2467/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5095 - accuracy: 0.7313 - val_loss: 0.5222 - val_accuracy: 0.7282\n",
            "Epoch 2468/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5086 - accuracy: 0.7318 - val_loss: 0.5216 - val_accuracy: 0.7304\n",
            "Epoch 2469/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7308 - val_loss: 0.5205 - val_accuracy: 0.7315\n",
            "Epoch 2470/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7320 - val_loss: 0.5211 - val_accuracy: 0.7305\n",
            "Epoch 2471/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - accuracy: 0.7310 - val_loss: 0.5214 - val_accuracy: 0.7302\n",
            "Epoch 2472/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7314 - val_loss: 0.5238 - val_accuracy: 0.7297\n",
            "Epoch 2473/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7318 - val_loss: 0.5220 - val_accuracy: 0.7295\n",
            "Epoch 2474/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5088 - accuracy: 0.7313 - val_loss: 0.5222 - val_accuracy: 0.7300\n",
            "Epoch 2475/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7316 - val_loss: 0.5212 - val_accuracy: 0.7304\n",
            "Epoch 2476/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5086 - accuracy: 0.7317 - val_loss: 0.5223 - val_accuracy: 0.7300\n",
            "Epoch 2477/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7304 - val_loss: 0.5208 - val_accuracy: 0.7296\n",
            "Epoch 2478/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7304 - val_loss: 0.5218 - val_accuracy: 0.7308\n",
            "Epoch 2479/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7306 - val_loss: 0.5215 - val_accuracy: 0.7301\n",
            "Epoch 2480/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7305 - val_loss: 0.5219 - val_accuracy: 0.7310\n",
            "Epoch 2481/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7310 - val_loss: 0.5214 - val_accuracy: 0.7309\n",
            "Epoch 2482/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.7311 - val_loss: 0.5224 - val_accuracy: 0.7298\n",
            "Epoch 2483/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7323 - val_loss: 0.5226 - val_accuracy: 0.7307\n",
            "Epoch 2484/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5089 - accuracy: 0.7309 - val_loss: 0.5211 - val_accuracy: 0.7309\n",
            "Epoch 2485/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5087 - accuracy: 0.7308 - val_loss: 0.5243 - val_accuracy: 0.7291\n",
            "Epoch 2486/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7305 - val_loss: 0.5208 - val_accuracy: 0.7305\n",
            "Epoch 2487/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7306 - val_loss: 0.5233 - val_accuracy: 0.7304\n",
            "Epoch 2488/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5097 - accuracy: 0.7313 - val_loss: 0.5241 - val_accuracy: 0.7297\n",
            "Epoch 2489/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7315 - val_loss: 0.5233 - val_accuracy: 0.7292\n",
            "Epoch 2490/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7302 - val_loss: 0.5218 - val_accuracy: 0.7306\n",
            "Epoch 2491/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7317 - val_loss: 0.5236 - val_accuracy: 0.7305\n",
            "Epoch 2492/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5099 - accuracy: 0.7305 - val_loss: 0.5224 - val_accuracy: 0.7304\n",
            "Epoch 2493/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5100 - accuracy: 0.7308 - val_loss: 0.5212 - val_accuracy: 0.7303\n",
            "Epoch 2494/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7321 - val_loss: 0.5217 - val_accuracy: 0.7300\n",
            "Epoch 2495/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5087 - accuracy: 0.7315 - val_loss: 0.5222 - val_accuracy: 0.7305\n",
            "Epoch 2496/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7305 - val_loss: 0.5224 - val_accuracy: 0.7310\n",
            "Epoch 2497/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5091 - accuracy: 0.7315 - val_loss: 0.5227 - val_accuracy: 0.7305\n",
            "Epoch 2498/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7309 - val_loss: 0.5244 - val_accuracy: 0.7295\n",
            "Epoch 2499/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7318 - val_loss: 0.5236 - val_accuracy: 0.7296\n",
            "Epoch 2500/2500\n",
            "293/293 [==============================] - 1s 3ms/step - loss: 0.5086 - accuracy: 0.7315 - val_loss: 0.5218 - val_accuracy: 0.7299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARIlkX8KMO3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the model weights to drive\n",
        "network.save('patent.h5')\n",
        "network.save_weights('patent_weights.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-OTIp7c_GfY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9342a993-9bfb-4f0d-aa1a-c3519f70d1d6"
      },
      "source": [
        "# Evaluating the model to evaluate accuracy\n",
        "eval_model = network.evaluate(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4688/4688 [==============================] - 7s 2ms/step - loss: 0.4961 - accuracy: 0.7429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5K3REMf-aQe",
        "colab_type": "text"
      },
      "source": [
        "### ***Evaluating Threshold value for classification***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNrxWArSEoW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computing F2-Measure using Precision, Recall and beta \n",
        "def F2( beta, P, R):\n",
        "  first = 1 + np.power(beta,2)\n",
        "  denom = np.multiply(P, (first - 1)) + R\n",
        "  num = P*R\n",
        "  return np.divide(np.multiply(first,num),denom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6D-xuQqAuOq",
        "colab_type": "text"
      },
      "source": [
        "**Generating set of values for plotting a graph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UjxQPgqzfY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating empty list to draw graphs \n",
        "# It may take some time\n",
        "t  = []\n",
        "# Creating values of threshold in range of (0.2,0.99)\n",
        "for i in range(30,80):\n",
        "  t.append((i+1)/100)\n",
        "Pr = [ ]\n",
        "Rr = [ ]\n",
        "F2r = [ ]\n",
        "\n",
        "for value in t :\n",
        "  y_pred = network.predict(X_test)\n",
        "  y_pred = (y_pred > value) # Predicting output values of test set \n",
        "  cn = confusion_matrix(y_test, y_pred) # Defining confusion matrix to compute Precision and Recall \n",
        "  P = (cn[0][0]/((cn[1][0])+(cn[0][0])))  # Precision value\n",
        "  R = (cn[0][0]/((cn[0][1])+(cn[0][0])))  # Recall value\n",
        "  beta = 2  \n",
        "\n",
        "  F2r.append(F2( beta, P, R)) # List of values of F2 measure\n",
        "  Pr.append(P)      # List of values of Precision\n",
        "  Rr.append(R)      # List of values of Recall "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEybP2evAqeR",
        "colab_type": "text"
      },
      "source": [
        "**Plotting graph for evaluating threshold value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGswK1hhAZjp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e23bdb-b9d0-4469-926f-bd69659e26d4"
      },
      "source": [
        "plt.figure(figsize=(12,10))    # Size of the graph\n",
        "\n",
        "# Graph of Precision(Pr) vs threshold value(t) in colour red\n",
        "plt.plot(np.array(t),np.array(Pr),'r')  \n",
        "\n",
        "# Graph of Recall(Rr) vs threshold value(t) in colour green\n",
        "plt.plot(np.array(t),np.array(Rr),'g')\n",
        "\n",
        "# Graph of F2 Measure(F2r) vs threshold value(t) in colour blue\n",
        "plt.plot(np.array(t),np.array(F2r),'b')\n",
        "\n",
        "plt.xlabel('Threshold values')\n",
        "plt.ylabel('Precision,Recall,F2-Measure')\n",
        "plt.title('Threshold Evaluation Graph')\n",
        "plt.grid(which = 'major', axis= 'both')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYJXeFk7Ntgx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "754e8a53-1101-4014-9f43-d6d83234993e"
      },
      "source": [
        "y_pred = network.predict(X_test)\n",
        "y_pred = ( y_pred > 0.50)    # Inputting threshold value after evaluation \n",
        "\n",
        "def plot_confusion_matrix(normalize):\n",
        "  classes = ['CBP','NON_CBP']\n",
        "  tick_marks = [0.5,1.5]\n",
        "  cn = confusion_matrix(y_test, y_pred,normalize=normalize)\n",
        "  sns.heatmap(cn, annot = True)\n",
        "  plt.xticks(tick_marks, classes)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "print('Confusion Matrix with Normalized Values')\n",
        "plot_confusion_matrix(normalize='true')\n",
        "\n",
        "print('Confusion Matrix without Normalization')\n",
        "plot_confusion_matrix(normalize=None)\n",
        "\n",
        "cn = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "P = (cn[0][0]/((cn[1][0])+(cn[0][0])))\n",
        "\n",
        "R = (cn[0][0]/((cn[0][1])+(cn[0][0])))\n",
        "\n",
        "def F2( beta, P, R):\n",
        "  first = 1 + np.power(beta,2)\n",
        "  denom = np.multiply(P, (first - 1)) + R\n",
        "  num = P*R\n",
        "  return np.divide(np.multiply(first,num),denom)\n",
        "\n",
        "print('Precision : ', P)\n",
        "print('Recall : ', R)\\\n",
        "\n",
        "beta = 2\n",
        "print('F2-Measure :',F2(beta,P,R))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix with Normalized Values\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix without Normalization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Precision :  0.815373642049819\n",
            "Recall :  0.59444\n",
            "F2-Measure : 0.6284996532065705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76ugr2kFa-yb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de33888a-9e3b-4345-d96c-984b63d021a9"
      },
      "source": [
        "1plt.figure(figsize=(10,10))\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend(['Training', 'Testing'])\n",
        "plt.savefig('patent_accuracy.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny7Fu3OzeLt4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255fa2b3-af4c-49f1-c420-d6838aa158d2"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend(['Training', 'Testing'])\n",
        "plt.savefig('patent_loss.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y2ZnGZaz9kx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}